{"./":{"url":"./","title":"前言","keywords":"","body":"Swift Handbook 此书是为帮助大家更快了解swift的管理和使用。 GitHub地址：https://github.com/eiuapp/swift-handbook 在线访问地址：https://eiu.app/swift-handbook swift是著名的对象存储系统。 下图是swift生态圈图： 相关资源 关于 本书中引用了一些公开的分享与链接并加以整理。 本书作于2019年初，会持续更新。 Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/env.html":{"url":"docs/env.html","title":"swift 2.17 环境配置","keywords":"","body":"这里是贯穿本书的环境 env-1 os：Ubuntu Server 16.04×64 openstack: queen 版 mariadb: 10.0 keystone: swift: 2.17.1dev20 Python: 2.7 rsync: 3.0 存储设置：4T 架构部署: 主机名 IP 作用 controller 192.168.100.50 controller node keystone 192.168.100.50 keystone node proxy 192.168.100.50 proxy object1 192.168.100.105 存储节点1(zone1) object2 192.168.100.106 存储节点2(zone1) object3 192.168.100.107 存储节点3(zone1) object4 192.168.100.107 存储节点4(zone1) 其中 192.168.100.50 有一个另一个网卡IP 192.168.0.50 env-2 os：Ubuntu Server 16.04×64 openstack: queen 版 mariadb: 10.2 keystone: swift: 2.17.1dev20 Python: 2.7 rsync: 3.0 存储设置：4T 架构部署: 主机名 IP 作用 controller 192.168.10.13 controller node mysql 192.168.10.13 存 keystone 中用户帐户密码等数据 keystone 192.168.10.13 auth Proxy 192.168.10.13 swift proxy node Proxy 192.168.10.12 swift proxy node Proxy 192.168.10.11 swift proxy node object1 192.168.0.127 存储节点1(zone1) object2 192.168.0.134 存储节点2(zone1) object3 192.168.0.135 存储节点3(zone1) object4 192.168.0.180 存储节点4(zone1) object5 192.168.0.189 存储节点5(zone1) Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-queens-install.html":{"url":"docs/swift-queens-install.html","title":"swift 安装过程","keywords":"","body":"+++ title = \"swift 安装过程（queen）\" date = 2019-01-08T00:00:00+08:00 lastmod = 2019-01-09T16:27:38+08:00 tags = [\"openstack\", \"swift\", \"install\"] categories = [\"openstack\"] draft = false weight = 3001 +++ 主要记录一下根据官方文档安装过程中遇到的一些小的问题。 env os: 均为 ubuntu server 16.04 controller node: 192.168.100.50 网络适配器 3 个： VMnet0 VMnet1 VMnet8(NAT) storage node: 192.168.100.105 192.168.100.106 192.168.100.107 各节点网络适配器相同，2个： VMnet0 VMnet1 注意：钱工说 VMnet0 就是桥接网络 以 storage node 中的 IP 为 192.168.100.107 为例子，展示一下 /etc/hosts 文件 。 ubuntu@swift107:~$ cat /etc/hosts 127.0.0.1 localhost 127.0.0.1 swift107 127.0.1.1 ubuntu # controller 192.168.100.50 controller # storage 192.168.100.105 swift105 192.168.100.106 swift106 192.168.100.107 swift107 官方文档这里只显示了 storage node : 10.0.0.51 object1 10.0.0.52 object2 step environment networking https://docs.openstack.org/swift/queens/install/environment-networking.html 这里记录的2个节点，均指存储节点（storage node）. 官方文档这里对每个节点均设置了2个硬盘（/dev/sdb, /dev/sdb）. storage node https://docs.openstack.org/swift/queens/install/storage-install-ubuntu-debian.html storage node 可以使用已挂载硬盘的某个新分区 Format the /dev/sdb and /dev/sdc devices as XFS: $ mkfs.xfs /dev/sda6 storage node 中的 MANAGEMENT_INTERFACE_IP_ADDRESS https://docs.openstack.org/swift/queens/install/storage-install-ubuntu-debian.html#install-and-configure-components Replace MANAGEMENT_INTERFACE_IP_ADDRESS with the IP address of the management network on the storage node. 这里的 MANAGEMENT_INTERFACE_IP_ADDRESS 应该理解为 storage node 上的对应与 management network 中的IP，也就是 192.168.100.105 之类的IP rings https://docs.openstack.org/swift/queens/install/initial-rings.html#distribute-ring-configuration-files 这一步不要忘记了。也就是把 controller node 中的 account.ring.gz, container.ring.gz, and object.ring.gz 复制过去。 finalize-installation https://docs.openstack.org/swift/queens/install/finalize-installation-ubuntu-debian.html On the controller node and any other nodes running the proxy service, restart the Object Storage proxy service including its dependencies: $ service memcached restart $ service swift-proxy restart 这里运行完成后，必须要用 status 检查一下。当时我就是没有检查（实际上是storage node没有开放相应端口），导致后面一步的 `swift stat` 失效。 $ service memcached status $ service swift-proxy status verify https://docs.openstack.org/swift/queens/install/verify.html 基本的命令，也可以通过 horizon 进行。 demo-openrc 文件应该之前就有了。 root@controller:~# cat demo-openrc export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=demo export OS_USERNAME=demo export OS_PASSWORD=openstack export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 root@controller:~# . demo-openrc 上传文件 root@controller:~# openstack object create container1 360bdoctor.exe root@controller:~# openstack object list container1 +----------------+ | Name | +----------------+ | 360bdoctor.exe | +----------------+ root@controller:~# 当上传文件太大时，会出503错误。 root@controller:~# openstack object create container1 cn_sql_server_2008_r2_enterprise_x86_x64_ia64_dvd_522233.rar -v START with options: [u'object', u'create', u'container1', u'cn_sql_server_2008_r2_enterprise_x86_x64_ia64_dvd_522233.rar', u'-v'] command: object create -> openstackclient.object.v1.object.CreateObject (auth=True) Using auth plugin: password Service Unavailable (HTTP 503) (Request-ID: tx063c37fe0f004a6fba14c-005c345bf6) END return value: 1 root@controller:~# du -sh cn_sql_server_2008_r2_enterprise_x86_x64_ia64_dvd_522233.rar 3.3G cn_sql_server_2008_r2_enterprise_x86_x64_ia64_dvd_522233.rar root@controller:~# 是不是与配置过程中的 https://docs.openstack.org/swift/queens/install/initial-rings.html For simplicity, this guide uses one region and two zones with 2^10 (1024) maximum partitions, 3 replicas of each object, and 1 hour minimum time between moving a partition more than once. 中的 2^10 (1024) maximum partitions 这个位置相关。如果是这个思路，我们可以尝试往这个方向，改一下。 在上传文件时，请不要使用 \"./****\" 形式 因为这样的话，会导致在 horizon 中显示为目录形式，这明显不对。 而且，当有这样的目录文件时，在 horizon 是删除不了这个文件的（可能是因为，horizon 传过去的就是 \".\" ，而不是你希望的 \"./360bdoctor.exe\" 这样的文件） 所以这时，只能通过命令行`openstack object delete controller1 ./360bdoctor.exe`这样的删除了。 (下面命令请不要运行) $ openstack object create container1 ./360bdoctor.exe 不能上传文件夹 官网的这次配置不能上传文件夹，是不是与配置过程中的某个因素相关 root@controller:~# ll total 3450604 drwx------ 8 root root 4096 Jan 8 15:52 ./ drwxr-xr-x 23 root root 4096 Oct 22 18:11 ../ -rw-r--r-- 1 root root 1993728 Jan 4 18:58 360bdoctor.exe drwx------ 2 root root 4096 Oct 29 10:50 .ssh/ root@controller:~# openstack object create container1 ttt -v START with options: [u'object', u'create', u'container1', u'ttt', u'-v'] command: object create -> openstackclient.object.v1.object.CreateObject (auth=True) Using auth plugin: password [Errno 21] Is a directory: u'ttt' END return value: 1 root@controller:~# Ref https://docs.openstack.org/swift/queens/install/ Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-controller-node-add-a-new-storage-node.html":{"url":"docs/swift-controller-node-add-a-new-storage-node.html","title":"增加storage节点","keywords":"","body":"controller node add a new storage node 注意： 当controller node 做了 rebalance 后，希望 oss 保持最新，则必须： 向所有的storage node 分发最新的 /etc/swift/*.ring.gz 文件 在所有的 storage node 重启 swift-init step 修改hosts tail -1 hosts >> /etc/hosts 删除老节点 ## remove swift-ring-builder object.builder swift-ring-builder object.builder remove --id=1 swift-ring-builder container.builder swift-ring-builder container.builder remove --id=1 swift-ring-builder account.builder swift-ring-builder account.builder remove --id=1 ## check swift-ring-builder object.builder swift-ring-builder container.builder swift-ring-builder account.builder ## add swift-ring-builder object.builder add --region 1 --zone 1 --ip 192.168.0.127 --port 6200 --device sdb --weight 100 swift-ring-builder container.builder add --region 1 --zone 1 --ip 192.168.0.127 --port 6201 --device sdb --weight 100 swift-ring-builder account.builder add --region 1 --zone 1 --ip 192.168.0.127 --port 6202 --device sdb --weight 100 swift-ring-builder object.builder add --region 1 --zone 1 --ip 192.168.0.180 --port 6200 --device sdb --weight 100 swift-ring-builder container.builder add --region 1 --zone 1 --ip 192.168.0.180 --port 6201 --device sdb --weight 100 swift-ring-builder account.builder add --region 1 --zone 1 --ip 192.168.0.180 --port 6202 --device sdb --weight 100 ## check swift-ring-builder object.builder swift-ring-builder container.builder swift-ring-builder account.builder ## rebalance swift-ring-builder object.builder rebalance swift-ring-builder account.builder rebalance swift-ring-builder container.builder rebalance ## check swift-ring-builder object.builder swift-ring-builder container.builder swift-ring-builder account.builder 传*.ring.gz文件给其它node cp /etc/swift/*.ring.gz /tmp/storage-node/swift/ storage node get the ring.gz scp ubuntu@192.168.0.51:/tmp/storage-node-files/storage-node/ring.gz/ /tmp/storage-node/ cp /tmp/storage-node/ring.gz/* /etc/swift/ chown -R root:swift /etc/swift On the controller node service memcached restart service swift-proxy restart service memcached status service swift-proxy status On the storage nodes, start the Object Storage services: service rsync stop service rsync start service rsync status swift-init all restart Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-multy-proxy-node-conf.html":{"url":"docs/swift-multy-proxy-node-conf.html","title":"多代理节点安装","keywords":"","body":"Openstack存储swift多代理节点安装配置 env 系统：Ubuntu Server 16.04×64 存储设置：4T 架构部署: 主机名 IP 作用 Proxy 192.168.0.51 代理节点, controller node object1 192.168.0.127 存储节点1(zone1) object2 192.168.0.134 存储节点2(zone1) object3 192.168.0.135 存储节点3(zone1) object4 192.168.0.180 存储节点4(zone1) object5 192.168.0.189 存储节点5(zone1) 增加代理节点 主机名 IP 作用 Proxybak 192.168.0.141 代理节点, 做冗余备份 注意 /etc/swift/proxy-swift.conf 中 关于auth的部分,IP 一定要指向 keystone IP。 在使用时，OS_AUTH_URL 中体现的 IP 一定要指向 keystone。如：OS_AUTH_URL=http://controller:5000/v3 中的controller 的IP要指向 keystone IP。 swift服务endpoint的IP要指向 swift proxy 的IP。如：本环境中，指向 192.168.0.51或者192.168.0.141。 新增节点的python请一定要能import memcache。 step swift 环境 之前已经正常安装好了proxy node与 object1-object5. 具体步骤, 参考前文，这里省略。 下面讲一下，新增 proxybak 节点的部分。 增加代理节点从这里开始 proxy node 准备一些文件 root@controller:~# cp -a /etc/swift/ /tmp/swift/ root@controller:~# cp /etc/hosts /tmp/ root@controller:~# proxybak node 安装依赖 apt-get install swift swift-proxy python-swiftclient \\ python-keystoneclient python-keystonemiddleware \\ memcached 拿文件 /etc/hosts 文件请与proxy node保持一致。ping controller 保证连接到 controller node。 root@ubuntu:~# scp -r ubuntu@192.168.0.51:/tmp/swift/ . root@ubuntu:~# scp -r ubuntu@192.168.0.51:/tmp/hosts . root@ubuntu:~# tail -10 hosts >> /etc/hosts root@ubuntu:~# cat /etc/hosts root@ubuntu:~# ping controller PING controller (192.168.0.51) 56(84) bytes of data. 64 bytes from controller (192.168.0.51): icmp_seq=1 ttl=64 time=45.1 ms ^C --- controller ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 45.145/45.145/45.145/0.000 ms root@ubuntu:~# 准备配置 root@ubuntu:~# mv swift/ /etc/swift/ root@ubuntu:~# ls /etc/swift/ 什么也不要修改。如果说你有需求要修改配置，请注意： swift.conf配置文件的hash值不要修改，请保持一致。 修改proxy-server.conf 中的IP 设置为Proxybak的IP 比如，这里就是把bind_ip设置为 192.168.0.141。 root@ubuntu:/etc/swift# head proxy-server.conf [DEFAULT] bind_ip = 192.168.0.141 bind_port = 8080 root@ubuntu:/etc/swift# 开启swift-proxy root@ubuntu:/etc/swift# chown -R root:swift /etc/swift root@ubuntu:/etc/swift# service memcached restart root@ubuntu:/etc/swift# service swift-proxy restart root@ubuntu:/etc/swift# service memcached status root@ubuntu:/etc/swift# service swift-proxy status 开启 swift-proxy 后，会在本地启动 8080 端口 root@ubuntu:/etc/swift# netstat -ntlp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.1:11211 0.0.0.0:* LISTEN 7517/memcached tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 7818/python tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 3415/sshd tcp 0 0 127.0.0.1:6010 0.0.0.0:* LISTEN 3572/0 tcp 0 0 127.0.0.1:6011 0.0.0.0:* LISTEN 6366/1 tcp6 0 0 :::22 :::* LISTEN 3415/sshd tcp6 0 0 ::1:6010 :::* LISTEN 3572/0 tcp6 0 0 ::1:6011 :::* LISTEN 6366/1 root@ubuntu:/etc/swift# 在 proxybak node 上访问 swift 服务。 root@ubuntu:/etc/swift# cat demo-openrc export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=demo export OS_USERNAME=demo export OS_PASSWORD=openstack export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 root@ubuntu:/etc/swift# . demo-openrc root@ubuntu:/etc/swift# swift list container1 container2 root@ubuntu:/etc/swift# 结果正常。说明proxybak node 可用。 增加代理节点成功，操作结束。 ref https://docs.openstack.org/swift/latest/install/controller-install-ubuntu.html#install-and-configure-components Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-multy-proxy-node-ha-with-nginx.html":{"url":"docs/swift-multy-proxy-node-ha-with-nginx.html","title":"多代理节点通过nginx实现HA","keywords":"","body":"Openstack存储swift多代理节点通过nginx实现HA 上图 Load Balancer 由 nginx upsteam 来实现 env 系统：Ubuntu Server 16.04×64 存储设置：4T 架构部署: 主机名 IP 作用 Proxy 192.168.0.51 keystone Proxybak 192.168.0.142 nginx Proxy 192.168.0.51 代理节点 Proxybak 192.168.0.141 代理节点, 做冗余备份 object1 192.168.0.127 存储节点1(zone1) object2 192.168.0.134 存储节点2(zone1) object3 192.168.0.135 存储节点3(zone1) object4 192.168.0.180 存储节点4(zone1) object5 192.168.0.189 存储节点5(zone1) Assumptions: KeyStone V3 is setup and Swift is configured. Python openstack client command line interface is available. By default, 'admin' user and 'admin' project is created and assigned to 'default' Domain. We will use this in this exercise, however you can configure different user, project and domain to accomplish ResellerAdmin setup. Openrc file is available to work with openstack CLI client. 注意 无 step 验证各proxy node 本身有效 分别在 proxy node 运行 . demo-openrc && swift stat 配置 nginx 把下面配置加入到 nginx 配置文件 /etc/nginx/nginx.conf 中 upstream swiftproxy { server 192.168.0.141:8080; server 192.168.0.51:8080; } 新建立一个 swiftproxy.conf 文件，实现HA root@ubuntu:/etc/nginx/conf.d# head -13 swiftproxy.conf server { listen 8080; server_name localhost; #charset koi8-r; #access_log /var/log/nginx/host.access.log main; location / { root /usr/share/nginx/html; index index.html index.htm; proxy_pass http://swiftproxy; } root@ubuntu:/etc/nginx/conf.d# tail 开启日志 root@ubuntu:~# tail -f /var/log/nginx/access.log 192.168.0.129 - - [25/Jan/2019:09:36:59 +0800] \"GET /requests/status.xml HTTP/1.1\" 502 537 \"-\" \"Dalvik/2.1.0 (Linux; U; Android 8.1.0; vivo NEX S Build/OPM1.171019.026)\" \"-\" 192.168.0.129 - - [25/Jan/2019:09:36:59 +0800] \"GET /requests/status.xml HTTP/1.1\" 502 537 \"-\" \"Dalvik/2.1.0 (Linux; U; Android 8.1.0; vivo NEX S Build/OPM1.171019.026)\" \"-\" 192.168.0.129 - - [25/Jan/2019:09:37:00 +0800] \"GET /requests/status.xml HTTP/1.1\" 502 537 \"-\" \"Dalvik/2.1.0 (Linux; U; Android 8.1.0; vivo NEX S Build/OPM1.171019.026)\" \"-\" （可跳过）有意 写错误的 swiftproxy IP root@controller:~# sudo vi /etc/hosts root@controller:~# . demo-openrc root@controller:~# swift stat HTTPConnectionPool(host='swiftproxy', port=8080): Max retries exceeded with url: /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 (Caused byoute to host',)) root@controller:~# 把 swiftproxy 指向 nginx IP root@controller:~# sudo vi /etc/hosts root@controller:~# grep swiftproxy -rn /etc/hosts 18:# swiftproxy 19:192.168.0.142 swiftproxy root@controller:~# 开始吧 在任意，能发送 swift 请求的地方，向 nginx IP 发送 swift-proxy 相关的请求吧。 root@controller:~# . demo-openrc root@controller:~# swift stat Account: AUTH_7d6eaa90d74a4f239963933c3a744df3 Containers: 2 Objects: 30 Bytes: 3718416484 Containers in policy \"policy-0\": 2 Objects in policy \"policy-0\": 30 Bytes in policy \"policy-0\": 3718416484 X-Openstack-Request-Id: tx8b15b33e61dd4e83bca33-005c4a6cd1 X-Account-Project-Domain-Id: default Server: nginx/1.14.2 Connection: keep-alive X-Timestamp: 1547188179.49086 X-Trans-Id: tx8b15b33e61dd4e83bca33-005c4a6cd1 Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# swift stat -v StorageURL: http://swiftproxy:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 Auth Token: gAAAAABcSmz_UrWKnnqf8yuLfRFR80v_m2xh9sLOwEXuxt9L3lPAHdmY7GuUmLDMRbVg42gDsuY20tI67lxWXk-cwU39eguQcnU1 Account: AUTH_7d6eaa90d74a4f239963933c3a744df3 Containers: 2 Objects: 30 Bytes: 3718416484 Containers in policy \"policy-0\": 2 Objects in policy \"policy-0\": 30 Bytes in policy \"policy-0\": 3718416484 X-Openstack-Request-Id: txf37fddd19e834f35ba1eb-005c4a6cff X-Account-Project-Domain-Id: default Server: nginx/1.14.2 Connection: keep-alive X-Timestamp: 1547188179.49086 X-Trans-Id: txf37fddd19e834f35ba1eb-005c4a6cff Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# 同时查看 nginx 日志 root@ubuntu:~# tail -f /var/log/nginx/access.log 192.168.0.129 - - [25/Jan/2019:09:36:59 +0800] \"GET /requests/status.xml HTTP/1.1\" 502 537 \"-\" \"Dalvik/2.1.0 (Linux; U; Android 8.1.0; vivo NEX S Build/OPM1.171019.026)\" \"-\" 192.168.0.129 - - [25/Jan/2019:09:36:59 +0800] \"GET /requests/status.xml HTTP/1.1\" 502 537 \"-\" \"Dalvik/2.1.0 (Linux; U; Android 8.1.0; vivo NEX S Build/OPM1.171019.026)\" \"-\" 192.168.0.129 - - [25/Jan/2019:09:37:00 +0800] \"GET /requests/status.xml HTTP/1.1\" 502 537 \"-\" \"Dalvik/2.1.0 (Linux; U; Android 8.1.0; vivo NEX S Build/OPM1.171019.026)\" \"-\" 192.168.0.51 - - [25/Jan/2019:09:56:34 +0800] \"HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 HTTP/1.1\" 204 0 \"-\" \"python-swiftclient-3.5.0\" \"-\" 192.168.0.51 - - [25/Jan/2019:09:57:19 +0800] \"HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 HTTP/1.1\" 204 0 \"-\" \"python-swiftclient-3.5.0\" \"-\" 有正确请求和响应。 结果正常。说明proxybak node 实现高可用。 操作结束。 ref http://nginx.org/ https://www.cnblogs.com/wzjhoutai/p/6932007.html Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-multy-proxy-node-illustration.html":{"url":"docs/swift-multy-proxy-node-illustration.html","title":"多代理节点效果展示","keywords":"","body":"Openstack存储swift多代理节点效果展示 env 系统：Ubuntu Server 16.04×64 存储设置：4T 架构部署: 主机名 IP 作用 Proxy 192.168.0.51 代理节点, controller node object1 192.168.0.127 存储节点1(zone1) object2 192.168.0.134 存储节点2(zone1) object3 192.168.0.135 存储节点3(zone1) object4 192.168.0.180 存储节点4(zone1) object5 192.168.0.189 存储节点5(zone1) 增加代理节点 Proxybak 192.168.0.141 代理节点, 做冗余备份 step 直接看图 swift-multy-proxy-node-illustration 在 141 上 先使用 192.168.0.51 作为 proxy，访问正常。 把 51 关闭，访问不正常。 在 51 上 使用 141 作为 proxy, 访问正常。 在 141 上 使用 141 作为 proxy, 访问正常。 结论 51或者141 中，只有要一个 proxy 节点存在，整个 swift 服务都可以对外提供服务。耶耶耶！~ Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/proxy-node-install.html":{"url":"docs/proxy-node-install.html","title":"代理节点安装","keywords":"","body":"在 192.168.10.12 上安装 proxy node env 因为 192.168.10.13 中的 用户信息获取时总是有问题。所以，组员决定不折腾（更折腾）在另一台机器上重新安装 proxy , keystone, controller 组件。 proxy 搭建 对应的controller，keystone 均为 192.168.10.12 step root@ubuntu:~# sudo apt-get install swift swift-proxy python-swiftclient \\ > python-keystoneclient python-keystonemiddleware \\ > memcached -y 发现没有 /etc/swift/ 文件夹，那么把 192.168.10.13 上的 /etc/swift/ 直接复制过来。 root@ubuntu:~# ls /etc/swift ls: cannot access '/etc/swift': No such file or directory root@ubuntu:~# cp -a /home/administrator/swift/ /etc/swift/ root@ubuntu:~# cd /etc/swift/ root@ubuntu:/etc/swift# head proxy-server.conf [DEFAULT] # bind_ip = 0.0.0.0 bind_port = 8080 # bind_timeout = 30 # backlog = 4096 swift_dir = /etc/swift user = swift # Enables exposing configuration settings via HTTP GET /info. # expose_info = true root@ubuntu:/etc/swift# chown -R root:swift /etc/swift 修改 /etc/hosts 文件 root@ubuntu:~# vi /etc/hosts root@ubuntu:~# cat /etc/hosts 127.0.0.1 localhost 127.0.1.1 ubuntu #192.168.10.12 controller # The following lines are desirable for IPv6 capable hosts ::1 localhost ip6-localhost ip6-loopback ff02::1 ip6-allnodes ff02::2 ip6-allrouters # controller 192.168.10.13 controller # swiftproxy 192.168.10.12 swiftproxy root@ubuntu:~# 确保有 memcache 包 root@ubuntu:/etc/swift# python Python 2.7.12 (default, Nov 12 2018, 14:36:49) [GCC 5.4.0 20160609] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import memcache >>> root@ubuntu:/etc/swift# root@ubuntu:/etc/swift# service memcached restart root@ubuntu:/etc/swift# service swift-proxy restart root@ubuntu:/etc/swift# service memcached status root@ubuntu:/etc/swift# service memcached status root@ubuntu:/etc/swift# service swift-proxy status root@ubuntu:/etc/swift# netstat -ntlp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 7904/sshd tcp 0 0 127.0.0.1:7001 0.0.0.0:* LISTEN 11437/etcd tcp 0 0 127.0.0.1:6010 0.0.0.0:* LISTEN 15627/0 tcp 0 0 127.0.0.1:4001 0.0.0.0:* LISTEN 11437/etcd tcp 0 0 0.0.0.0:25672 0.0.0.0:* LISTEN 9901/beam.smp tcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTEN 8792/mysqld tcp 0 0 0.0.0.0:11211 0.0.0.0:* LISTEN 16688/memcached tcp 0 0 127.0.0.1:2379 0.0.0.0:* LISTEN 11437/etcd tcp 0 0 127.0.0.1:2380 0.0.0.0:* LISTEN 11437/etcd tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 16714/python tcp 0 0 0.0.0.0:4369 0.0.0.0:* LISTEN 9788/epmd tcp6 0 0 :::22 :::* LISTEN 7904/sshd tcp6 0 0 ::1:6010 :::* LISTEN 15627/0 tcp6 0 0 :::5000 :::* LISTEN 13217/apache2 tcp6 0 0 :::5672 :::* LISTEN 9901/beam.smp tcp6 0 0 :::80 :::* LISTEN 13217/apache2 tcp6 0 0 :::4369 :::* LISTEN 9788/epmd root@ubuntu:/etc/swift# root@ubuntu:~# cd root@ubuntu:~# ls demo-openrc root@ubuntu:~# vi demo-openrc root@ubuntu:~# . demo-openrc root@ubuntu:~# swift list ab abc abcd container1 test root@ubuntu:~# 这里还是 192.168.10.13 上的数据 查一下 endpoint root@ubuntu:~# cat demo-openrc export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=demo export OS_USERNAME=demo export OS_PASSWORD=413eaac0d4cff66fde56 export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 root@ubuntu:~# root@ubuntu:~# openstack endpoint list +----------------------------------|-----------|--------------|--------------|---------|-----------|--------------------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------|-----------|--------------|--------------|---------|-----------|--------------------------------------------------+ | 3736d384b23f45a8a21e6f5e5888d759 | RegionOne | keystone | identity | True | admin | http://192.168.10.13:5000/v3/ | | 3fc42de3790b4c5f8d635a9721ed23a6 | RegionOne | swift | object-store | True | public | http://192.168.10.13:8080/v1/AUTH_%(project_id)s | | 88634b7b1e6c4b0989f079f00adbffb8 | RegionOne | keystone | identity | True | internal | http://192.168.10.13:5000/v3/ | | 8b4faa17e4404ef4aa709c0e78636742 | RegionOne | swift | object-store | True | internal | http://192.168.10.13:8080/v1/AUTH_%(project_id)s | | bf0706eea0e04412ae4588ec9dd067a0 | RegionOne | keystone | identity | True | public | http://192.168.10.13:5000/v3/ | | bfae1eb954c84364803023604999ef93 | RegionOne | swift | object-store | True | admin | http://192.168.10.13:8080/v1/ | +----------------------------------|-----------|--------------|--------------|---------|-----------|--------------------------------------------------+ root@ubuntu:~# 这里 肯定要修改一下 endpoint, 让 swift 的服务 url 指向 192.168.10.12 修改 /etc/hosts root@ubuntu:~# vi /etc/hosts root@ubuntu:~# cat /etc/hosts 127.0.0.1 localhost 127.0.1.1 ubuntu #192.168.10.12 controller # The following lines are desirable for IPv6 capable hosts ::1 localhost ip6-localhost ip6-loopback ff02::1 ip6-allnodes ff02::2 ip6-allrouters # controller 192.168.10.12 controller # swiftproxy 192.168.10.12 swiftproxy root@ubuntu:~# 重启服务，发现 openstack 认证不了。要去找 admin 用户。 root@ubuntu:~# service swift-proxy restart root@ubuntu:~# service memcached restart root@ubuntu:~# openstack endpoint list The request you have made requires authentication. (HTTP 401) (Request-ID: req-afb2463d-ba10-455b-a3af-5ebd0b1a1bb4) root@ubuntu:~# root@ubuntu:~# vi admin-openrc root@ubuntu:~# . admin-openrc root@ubuntu:~# openstack endpoint list +----------------------------------|-----------|--------------|--------------|---------|-----------|-------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------|-----------|--------------|--------------|---------|-----------|-------------------------------+ | 3424fdd10aa047418502f4fe92f1f10c | RegionOne | keystone | identity | True | public | http://192.168.10.12:5000/v3/ | | 698a2a4b64b74e5ea289c21296e36303 | RegionOne | keystone | identity | True | admin | http://192.168.10.12:5000/v3/ | | d66b0bd887d141c49a166c2426633cb8 | RegionOne | keystone | identity | True | internal | http://192.168.10.12:5000/v3/ | +----------------------------------|-----------|--------------|--------------|---------|-----------|-------------------------------+ root@ubuntu:~# openstack user list +----------------------------------|-------+ | ID | Name | +----------------------------------|-------+ | 590061f35488419e8aa33fd9cfc9ce3f | demo | | e73b44116b8a4a0aacb9ad6b4f08d7b8 | admin | +----------------------------------|-------+ 创建 domain root@ubuntu:~# openstack user create --domain default --password-prompt swift User Password: Repeat User Password: +---------------------|----------------------------------+ | Field | Value | +---------------------|----------------------------------+ | domain_id | default | | enabled | True | | id | d1eecb70a7314472b386e8eb2d05aed8 | | name | swift | | options | {} | | password_expires_at | None | +---------------------|----------------------------------+ root@ubuntu:~# swift 对应 密码是 5df7764659a27b695e85 创建 service , endpoint root@ubuntu:~# openstack role add --project service --user swift admin root@ubuntu:~# openstack service create --name swift \\ > --description \"OpenStack Object Storage\" object-store +-------------|----------------------------------+ | Field | Value | +-------------|----------------------------------+ | description | OpenStack Object Storage | | enabled | True | | id | 25dfd6435b4245a9991f6a0e0c5f609c | | name | swift | | type | object-store | +-------------|----------------------------------+ root@ubuntu:~# openstack endpoint create --region RegionOne \\ > object-store public http://controller:8080/v1/AUTH_%\\(project_id\\)s +--------------|-----------------------------------------------+ | Field | Value | +--------------|-----------------------------------------------+ | enabled | True | | id | 4327c259c9a843e5b0b9dff1140a1e72 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 25dfd6435b4245a9991f6a0e0c5f609c | | service_name | swift | | service_type | object-store | | url | http://controller:8080/v1/AUTH_%(project_id)s | +--------------|-----------------------------------------------+ root@ubuntu:~# openstack endpoint create --region RegionOne \\ > object-store internal http://controller:8080/v1/AUTH_%\\(project_id\\)s +--------------|-----------------------------------------------+ | Field | Value | +--------------|-----------------------------------------------+ | enabled | True | | id | b87a66d75ea242ef97c1969c0deaff57 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 25dfd6435b4245a9991f6a0e0c5f609c | | service_name | swift | | service_type | object-store | | url | http://controller:8080/v1/AUTH_%(project_id)s | +--------------|-----------------------------------------------+ root@ubuntu:~# openstack endpoint create --region RegionOne \\ > object-store admin http://controller:8080/v1 +--------------|----------------------------------+ | Field | Value | +--------------|----------------------------------+ | enabled | True | | id | 8eba4b8e86654a999c1cd8fe27f94735 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 25dfd6435b4245a9991f6a0e0c5f609c | | service_name | swift | | service_type | object-store | | url | http://controller:8080/v1 | +--------------|----------------------------------+ root@ubuntu:~# openstack endpoint list +----------------------------------|-----------|--------------|--------------|---------|-----------|-----------------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------|-----------|--------------|--------------|---------|-----------|-----------------------------------------------+ | 3424fdd10aa047418502f4fe92f1f10c | RegionOne | keystone | identity | True | public | http://192.168.10.12:5000/v3/ | | 4327c259c9a843e5b0b9dff1140a1e72 | RegionOne | swift | object-store | True | public | http://controller:8080/v1/AUTH_%(project_id)s | | 698a2a4b64b74e5ea289c21296e36303 | RegionOne | keystone | identity | True | admin | http://192.168.10.12:5000/v3/ | | 8eba4b8e86654a999c1cd8fe27f94735 | RegionOne | swift | object-store | True | admin | http://controller:8080/v1 | | b87a66d75ea242ef97c1969c0deaff57 | RegionOne | swift | object-store | True | internal | http://controller:8080/v1/AUTH_%(project_id)s | | d66b0bd887d141c49a166c2426633cb8 | RegionOne | keystone | identity | True | internal | http://192.168.10.12:5000/v3/ | +----------------------------------|-----------|--------------|--------------|---------|-----------|-----------------------------------------------+ 重启服务 root@ubuntu:~# chown -R root:swift /etc/swift root@ubuntu:~# service swift-proxy restart root@ubuntu:~# service memcached restart root@ubuntu:~# netstat -ntlp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 7904/sshd tcp 0 0 127.0.0.1:7001 0.0.0.0:* LISTEN 11437/etcd tcp 0 0 127.0.0.1:6010 0.0.0.0:* LISTEN 15627/0 tcp 0 0 127.0.0.1:4001 0.0.0.0:* LISTEN 11437/etcd tcp 0 0 0.0.0.0:25672 0.0.0.0:* LISTEN 9901/beam.smp tcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTEN 8792/mysqld tcp 0 0 0.0.0.0:11211 0.0.0.0:* LISTEN 17294/memcached tcp 0 0 127.0.0.1:2379 0.0.0.0:* LISTEN 11437/etcd tcp 0 0 127.0.0.1:2380 0.0.0.0:* LISTEN 11437/etcd tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 17265/python tcp 0 0 0.0.0.0:4369 0.0.0.0:* LISTEN 9788/epmd tcp6 0 0 :::22 :::* LISTEN 7904/sshd tcp6 0 0 ::1:6010 :::* LISTEN 15627/0 tcp6 0 0 :::5000 :::* LISTEN 13217/apache2 tcp6 0 0 :::5672 :::* LISTEN 9901/beam.smp tcp6 0 0 :::80 :::* LISTEN 13217/apache2 tcp6 0 0 :::4369 :::* LISTEN 9788/epmd root@ubuntu:~# . admin-openrc root@ubuntu:~# swift list 用原来 192.168.10.13 的 demo 用户 openrc 文件，已经不可用。 root@ubuntu:~# . demo-openrc root@ubuntu:~# swift list Unauthorized. Check username/id, password, tenant name/id and user/tenant domain name/id. root@ubuntu:~# 创建新的 demo 用户openrc 文件。 root@ubuntu:~# cp demo-openrc demo-openrc.12 root@ubuntu:~# vi demo-openrc.12 root@ubuntu:~# cat demo-openrc.12 export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=demo export OS_USERNAME=demo export OS_PASSWORD=openstack export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 root@ubuntu:~# . demo-openrc.12 root@ubuntu:~# swift list root@ubuntu:~# 好了，现在可以使用了。 这样，全新的一个 proxy node 节点就搭建好了。 ref https://docs.openstack.org/swift/queens/install/controller-install-ubuntu.html https://eiuapp.github.io/swift-handbook/docs/swift-multy-proxy-node-conf.html Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/analysis-openstack-swift-keystone-authentication.html":{"url":"docs/analysis-openstack-swift-keystone-authentication.html","title":"与keystone结合","keywords":"","body":"+++ title = \"SWIFt 通过 keystone 认证，并实现调用swift接口\" date = 2018-11-23T00:00:00-08:00 lastmod = 2019-01-18T02:11:47-08:00 tags = [\"openstack\", \"swift\"] categories = [\"openstack\"] draft = false weight = 3001 +++ 总体 安装 keystone 认证 调用swift接口 向keystone发送请求。得到X-Auth-Token 带X-Auth-Token向swift服务发出请求 env controller node 上安装了 keystone, swift-proxy 服务。 storage node 安装 account, container, object 服务。 ip: controller node: 192.168.0.50 storage node: 192.168.0.198 192.168.0.134 192.168.0.135 安装 keystone 认证 swift 与 keystone 结合的配置文档在 https://docs.openstack.org/swift/latest/overview%5Fauth.html 文档最后 https://docs.openstack.org/swift/latest/overview%5Fauth.html#troubleshooting-tips-for-keystoneauth-deployment 指出了一个验证方式 swift --os-auth-url https://api.example.com/v3 --auth-version 3\\ --os-project-name project1 --os-project-domain-name domain1 \\ --os-username user --os-user-domain-name domain1 \\ --os-password password list 当然这个是按照配置来的。因为，我这里面swift对应的password是 openstack, 所以，我这里的验证如下： root@controller:~# openstack --os-identity-api-version=3 --os-auth-url=http://keystonehost:5000/ --os-username=swift --os-user-domain-id=default --os-project-name=service --os-project-domain-id=default --os-password=openstack catalog show object-store +-----------|-----------------------------------------------------------------------------+ | Field | Value | +-----------|-----------------------------------------------------------------------------+ | endpoints | RegionOne | | | public: http://controller:8080/v1/AUTH_a640c74e595c44c4902d1c5ebc3afa8a | | | RegionOne | | | admin: http://controller:8080/v1 | | | RegionOne | | | internal: http://controller:8080/v1/AUTH_a640c74e595c44c4902d1c5ebc3afa8a | | | | | id | e929673efa1a4acb9adc4a06e4f56a31 | | name | swift | | type | object-store | +-----------|-----------------------------------------------------------------------------+ root@controller:~# 这里看出，配置是没有问题的。 调用swift接口 准备工作 endpoint root@controller:~# openstack endpoint list Missing value auth-url required for auth plugin password root@controller:~# . admin-openrc root@controller:~# openstack endpoint list +----------------------------------|-----------|--------------|--------------|---------|-----------|-----------------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------|-----------|--------------|--------------|---------|-----------|-----------------------------------------------+ | 014c4ed5c42042c394d62a1194bf07ce | RegionOne | keystone | identity | True | internal | http://controller:5000/v3/ | | 0b3dc5788cac4f2cb66edc3efacf10c0 | RegionOne | keystone | identity | True | public | http://controller:5000/v3/ | | 19e727c1668c4e8aae4315e13057fbbd | RegionOne | cinderv2 | volumev2 | True | public | http://controller:8776/v2/%(project_id)s | | 216d8d8f353a49f78aa4421ff6e8c27b | RegionOne | swift | object-store | True | public | http://controller:8080/v1/AUTH_%(project_id)s | | 2548787f06524287b6a27d0a562da375 | RegionOne | glance | image | True | internal | http://controller:9292 | | 295cfee33ff04ab7890a84b47e95bc3f | RegionOne | keystone | identity | True | admin | http://controller:5000/v3/ | | 50d3b61b60654a65bda26584b4fe0896 | RegionOne | neutron | network | True | admin | http://controller:9696 | | 55367bdc8db6438da3a4ed82b1a1b04c | RegionOne | glance | image | True | public | http://controller:9292 | | 5b60dcd8e374410f83e675201d76f06f | RegionOne | placement | placement | True | admin | http://controller:8778 | | 610f89fa29764f7eb5c91e737ad0110e | RegionOne | swift | object-store | True | admin | http://controller:8080/v1 | | 76d61f360fc140a58b4258c344ea9ae5 | RegionOne | nova | compute | True | internal | http://controller:8774/v2.1 | | 7eece3971f9f4105b031214666940d54 | RegionOne | placement | placement | True | public | http://controller:8778 | | 8237211a85314e0c914ea34851e324b7 | RegionOne | cinderv2 | volumev2 | True | admin | http://controller:8776/v2/%(project_id)s | | 99b6673fda7f48bb8b6ae11a87ba0e1b | RegionOne | glance | image | True | admin | http://controller:9292 | | a39982358d3041e0aebd7a1a1fb691bb | RegionOne | swift | object-store | True | internal | http://controller:8080/v1/AUTH_%(project_id)s | | a61cbe1448284dd482725fb3067fa25c | RegionOne | nova | compute | True | public | http://controller:8774/v2.1 | | cfb1df03d1bd42719dd506e0fbee7e5a | RegionOne | cinderv3 | volumev3 | True | internal | http://controller:8776/v3/%(project_id)s | | cfe6b374e86048c98951d079a3cdaa42 | RegionOne | placement | placement | True | internal | http://controller:8778 | | d9bb0785c23143ed855d0bb74dfe53bb | RegionOne | cinderv3 | volumev3 | True | public | http://controller:8776/v3/%(project_id)s | | e1f98d0e79f549a5bc46bcf05705c4b1 | RegionOne | neutron | network | True | public | http://controller:9696 | | f089ebe1cb5040319e942dbc607e9930 | RegionOne | cinderv3 | volumev3 | True | admin | http://controller:8776/v3/%(project_id)s | | f3350a1d66fa4813a091beef3782b6fd | RegionOne | neutron | network | True | internal | http://controller:9696 | | f5460d92ef8b428b9dc354628acdb289 | RegionOne | cinderv2 | volumev2 | True | internal | http://controller:8776/v2/%(project_id)s | | fb5c30679ed14e078646041e75e77294 | RegionOne | nova | compute | True | admin | http://controller:8774/v2.1 | +----------------------------------|-----------|--------------|--------------|---------|-----------|-----------------------------------------------+ root@controller:~# 从 openstack endpoint 拿到 swift 的 URL 是： http://controller:8080/v1 试验swift 状态 root@controller:~# swift stat -v StorageURL: http://controller:8080/v1/AUTH_f04ec0abf3d1460dad82608bb03af589 Auth Token: gAAAAABcQUnrEMgo3fTeFVMWrYcA6-6t6XNAGljTzPNdH111cbawcBjozo0DUvHhsOY4hucyfzhBAzwMlzsigyj1jOqdQndn_6cO5k8JTrQtyYGzLxJ5weEdHnV6HVszgQBtyxq17ut1_RTu_DhVT-m5MLKpMGcglszF-XCcC-Ua9I2NHTLicCg Account: AUTH_f04ec0abf3d1460dad82608bb03af589 Containers: 1 Objects: 1 Bytes: 26262280 Containers in policy \"policy-0\": 1 Objects in policy \"policy-0\": 1 Bytes in policy \"policy-0\": 26262280 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: tx753c958eb6ad4bd5b2ef0-005c4149eb X-Timestamp: 1547641627.86782 X-Trans-Id: tx753c958eb6ad4bd5b2ef0-005c4149eb Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# 从 swift stat -v 的返回可以看到 Auth Token 且状态正常。 所以，是可以使用 curl, python 通过RESTFUL API调用swift服务的。 通过swift命令，获得请求参数 root@controller:~# swift stat -v StorageURL: http://controller:8080/v1/AUTH_f04ec0abf3d1460dad82608bb03af589 Auth Token: gAAAAABcQUnrEMgo3fTeFVMWrYcA6-6t6XNAGljTzPNdH111cbawcBjozo0DUvHhsOY4hucyfzhBAzwMlzsigyj1jOqdQndn_6cO5k8JTrQtyYGzLxJ5weEdHnV6HVszgQBtyxq17ut1_RTu_DhVT-m5MLKpMGcglszF-XCcC-Ua9I2NHTLicCg Account: AUTH_f04ec0abf3d1460dad82608bb03af589 Containers: 1 Objects: 1 Bytes: 26262280 Containers in policy \"policy-0\": 1 Objects in policy \"policy-0\": 1 Bytes in policy \"policy-0\": 26262280 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: tx753c958eb6ad4bd5b2ef0-005c4149eb X-Timestamp: 1547641627.86782 X-Trans-Id: tx753c958eb6ad4bd5b2ef0-005c4149eb Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# swift --help usage: swift [--version] [--help] [--os-help] [--snet] [--verbose] Command-line interface to the OpenStack Swift API. Positional arguments: Examples: swift download --help swift -A https://api.example.com/v1.0 \\ -U user -K api_key stat -v swift --os-auth-url https://api.example.com/v2.0 \\ --os-tenant-name tenant \\ --os-username user --os-password password list swift --os-auth-url https://api.example.com/v3 --auth-version 3\\ --os-project-name project1 --os-project-domain-name domain1 \\ --os-username user --os-user-domain-name domain1 \\ --os-password password list swift --os-auth-url https://api.example.com/v3 --auth-version 3\\ --os-project-id 0123456789abcdef0123456789abcdef \\ --os-user-id abcdef0123456789abcdef0123456789 \\ --os-password password list swift --os-auth-token 6ee5eb33efad4e45ab46806eac010566 \\ --os-storage-url https://10.1.5.2:8080/v1/AUTH_ced809b6a4baea7aeab61a \\ list swift list --lh optional arguments: root@controller:~# 通过这个example 可以看到，原来之前的 swift stat -v 就是因为使用了 . admin-openrc 中的参数。 因为，后续，我们是不会使用 admin 这个帐号的，所以，现在起我们切换成 demo 帐号。 root@controller:~# . demo-openrc root@controller:~# openstack endpoint list You are not authorized to perform the requested action: identity:list_endpoints. (HTTP 403) (Request-ID: req-b7efc3d8-0046-4847-b4ed-2e99d53d9e35) root@controller:~# swift stat -v StorageURL: http://controller:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 Auth Token: gAAAAABcQUz9FSJBIUSo2DC5sOCFLcytonKVdX6MOf6m9KY4-IcGlTztCZkULlkUKhaiRzCdYKJeNE06MblaIjArbb-VASeMelaKL3SBRWTdRn2Qb3TygrmRt3CFuU2bRQGitjuxp4m8WY7hYroL97cSyrDKTpCHKYhfz9X05F2yFlQ-gLZqC7w Account: AUTH_7d6eaa90d74a4f239963933c3a744df3 Containers: 6 Objects: 29 Bytes: 7319499838 Containers in policy \"policy-0\": 6 Objects in policy \"policy-0\": 29 Bytes in policy \"policy-0\": 7319499838 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: tx85bf81afca4b40bdbd431-005c414cfd X-Timestamp: 1546928904.74292 X-Trans-Id: tx85bf81afca4b40bdbd431-005c414cfd Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# 从上面的结果看到，demo 用户是不能访问 endpoint 的。但是，可以访问到 swift 服务。 构造 swift example 看一下 demo-openrc 中的内容。 root@controller:~# cat demo-openrc export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=demo export OS_USERNAME=demo export OS_PASSWORD=openstack export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 root@controller:~# 按 swift example 的方式配置如下。 注意了：下面的这个命令，是可以不预先运行`. demo-openrc`命令的。 ubuntu@controller:~$ swift --os-auth-url http://controller:5000/v3 --auth-version 3\\ > --os-project-name demo --os-project-domain-name Default \\ > --os-username demo --os-user-domain-name Default \\ > --os-password openstack list container1 container1_segments container2 jinweilai-work pub1 test3 ubuntu@controller:~$ 如果像下面的这样，把project 的信息去除，则会报错 ubuntu@controller:~$ swift --os-auth-url http://controller:5000/v3 --auth-version 3\\ > --os-username demo --os-user-domain-name Default \\ > --os-password openstack list No project name or project id specified. ubuntu@controller:~$ 说明每一个地方的设置，都要加到请求参数中去。 也就是这个 demo-openrc 是有scope 的。 分析swift example 那么，上面的命令是如何实现查看出 demo 这个帐号的 container 信息的呢？ 根据文档知道，是先向 keystone 拿到 X-Auth-Token, 然后拿上这个X-Auth-Token 向swift 请求的。 那这样子说，我们可以用 `swift stat -v` 中的 `Auth Token` 直接向swift请求，试一下，是不是有效。（其实通过keystone日志/var/log/keystone/keystone-wsgi-public.log 和 swift日志（存放在系统日志）/var/log/syslog , 可以知道，就是这样的。） 让我们试一下。 在 controller node 中 使用 `swift stat -v`再来一次。 root@controller:~# swift stat -v StorageURL: http://controller:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 Auth Token: gAAAAABcQVk7Q6QgnbTmTobnt36rLZmifbCMow4W-MWfQ9txcRyHEYOPXWRKAcc2r7bAVG0uS_VNC5GyIPw6FjQx3Bb-mofESZDEPs5AHe8m2Pg1Nwfmhrd8lg_4VqWZRffUQIrrRiNH1JSViBXFRZJn0zwSdwUREoskIuetQ0uZ5FWXuQOz240 Account: AUTH_7d6eaa90d74a4f239963933c3a744df3 Containers: 6 Objects: 29 Bytes: 7319499838 Containers in policy \"policy-0\": 6 Objects in policy \"policy-0\": 29 Bytes in policy \"policy-0\": 7319499838 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: txb40ee72900e8487a8e5ec-005c41593b X-Timestamp: 1546928904.74292 X-Trans-Id: txb40ee72900e8487a8e5ec-005c41593b Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# 然后根据 https://developer.openstack.org/api-ref/object-store/index.html?expanded=show-account-details-and-list-containers-detail#show-container-details-and-list-objects 去任何一台公网机器上 ➜ swift curl -i \"http://192.168.0.50:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3?format=json\" -X GET -H \"X-Auth-Token: gAAAAABcQVk7Q6QgnbTmTobnt36rLZmifbCMow4W-MWfQ9txcRyHEYOPXWRKAcc2r7bAVG0uS_VNC5GyIPw6FjQx3Bb-mofESZDEPs5AHe8m2Pg1Nwfmhrd8lg_4VqWZRffUQIrrRiNH1JSViBXFRZJn0zwSdwUREoskIuetQ0uZ5FWXuQOz240\" HTTP/1.1 200 OK Content-Length: 337 X-Account-Object-Count: 29 X-Account-Storage-Policy-Policy-0-Bytes-Used: 7319499838 X-Account-Storage-Policy-Policy-0-Container-Count: 6 X-Timestamp: 1546928904.74292 X-Account-Storage-Policy-Policy-0-Object-Count: 29 X-Account-Bytes-Used: 7319499838 X-Account-Container-Count: 6 Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes x-account-project-domain-id: default X-Trans-Id: txcd8c653a027b4a5d9a9a0-005c415a1d X-Openstack-Request-Id: txcd8c653a027b4a5d9a9a0-005c415a1d Date: Fri, 18 Jan 2019 04:46:21 GMT [{\"count\": 9, \"bytes\": 506788650, \"name\": \"container1\"}, {\"count\": 4, \"bytes\": 3460191876, \"name\": \"container1_segments\"}, {\"count\": 1, \"bytes\": 18290688, \"name\": \"container2\"}, {\"count\": 1, \"bytes\": 375626779, \"name\": \"jinweilai-work\"}, {\"count\": 1, \"bytes\": 399336, \"name\": \"pub1\"}, {\"count\": 13, \"bytes\": 2958202509, \"name\": \"test3\"}]% ➜ swift 得到结果了。哈哈。 那这样子说，我们只要得到正确的`X-Auth-Token`，就一定能调用swift服务了。 下面就是依据我们现有的材料，向keystone发送请求, 得到我们要的 X-Auth-Token 了。 向keystone发送请求。得到X-Auth-Token 根据下面文档 https://developer.openstack.org/api-ref/identity/v3/index.html?expanded=password-authentication-with-scoped-authorization-detail#identity-api-operations 结合我们 demo-openrc 和 刚刚的 swift example, 我们得到 生成JSON格式请求body: { \"auth\": { \"identity\": { \"methods\": [ \"password\" ], \"password\": { \"user\": { \"name\": \"demo\", \"domain\": { \"name\": \"Default\" }, \"password\": \"openstack\" } } }, \"scope\": { \"project\": { \"domain\": { \"name\": \"Default\" }, \"name\": \"demo\" } } } } 向 keystone 的 endpoint | 0b3dc5788cac4f2cb66edc3efacf10c0 | RegionOne | keystone | identity | True | public | http://controller:5000/v3/ | URL: http://controller:5000/v3/ 发送请求。得到response. 其中响应 headers 中的 'X-Subject-Token', 就是 X-Auth-Token 了。 带X-Auth-Token向swift服务发出请求 这一步就是重复刚刚讲过的内容，向swift请求了. 然后根据 https://developer.openstack.org/api-ref/object-store/index.html?expanded=show-account-details-and-list-containers-detail#show-container-details-and-list-objects 去任何一台公网机器上 ➜ swift curl -i \"http://192.168.0.50:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3?format=json\" -X GET -H \"X-Auth-Token: gAAAAABcQVk7Q6QgnbTmTobnt36rLZmifbCMow4W-MWfQ9txcRyHEYOPXWRKAcc2r7bAVG0uS_VNC5GyIPw6FjQx3Bb-mofESZDEPs5AHe8m2Pg1Nwfmhrd8lg_4VqWZRffUQIrrRiNH1JSViBXFRZJn0zwSdwUREoskIuetQ0uZ5FWXuQOz240\" HTTP/1.1 200 OK Content-Length: 337 X-Account-Object-Count: 29 X-Account-Storage-Policy-Policy-0-Bytes-Used: 7319499838 X-Account-Storage-Policy-Policy-0-Container-Count: 6 X-Timestamp: 1546928904.74292 X-Account-Storage-Policy-Policy-0-Object-Count: 29 X-Account-Bytes-Used: 7319499838 X-Account-Container-Count: 6 Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes x-account-project-domain-id: default X-Trans-Id: txcd8c653a027b4a5d9a9a0-005c415a1d X-Openstack-Request-Id: txcd8c653a027b4a5d9a9a0-005c415a1d Date: Fri, 18 Jan 2019 04:46:21 GMT [{\"count\": 9, \"bytes\": 506788650, \"name\": \"container1\"}, {\"count\": 4, \"bytes\": 3460191876, \"name\": \"container1_segments\"}, {\"count\": 1, \"bytes\": 18290688, \"name\": \"container2\"}, {\"count\": 1, \"bytes\": 375626779, \"name\": \"jinweilai-work\"}, {\"count\": 1, \"bytes\": 399336, \"name\": \"pub1\"}, {\"count\": 13, \"bytes\": 2958202509, \"name\": \"test3\"}]% ➜ swift 得到结果了。哈哈。 利用 python 来访问swift 直接上代码了。 import requests import json URL = 'http://192.168.0.50:5000/v3/auth/tokens' body = { \"auth\": { \"identity\": { \"methods\": [ \"password\" ], \"password\": { \"user\": { \"name\": \"demo\", \"domain\": { \"name\": \"Default\" }, \"password\": \"openstack\" } } }, \"scope\": { \"project\": { \"domain\": { \"name\": \"Default\" }, \"name\": \"demo\" } } } } body = json.dumps(body) print(body) headers = {'Content-Type':'application/json'} res = requests.post(URL,data=body,headers=headers) token = res.headers['X-Subject-Token'] print(token) URL = 'http://192.168.0.50:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3' headers = {'X-Auth-Token':token,\"Content-Type\": 'application/json'} res = requests.get(URL,headers=headers) print(res.text) print(res.status_code) Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-keystone-auth-with-jinweilai-openstack-dev-env2.html":{"url":"docs/swift-keystone-auth-with-jinweilai-openstack-dev-env2.html","title":"让swift通过keystone认证","keywords":"","body":"让 openstack dev 2 环境中的swift 通过 keystone 认证 env 系统：Ubuntu Server 16.04×64 存储设置：4T 架构部署: 主机名 IP 作用 Proxy 192.168.0.51 代理节点, controller node object1 192.168.0.127 存储节点1(zone1) object2 192.168.0.134 存储节点2(zone1) object3 192.168.0.135 存储节点3(zone1) object4 192.168.0.180 存储节点4(zone1) object5 192.168.0.189 存储节点5(zone1) 增加代理节点 Proxybak 192.168.0.141 代理节点, 做冗余备份 step 配置 只需要把下面的文本放至 /etc/keystone/default_catalog.templates 末尾 catalog.RegionOne.object_store.name = Swift Service catalog.RegionOne.object_store.publicURL = http://swiftproxy:8080/v1/AUTH_$(tenant_id)s catalog.RegionOne.object_store.adminURL = http://swiftproxy:8080/ catalog.RegionOne.object_store.internalURL = http://swiftproxy:8080/v1/AUTH_$(tenant_id)s 重启 keystone service apache2 restart 检查 root@controller:~# cat demo-openrc export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=demo export OS_USERNAME=demo export OS_PASSWORD=openstack export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 root@controller:~# . demo-openrc root@controller:~# swift list container1 container2 root@controller:~# ref https://docs.openstack.org/swift/latest/overview_auth.html Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/analysis-openstack-swift-overview-ring.html":{"url":"docs/analysis-openstack-swift-overview-ring.html","title":"overview ring","keywords":"","body":"+++ title = \"openstack swift overview ring\" date = 2019-01-20T00:00:00-08:00 lastmod = 2019-01-21T18:09:25-08:00 tags = [\"swift\"] categories = [\"swift\"] draft = false weight = 3002 +++ list of devices https://docs.openstack.org/swift/queens/overview%5Fring.html#list-of-devices 这里的 list of devices 可以通过下面方式，查看得到。 root@controller:~/tmp2# cp /etc/swift/account.ring.gz . root@controller:~/tmp2# gzip -d ./account.ring.gz gzip: ./account.ring already exists; do you wish to overwrite (y or n)? y root@controller:~/tmp2# tail account.ring R1NG\"byteorder\": \"little\", \"devs\": [{\"device\": \"sdb\", \"id\": 0, \"ip\": \"192.168.0.134\", \"meta\": \"\", \"port\": 6202, \"region\": 1, \"replication_ip\": \"192.168.0.134\", \"replication_port\": 6202, \"weight\": 100.0, \"zone\": 1}, null, {\"device\": \"sdb\", \"id\": 2, \"ip\": \"192.168.0.127\", \"meta\": \"\", \"port\": 6202, \"region\": 1, \"replication_ip\": \"192.168.0.127\", \"replication_port\": 6202, \"weight\": 100.0, \"zone\": 1}, {\"device\": \"sdb\", \"id\": 3, \"ip\": \"192.168.0.198\", \"meta\": \"\", \"port\": 6202, \"region\": 1, \"replication_ip\": \"192.168.0.198\", \"replication_port\": 6202, \"weight\": 100.0, \"zone\": 1}, {\"device\": \"sdb\", \"id\": 4, \"ip\": \"192.168.0.180\", \"meta\": \"\", \"port\": 6202, \"region\": 1, \"replication_ip\": \"192.168.0.180\", \"replication_port\": 6202, \"weight\": 100.0, \"zone\": 1}, {\"device\": \"sdb\", \"id\": 5, \"ip\": \"192.168.0.135\", \"meta\": \"\", \"port\": 6202, \"region\": 1, \"replication_ip\": \"192.168.0.135\", \"replication_port\": 6202, \"weight\": 100.0, \"zone\": 1}], \"part_shift\": 22, \"replica_count\": 3}root@controller:~/tmp2# R1NG{\"byteorder\": \"little\", \"devs\": [{\"device\": \"sdb\", \"id\": 0, \"ip\": \"192.168.0.134\", \"meta\": \"\", \"port\": 6202, \"region\": 1, \"replication_ip\": \"192.168.0.134\", \"replication_port\": 6202, \"weight\": 100.0, \"zone\": 1}, null, {\"device\": \"sdb\", \"id\": 2, \"ip\": \"192.168.0.127\", \"meta\": \"\", \"port\": 6202, \"region\": 1, \"replication_ip\": \"192.168.0.127\", \"replication_port\": 6202, \"weight\": 100.0, \"zone\": 1}, {\"device\": \"sdb\", \"id\": 3, \"ip\": \"192.168.0.198\", \"meta\": \"\", \"port\": 6202, \"region\": 1, \"replication_ip\": \"192.168.0.198\", \"replication_port\": 6202, \"weight\": 100.0, \"zone\": 1}, {\"device\": \"sdb\", \"id\": 4, \"ip\": \"192.168.0.180\", \"meta\": \"\", \"port\": 6202, \"region\": 1, \"replication_ip\": \"192.168.0.180\", \"replication_port\": 6202, \"weight\": 100.0, \"zone\": 1}, {\"device\": \"sdb\", \"id\": 5, \"ip\": \"192.168.0.135\", \"meta\": \"\", \"port\": 6202, \"region\": 1, \"replication_ip\": \"192.168.0.135\", \"replication_port\": 6202, \"weight\": 100.0, \"zone\": 1}], \"part_shift\": 22, \"replica_count\": 3}root@controller:~/tmp2# root@controller:~/tmp2# 格式化一下json，如下 { \"byteorder\": \"little\", \"devs\": [{ \"device\": \"sdb\", \"id\": 0, \"ip\": \"192.168.0.134\", \"meta\": \"\", \"port\": 6202, \"region\": 1, \"replication_ip\": \"192.168.0.134\", \"replication_port\": 6202, \"weight\": 100.0, \"zone\": 1 }, null, { \"device\": \"sdb\", \"id\": 2, \"ip\": \"192.168.0.127\", \"meta\": \"\", \"port\": 6202, \"region\": 1, \"replication_ip\": \"192.168.0.127\", \"replication_port\": 6202, \"weight\": 100.0, \"zone\": 1 }, { \"device\": \"sdb\", \"id\": 3, \"ip\": \"192.168.0.198\", \"meta\": \"\", \"port\": 6202, \"region\": 1, \"replication_ip\": \"192.168.0.198\", \"replication_port\": 6202, \"weight\": 100.0, \"zone\": 1 }, { \"device\": \"sdb\", \"id\": 4, \"ip\": \"192.168.0.180\", \"meta\": \"\", \"port\": 6202, \"region\": 1, \"replication_ip\": \"192.168.0.180\", \"replication_port\": 6202, \"weight\": 100.0, \"zone\": 1 }, { \"device\": \"sdb\", \"id\": 5, \"ip\": \"192.168.0.135\", \"meta\": \"\", \"port\": 6202, \"region\": 1, \"replication_ip\": \"192.168.0.135\", \"replication_port\": 6202, \"weight\": 100.0, \"zone\": 1 }], \"part_shift\": 22, \"replica_count\": 3 } building the ring https://docs.openstack.org/swift/queens/overview%5Fring.html#building-the-ring First the ring builder calculates the replicanths wanted at each tier in the ring’s topology based on weight. Then the ring builder calculates the replicanths wanted at each tier in the ring’s topology based on dispersion. Then the ring builder calculates the maximum deviation on a single device between its weighted replicanths and wanted replicanths. Next we interpolate between the two replicanth values (weighted & wanted) at each tier using the specified overload (up to the maximum required overload). It’s a linear interpolation, similar to solving for a point on a line between two points - we calculate the slope across the max required overload and then calculate the intersection of the line with the desired overload. This becomes the target. From the target we calculate the minimum and maximum number of replicas any partition may have in a tier. This becomes the replica-plan. Finally, we calculate the number of partitions that should ideally be assigned to each device based the replica-plan. On initial balance (i.e., the first time partitions are placed to generate a ring) we must assign each replica of each partition to the device that desires the most partitions excluding any devices that already have their maximum number of replicas of that partition assigned to some parent tier of that device’s failure domain. When building a new ring based on an old ring, the desired number of partitions each device wants is recalculated from the current replica-plan. Next the partitions to be reassigned are gathered up. Any removed devices have all their assigned partitions unassigned and added to the gathered list. Any partition replicas that (due to the addition of new devices) can be spread out for better durability are unassigned and added to the gathered list. Any devices that have more partitions than they now desire have random partitions unassigned from them and added to the gathered list. Lastly, the gathered partitions are then reassigned to devices using a similar method as in the initial assignment described above. Whenever a partition has a replica reassigned, the time of the reassignment is recorded. This is taken into account when gathering partitions to reassign so that no partition is moved twice in a configurable amount of time. This configurable amount of time is known internally to the RingBuilder class as min_part_hours. This restriction is ignored for replicas of partitions on devices that have been removed, as device removal should only happens on device failure and there’s no choice but to make a reassignment. The above processes don’t always perfectly rebalance a ring due to the random nature of gathering partitions for reassignment. To help reach a more balanced ring, the rebalance process is repeated a fixed number of times until the replica-plan is fulfilled or unable to be fulfilled (indicating we probably can’t get perfect balance due to too many partitions recently moved). 翻译如下： 首先，ring builder 根据weight计算 环的拓扑中每层所需的 replicanths。 然后，ring builder 根据dispersion计算 环形拓扑中每层所需的 replicanths。 然后，ring builder 计算单个设备在其weighted replicanths和wanted replicanths之间的最大偏差。 接下来，我们使用指定的过载（直到最大所需的过载）在每层的两个replicanth值（weighted & wanted）之间进行插值。这是一个线性插值，类似于求解两点之间的直线上的点 - 我们计算max required overload的斜率，然后计算该线与desired overload的交点。这成为目标。 从目标, 我们计算 在层中的 any partition 可能具有的最小和最大replicas数。这成为replica-plan。 Finally, we calculate the number of partitions that should ideally be assigned to each device based the replica-plan. On initial balance（即，第一次放置分区以生成环）时，我们必须将每个分区的每个副本分配给需要最多分区的设备，不包括已分配给某些分区的最大分区副本数的设备该设备的故障域的父层。 当基于旧环而建立新环时，从当前replica-plan重新计算每个设备所需的分区数。接下来，将收集要重新分配的分区。任何已删除的设备都会将所有已分配的分区取消分配并添加到gathered list中。任何分区副本（由于添加了新设备）可以被分散以获得更好的持久性，这些副本是未分配的，会添加到gathered list中。任何具有比现在更多分区的设备都会从它们中取消分配随机分区，并添加到gathered list中。最后，使用与上述初始分配中类似的方法，将收集的分区重新分配给设备。 每当分区重新分配副本时，都会记录重新分配的时间。在收集分区以重新分配时会考虑这一点，以便在可配置的时间内不会移动分区两次。这个可配置的时间量在RingBuilder类内部称为min_part_hours。对于已删除的设备上的分区副本，将忽略此限制，因为设备删除应仅在设备故障时发生，并且除了进行重新分配之外别无选择。 由于收集分区用于重新分配的随机性质，上述过程并不总是能完美地rebalance a ring。为了帮助reach a more balanced ring，重新平衡过程重复固定次数，until replica-plan is fulfilled or unable to be fulfilled（表明, 由于最近移动的分区太多，我们可能无法获得 perfect balance）。 这里是对 Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-what-is-a-swift-partition.html":{"url":"docs/swift-what-is-a-swift-partition.html","title":"what is a swift partition","keywords":"","body":"What is a Swift partition? step Things in Swift are divided up into partitions; you have 2^P partitions, where P is the part power specified at ring creation time. The partition of an object is the first P bits of a hash of its name. Here's an example: Let's say you've got an object named \"kitten.jpg\" in a container \"cats\" in your Swift account \"MY_account\", and your object ring has a part power of 16. Get the secret prefix and suffix (swift_hash_path_prefix and swift_hash_path_suffix) from /etc/swift/swift.conf. Let's say the prefix is \"SEC\" and the suffix is \"RET\". Take the object's full name, prepend the prefix, and append the suffix, so you get SEC/MY_account/cats/kitten.jpgRET. Compute the MD5 hash of that string to get 4f3146f77835ff33a6baba807f7f5c1, then take the first 16 bits to get 4f31, or in decimal, 20273. With that partition number, you can then look in the ring to see which drives hold replicas of that partition, and then contact the relevant nodes to manipulate the object (GET, PUT, DELETE, etc.) This is exactly what the Swift proxy server does to determine which backend(s) to contact to service a request. ref https://ask.openstack.org/en/question/6766/what-is-a-swift-partition/ Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/openstack-swift-analysis-partition.html":{"url":"docs/openstack-swift-analysis-partition.html","title":"ring如何生成partition","keywords":"","body":"ring 生成 partition 分析 env 系统：Ubuntu Server 16.04×64 存储设置：4T python-swiftclient 3.5.0 架构部署: 主机名 IP 作用 keystone 192.168.100.50 keystone node Proxy 192.168.100.50/192.168.0.50 代理节点 object1 192.168.100.105 存储节点1(zone1) object2 192.168.100.106 存储节点2(zone1) object3 192.168.100.107 存储节点3(zone1) object4 192.168.100.107 存储节点4(zone1) step 网上示例 通过 https://ask.openstack.org/en/question/6766/what-is-a-swift-partition/ 知道： SEC/MY_account/cats/kitten.jpgRET => 4f3146f77835ff33a6baba807f7f5c10 => 4f31 SEC/MY_account/cats/kitten.jpgRET => 4f3146f77835ff33a6baba807f7f5c10 这一步，放到网上md5加密就能得到了。 4f3146f77835ff33a6baba807f7f5c10 => 4f31 4f31 => 20273 这一步，放到网上16进制转10进制就能得到了。 那么我们这个版本的是如何生成 partition 的呢？ 现象 从下面的现象，我们知道，当传入 account= \"AUTH_7d6eaa90d74a4f239963933c3a744df3\" container = \"test4\" obj = \"object-a.txt\" 最后得到的 partition 值是 119 怎么得到下面的 119 呢？ 本质 直接看源码。 源码位置 在 python 安装包的路径下。我这里是 root@controller:/usr/lib/python2.7/dist-packages/swift# ls account cli common container __init__.py __init__.pyc locale obj proxy root@controller:/usr/lib/python2.7/dist-packages/swift# 如果你当前没有安装swift的环境，也没有关系，去github下载。 这里要注意版本。不同版本，源代码当然不同啦, 找与你对应的版本哟! 我的swift是queen版本时安装的。 下载后，有一个，进入，里有还有一个swift文件夹，此文件夹中的内容，与上面我这里的是一致的。 各文件夹是什么作用，这里先不介绍了。 get_part 在 ./common/ring/ring.py 的 333 行，找到 get_part 函数。这个就是我想要的。 def get_part(self, account, container=None, obj=None): \"\"\" Get the partition for an account/container/object. :param account: account name :param container: container name :param obj: object name :returns: the partition number \"\"\" key = hash_path(account, container, obj, raw_digest=True) if time() > self._rtime: self._reload() part = struct.unpack_from('>I', key)[0] >> self._part_shift return part 我们分析一下。里面主要就是 hash_path 和 struct.unpack_from 函数 hash_path 找到源码 common/utils.py 的 2265 行，找到 hash_path 函数 def hash_path(account, container=None, object=None, raw_digest=False): \"\"\" Get the canonical hash for an account/container/object :param account: Account :param container: Container :param object: Object :param raw_digest: If True, return the raw version rather than a hex digest :returns: hash string \"\"\" if object and not container: raise ValueError('container is required if object is provided') paths = [account] if container: paths.append(container) if object: paths.append(object) if raw_digest: return md5(HASH_PATH_PREFIX + '/' + '/'.join(paths) + HASH_PATH_SUFFIX).digest() else: return md5(HASH_PATH_PREFIX + '/' + '/'.join(paths) + HASH_PATH_SUFFIX).hexdigest() 看到了么？我们这里是执行 md5(HASH_PATH_PREFIX + '/' + '/'.join(paths) + HASH_PATH_SUFFIX).digest() 注意到这里有 HASH_PATH_PREFIX = \"openstack\" HASH_PATH_SUFFIX = \"openstack\" 来自于配置文件/etc/swift/swift.conf root@controller:/home/ubuntu# grep swift_hash_path_suffix -rn /etc/swift/swift.conf 9:swift_hash_path_suffix = openstack root@controller:/home/ubuntu# grep swift_hash_path_prefix -rn /etc/swift/swift.conf 10:swift_hash_path_prefix = openstack root@controller:/home/ubuntu# # md5 https://docs.python.org/2/library/md5.html struct.unpack_from struct.unpack_from 的 作用，请看这里 或者 这里 简单说就是： 将字节流转换成python数据类型。Unpack the buffer according to the given format. 我们这里是 struct.unpack_from('>I', key)[0] '>I', 表示转换成python的 unsigned int。 本质的一个示例 ubuntu@controller:~$ python Python 2.7.12 (default, Dec 4 2017, 14:50:18) [GCC 5.4.0 20160609] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import swift >>> from swift.common.utils import hash_path, validate_configuration >>> from hashlib import md5, sha1 >>> import struct >>> HASH_PATH_PREFIX = \"openstack\" >>> HASH_PATH_SUFFIX = \"openstack\" >>> account= \"AUTH_7d6eaa90d74a4f239963933c3a744df3\" >>> container = \"test4\" >>> obj = \"object-a.txt\" >>> paths = [account, container, obj] >>> '/'.join(paths) 'AUTH_7d6eaa90d74a4f239963933c3a744df3/test4/object-a.txt' >>> HASH_PATH_PREFIX + '/' + '/'.join(paths) + HASH_PATH_SUFFIX 'openstack/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4/object-a.txtopenstack' >>> md5(HASH_PATH_PREFIX + '/' + '/'.join(paths) + HASH_PATH_SUFFIX) >>> md5(HASH_PATH_PREFIX + '/' + '/'.join(paths) + HASH_PATH_SUFFIX).digest() '\\x1d\\xf3\\xd1_:\\x9a+x\\x8e75v\\xbc\\xf9v\\x11' >>> key = hash_path(account, container, obj, raw_digest=True) >>> key '\\x1d\\xf3\\xd1_:\\x9a+x\\x8e75v\\xbc\\xf9v\\x11' >>> print(struct.unpack_from('>I', key)) (502518111,) >>> print(struct.unpack_from('>I', key)[0]) 502518111 >>> 502518111 >> 22 119 >>> part = struct.unpack_from('>I', key)[0] >> 22 >>> part 119 >>> exit() 好了，现在基本上知道，119，就是这么来的了。 调试源代码 但是，如果你看是想亲眼见一下，119，怎么办？调试源代码吧。 因为我之前已经上传了 object = \"object-a.txt\" 。不能同名的。所以我现在object = \"object-a-3.txt\" 修改源代码 我这里只是示例。 ./common/ring/ring.py 的 333 行 def get_part(self, account, container=None, obj=None): \"\"\" Get the partition for an account/container/object. :param account: account name :param container: container name :param obj: object name :returns: the partition number \"\"\" key = hash_path(account, container, obj, raw_digest=True) print(\"--------------------------------------------------, my debug --------------------\") print(\"=========keys:\", key) if time() > self._rtime: self._reload() part = struct.unpack_from('>I', key)[0] >> self._part_shift print(\"11111111partition:\", part) return part common/utils.py 的 2265 行 def hash_path(account, container=None, object=None, raw_digest=False): \"\"\" Get the canonical hash for an account/container/object :param account: Account :param container: Container :param object: Object :param raw_digest: If True, return the raw version rather than a hex digest :returns: hash string \"\"\" if object and not container: raise ValueError('container is required if object is provided') paths = [account] if container: paths.append(container) if object: paths.append(object) if raw_digest: print(\"--------------------------------------------------, my debug --------------------\") print(HASH_PATH_PREFIX) print(HASH_PATH_PREFIX + '/' + '/'.join(paths) + HASH_PATH_SUFFIX) print(md5(HASH_PATH_PREFIX + '/' + '/'.join(paths) + HASH_PATH_SUFFIX).digest()) return md5(HASH_PATH_PREFIX + '/' + '/'.join(paths) + HASH_PATH_SUFFIX).digest() else: return md5(HASH_PATH_PREFIX + '/' + '/'.join(paths) + HASH_PATH_SUFFIX).hexdigest() 重启swift-proxy服务 root@controller:/home/ubuntu# service swift-proxy restart root@controller:/home/ubuntu# service swift-proxy status 看日志 root@controller:/home/ubuntu# tail -f /var/log/syslog Jan 30 16:07:44 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: openstack (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: openstack/AUTH_7d6eaa90d74a4f239963933c3a744df3openstack (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: ??70L??023G (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: ('=========keys:', '\\xdf\\xcf\\x8a\\xff\\x9b7\\xc1H0L\\xf4A\\xb2\\x13\\xc1\\x87') (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: ('11111111partition:', 895) (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: - - 30/Jan/2019/08/07/44 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Fformat%3Djson HTTP/1.0 204 - Swift - - - - tx5d49fd7b64cb4e0c89e04-005c515b50 - 0.0047 RL - 1548835664.549806118 1548835664.554487944 - Jan 30 16:07:44 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: openstack (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: openstack/AUTH_7d6eaa90d74a4f239963933c3a744df3openstack (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: ??70L??023G (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: ('=========keys:', '\\xdf\\xcf\\x8a\\xff\\x9b7\\xc1H0L\\xf4A\\xb2\\x13\\xc1\\x87') (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: ('11111111partition:', 895) (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: openstack (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: openstack/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4openstack (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: ?]?14Ν?#023#032#032#001#027??txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: ('=========keys:', '\\xbc]\\xec\\x0c\\xcf]\\x8a\\xb6\\x13\\x1a\\x96\\x1a\\x01\\x17\\xf6\\xd1') (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: ('11111111partition:', 753) (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) Jan 30 16:07:44 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) (client_ip: 127.0.0.1) Jan 30 16:07:44 controller proxy-server: STDOUT: openstack (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) (client_ip: 127.0.0.1) Jan 30 16:07:44 controller proxy-server: STDOUT: openstack/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4openstack (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) (client_ip: 127.0.0.1) Jan 30 16:07:44 controller proxy-server: STDOUT: ?]?14Ν?#023#032#032#001#027??txn: tx5d49fd7b64cb4e0c89e04-005c515b50) (client_ip: 127.0.0.1) Jan 30 16:07:44 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) (client_ip: 127.0.0.1) Jan 30 16:07:44 controller proxy-server: STDOUT: ('=========keys:', '\\xbc]\\xec\\x0c\\xcf]\\x8a\\xb6\\x13\\x1a\\x96\\x1a\\x01\\x17\\xf6\\xd1') (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) (client_ip: 127.0.0.1) Jan 30 16:07:44 controller proxy-server: STDOUT: ('11111111partition:', 753) (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) (client_ip: 127.0.0.1) Jan 30 16:07:44 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) (client_ip: 127.0.0.1) Jan 30 16:07:44 controller proxy-server: STDOUT: openstack (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) (client_ip: 127.0.0.1) Jan 30 16:07:44 controller proxy-server: STDOUT: openstack/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4/object-a-3.txtopenstack (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) (client_ip: 127.0.0.1) Jan 30 16:07:44 controller proxy-server: STDOUT: >惘&#013??#026%?? (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) (client_ip: 127.0.0.1) Jan 30 16:07:44 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) (client_ip: 127.0.0.1) Jan 30 16:07:44 controller proxy-server: STDOUT: ('=========keys:', '>\\xe6\\x83\\x98&\\x0b\\xaa\\xe1\\xe0\\xa1\\x16%\\xd9\\xf6\\x82\\xbf') (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) (client_ip: 127.0.0.1) Jan 30 16:07:44 controller proxy-server: STDOUT: ('11111111partition:', 251) (txn: tx5d49fd7b64cb4e0c89e04-005c515b50) (client_ip: 127.0.0.1) Jan 30 16:07:44 controller proxy-server: 127.0.0.1 127.0.0.1 30/Jan/2019/08/07/44 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4/object-a-3.txt HTTP/1.0 404 - python-swiftclient-3.5.0 gAAAAABcUVjCZNjJ... - - - tx5d49fd7b64cb4e0c89e04-005c515b50 - 0.0212 - - 1548835664.548610926 1548835664.569818020 0 Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4openstack (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ?]?14Ν?#023#032#032#001#027??txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('=========keys:', '\\xbc]\\xec\\x0c\\xcf]\\x8a\\xb6\\x13\\x1a\\x96\\x1a\\x01\\x17\\xf6\\xd1') (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('11111111partition:', 753) (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4/object-a-3.txtopenstack (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: >惘&#013??#026%?? (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('=========keys:', '>\\xe6\\x83\\x98&\\x0b\\xaa\\xe1\\xe0\\xa1\\x16%\\xd9\\xf6\\x82\\xbf') (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('11111111partition:', 251) (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: 127.0.0.1 127.0.0.1 30/Jan/2019/08/07/45 PUT /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4/object-a-3.txt HTTP/1.0 201 - python-swiftclient-3.5.0 gAAAAABcUVjCZNjJ... 18431 - - txacc37f453a6441499423e-005c515b51 - 0.0709 - - 1548835665.213526964 1548835665.284423113 0 Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx06a68fa051174db18c0aa-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack (txn: tx06a68fa051174db18c0aa-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack/AUTH_7d6eaa90d74a4f239963933c3a744df3openstack (txn: tx06a68fa051174db18c0aa-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ??70L??023G (txn: tx06a68fa051174db18c0aa-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx06a68fa051174db18c0aa-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('=========keys:', '\\xdf\\xcf\\x8a\\xff\\x9b7\\xc1H0L\\xf4A\\xb2\\x13\\xc1\\x87') (txn: tx06a68fa051174db18c0aa-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('11111111partition:', 895) (txn: tx06a68fa051174db18c0aa-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx06a68fa051174db18c0aa-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack (txn: tx06a68fa051174db18c0aa-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4openstack (txn: tx06a68fa051174db18c0aa-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ?]?14Ν?#023#032#032#001#027??txn: tx06a68fa051174db18c0aa-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx06a68fa051174db18c0aa-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('=========keys:', '\\xbc]\\xec\\x0c\\xcf]\\x8a\\xb6\\x13\\x1a\\x96\\x1a\\x01\\x17\\xf6\\xd1') (txn: tx06a68fa051174db18c0aa-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('11111111partition:', 753) (txn: tx06a68fa051174db18c0aa-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx8a9fb911df1d48af97b6f-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack (txn: tx8a9fb911df1d48af97b6f-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack/AUTH_7d6eaa90d74a4f239963933c3a744df3openstack (txn: tx8a9fb911df1d48af97b6f-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ??70L??023G (txn: tx8a9fb911df1d48af97b6f-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx8a9fb911df1d48af97b6f-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('=========keys:', '\\xdf\\xcf\\x8a\\xff\\x9b7\\xc1H0L\\xf4A\\xb2\\x13\\xc1\\x87') (txn: tx8a9fb911df1d48af97b6f-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('11111111partition:', 895) (txn: tx8a9fb911df1d48af97b6f-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx8a9fb911df1d48af97b6f-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack (txn: tx8a9fb911df1d48af97b6f-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4openstack (txn: tx8a9fb911df1d48af97b6f-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ?]?14Ν?#023#032#032#001#027??txn: tx8a9fb911df1d48af97b6f-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx8a9fb911df1d48af97b6f-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('=========keys:', '\\xbc]\\xec\\x0c\\xcf]\\x8a\\xb6\\x13\\x1a\\x96\\x1a\\x01\\x17\\xf6\\xd1') (txn: tx8a9fb911df1d48af97b6f-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('11111111partition:', 753) (txn: tx8a9fb911df1d48af97b6f-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: 127.0.0.1 127.0.0.1 30/Jan/2019/08/07/45 GET /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4/%3Fformat%3Djson HTTP/1.0 200 - python-swiftclient-3.5.0 gAAAAABcUVjCZNjJ... - 103 - tx06a68fa051174db18c0aa-005c515b51 - 0.0584 - - 1548835665.329989910 1548835665.388351917 0 Jan 30 16:07:45 controller proxy-server: 127.0.0.1 127.0.0.1 30/Jan/2019/08/07/45 GET /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4%3Fdelimiter%3D%252F%26limit%3D1001%26format%3Djson HTTP/1.0 200 - python-swiftclient-3.5.0 gAAAAABcUVjCZNjJ... - 1017 - tx8a9fb911df1d48af97b6f-005c515b51 - 0.0464 - - 1548835665.346412897 1548835665.392823935 0 Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx10ed616c142448debc5d4-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack (txn: tx10ed616c142448debc5d4-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack/AUTH_7d6eaa90d74a4f239963933c3a744df3openstack (txn: tx10ed616c142448debc5d4-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ??70L??023G (txn: tx10ed616c142448debc5d4-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx10ed616c142448debc5d4-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('=========keys:', '\\xdf\\xcf\\x8a\\xff\\x9b7\\xc1H0L\\xf4A\\xb2\\x13\\xc1\\x87') (txn: tx10ed616c142448debc5d4-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('11111111partition:', 895) (txn: tx10ed616c142448debc5d4-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx10ed616c142448debc5d4-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack (txn: tx10ed616c142448debc5d4-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4openstack (txn: tx10ed616c142448debc5d4-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ?]?14Ν?#023#032#032#001#027??txn: tx10ed616c142448debc5d4-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: tx10ed616c142448debc5d4-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('=========keys:', '\\xbc]\\xec\\x0c\\xcf]\\x8a\\xb6\\x13\\x1a\\x96\\x1a\\x01\\x17\\xf6\\xd1') (txn: tx10ed616c142448debc5d4-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('11111111partition:', 753) (txn: tx10ed616c142448debc5d4-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: 127.0.0.1 127.0.0.1 30/Jan/2019/08/07/45 GET /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4%3Fmarker%3Dobject-server.txt%26delimiter%3D%252F%26limit%3D1001%26format%3Djson HTTP/1.0 200 - python-swiftclient-3.5.0 gAAAAABcUVjCZNjJ... - 2 - tx10ed616c142448debc5d4-005c515b51 - 0.0069 - - 1548835665.394655943 1548835665.401551962 0 找到了 Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: openstack/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4/object-a-3.txtopenstack (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: >惘&#013??#026%?? (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: --------------------------------------------------, my debug -------------------- (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('=========keys:', '>\\xe6\\x83\\x98&\\x0b\\xaa\\xe1\\xe0\\xa1\\x16%\\xd9\\xf6\\x82\\xbf') (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: STDOUT: ('11111111partition:', 251) (txn: txacc37f453a6441499423e-005c515b51) (client_ip: 127.0.0.1) Jan 30 16:07:45 controller proxy-server: 127.0.0.1 127.0.0.1 30/Jan/2019/08/07/45 PUT /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3/test4/object-a-3.txt HTTP/1.0 201 - python-swiftclient-3.5.0 gAAAAABcUVjCZNjJ... 18431 - - txacc37f453a6441499423e-005c515b51 - 0.0709 - - 1548835665.213526964 1548835665.284423113 0 这下终于放心了。 几个想看到的点，都看到了。最后得到的 partition 是 251。哈哈！~ 现象结合本质 openstackAUTH_7d6eaa90d74a4f239963933c3a744df3/test4/object-a.txtopenstack Compute the MD5 hash of that string to get 942354320ddd6c54d9a4c052ef18bc25 then take the first 16 bits to get 9423, or in decimal, *. ref https://ask.openstack.org/en/question/6766/what-is-a-swift-partition/ https://docs.openstack.org/swift/queens/overview_ring.html Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/openstack-swift-object-storage-result-analysis.html":{"url":"docs/openstack-swift-object-storage-result-analysis.html","title":"上传文件结果分析","keywords":"","body":"当一个文件上传后，存储文件的结果现象分析 env 系统：Ubuntu Server 16.04×64 存储设置：4T 架构部署: 主机名 IP 作用 keystone 192.168.100.50 keystone node Proxy 192.168.100.50/192.168.0.50 代理节点 object1 192.168.100.105 存储节点1(zone1) object2 192.168.100.106 存储节点2(zone1) object3 192.168.100.107 存储节点3(zone1) object4 192.168.100.107 存储节点4(zone1) step 理论 以对象为例：在objects目录下存放的是各个partition目录，其中每个partition目录是由若干个suffix_path名的目录和一个hashes.pkl文件组成，suffix_path目录下是由object的hash_path名构成的目录，在hash_path目录下存放了关于object的数据和元数据，因此可以以partition目录为单位将整体复制到新分配的节点磁盘。 观察现场 root@swift105:/srv/node/sdb# ls accounts async_pending containers objects tmp root@swift105:/srv/node/sdb# cd objects/ root@swift105:/srv/node/sdb/objects# ls 0 128 162 185 20 206 24 257 28 313 33 342 360 38 411 453 491 516 53 555 579 586 624 67 760 848 874 90 915 921 97 981 99 999 1016 130 174 19 200 221 242 264 284 315 330 351 361 392 431 462 492 52 534 562 58 596 636 684 778 868 883 908 92 937 974 987 991 auditor_status_ALL.json 1022 149 18 193 201 226 253 266 309 32 340 354 379 408 446 49 507 524 542 576 582 614 66 706 782 87 899 911 920 952 977 989 992 auditor_status_ALL.json root@swift105:/srv/node/sdb/objects# upload 因为现在的文件少，所以，我们寄希望于，我们上传一个新的文件，创建新的 partition ，然后，我们观察一下，这个新的partition中有什么. 通过 chrome: http://192.168.0.50/horizon/project/containers/container/test4 上传一个 object-a.txt 文件 这个时候，先记录一下，文件的头尾，用于等会比较。 DELL@DESKTOP-MQ9CENU MINGW64 ~/Downloads $ head object-a.txt && tail object-a.txt [DEFAULT] bind_ip = 192.168.100.50 bind_port = 6200 # bind_timeout = 30 # backlog = 4096 user = swift swift_dir = /etc/swift devices = /srv/node mount_check = true # disable_fallocate = false # dump_timestamp = false # # This is the path of the URL to access the mini web UI. # path = /__profile__ # # Clear the data when the wsgi server shutdown. # flush_at_shutdown = false # # unwind the iterator of applications # unwind = false DELL@DESKTOP-MQ9CENU MINGW64 ~/Downloads $ ring 会产生 partition 如何生成 partition 的呢，请看 这里 或者 这里 观察现场 默认配置下，会产生partition，并复制3份，我在 100.105，100.106，100.107-sdb 中都找到了（107-sdc 中没有）。下面我就以 100.105 为例，进去分析： root@swift105:/srv/node/sdb/objects# ls 0 119 149 18 193 201 226 253 266 309 32 340 354 379 408 446 49 507 524 542 576 582 614 66 706 782 87 899 911 920 952 977 989 992 auditor_status_ALL.json 1016 128 162 185 20 206 24 257 28 313 33 342 360 38 411 453 491 516 53 555 579 586 624 67 760 848 874 90 915 921 97 981 99 999 1022 130 174 19 200 221 242 264 284 315 330 351 361 392 431 462 492 52 534 562 58 596 636 684 778 868 883 908 92 937 974 987 991 auditor_status_ALL.json root@swift105:/srv/node/sdb/objects# partition目录 - 119 观察到，多出了一个 119 文件夹，这个 119 就是我们的 object partition了。 root@swift105:/srv/node/sdb/objects# cd 119 && ls 611 hashes.invalid hashes.pkl root@swift105:/srv/node/sdb/objects/119# hashes.invalid ： 这个是什么，我不知道，谁能告诉我 611 : suffix_path名的目录 hashes.pkl ： a pickled dictionary. hashes.pkl The file itself is a pickled dictionary. The dictionary contains a dictionary where the key is a suffix directory name and the value is the MD5 hash of the directory listing for that suffix. In this manner, the daemon can quickly identify differences between local and remote suffix directories on a per partition basis as the scope of any one hashes.pkl file is a partition directory. 具体看 https://docs.openstack.org/swift/queens/overview_replication.html#hashes-pkl root@swift105:/srv/node/sdb/objects/119# cp hashes.pkl ~/a.pkl && cd && ls a.pkl root@swift105:~# python Python 2.7.12 (default, Nov 12 2018, 14:36:49) [GCC 5.4.0 20160609] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import cPickle as pickle >>> fr = open('a.pkl') >>> inf = pickle.load(fr) >>> print(inf) {u'611': 'e1955615281c88159dcc46d12766c6ef'} >>> fr.close() >>> exit() root@swift105:~# hashes.invalid 这个是什么，我不知道，谁知道的？ suffix_path目录 - 611 suffix_path目录下是由object的hash_path名构成的目录 root@swift105:/srv/node/sdb/objects/119# cd 611/ && ls 1df3d15f3a9a2b788e373576bcf97611 root@swift105:/srv/node/sdb/objects/119/611# 最后3位是 '611' object的hash_path目录 - 1df3d15f3a9a2b788e373576bcf97611 在hash_path目录下存放了关于object的数据和元数据 object的数据 - object-a.txt root@swift105:/srv/node/sdb/objects/119/611# cd 1df3d15f3a9a2b788e373576bcf97611/ && ls 1548756536.41498.data root@swift105:/srv/node/sdb/objects/119/611/1df3d15f3a9a2b788e373576bcf97611# 同时，我们可以看一下这个数据的数据内容。 root@swift105:/srv/node/sdb/objects/119/611/1df3d15f3a9a2b788e373576bcf97611# cp 1548756536.41498.data ~/a.txt && head ~/a.txt && tail ~/a.txt [DEFAULT] bind_ip = 192.168.100.50 bind_port = 6200 # bind_timeout = 30 # backlog = 4096 user = swift swift_dir = /etc/swift devices = /srv/node mount_check = true # disable_fallocate = false # dump_timestamp = false # # This is the path of the URL to access the mini web UI. # path = /__profile__ # # Clear the data when the wsgi server shutdown. # flush_at_shutdown = false # # unwind the iterator of applications # unwind = false root@swift105:/srv/node/sdb/objects/119/611/1df3d15f3a9a2b788e373576bcf97611# 与之前的内容比较，知道，就是这个文件，文件内容一样。 object的元数据 元数据存储在文件系统的扩展属性（xattr）中。 那怎么才能看得到呢？ ref https://docs.openstack.org/swift/queens/overview_replication.html#hashes-pkl Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/How-to-enable-Reseller-Admin-for-OpenStack-Swift-Object-Storage-and-KeyStone-V3.html":{"url":"docs/How-to-enable-Reseller-Admin-for-OpenStack-Swift-Object-Storage-and-KeyStone-V3.html","title":"enable reseller admin","keywords":"","body":"How to enable Reseller Admin for OpenStack Swift Object Storage and KeyStone V3? 本文整理自 https://www.ibm.com/developerworks/community/forums/html/topic?id=813eb4fd-c0c6-43a5-9317-a35e4081ef72 How to enable Reseller Admin for OpenStack Swift Object Storage and KeyStone V3? Jul 2, 2015 | Tags: account, admin, configuring, enable, enabling, how, keystone, management, object, openstack, reseller, reseller_admin, reselleradmin, scale, spectrum, swift, to, v3 How to enable Reseller Admin for OpenStack Swift Object Storage and KeyStone V3? The purpose of the article is to leverage the Reseller Admin concept in Swift to query account statistics from a single admin level user. As per OpenStack Swift documentation, Users with the Keystone role defined in reseller_admin_role (ResellerAdmin by default) can operate on any account. The auth system sets the request environ reseller_request to True if a request is coming from a user with this role. This article explains how this can be configured. In this sample exercise, our goal is to enable Reseller Admin access to 'admin' user belonging to 'admin' project. You can also create other users and enable ResellerAdmin access. Assumptions: KeyStone V3 is setup and Swift is configured. Python openstack client command line interface is available. By default, 'admin' user and 'admin' project is created and assigned to 'default' Domain. We will use this in this exercise, however you can configure different user, project and domain to accomplish ResellerAdmin setup. Openrc file is available to work with openstack CLI client. Initially, you will see something like this [root@host ~]# cat openrc # Mon Jun 29 03:34:36 EDT 2015 export OS_AUTH_URL=\"http://127.0.0.1:35357/v3\" export OS_IDENTITY_API_VERSION=3 export OS_AUTH_VERSION=3 export OS_USERNAME=\"admin\" export OS_PASSWORD=\"password\" export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=admin export OS_PROJECT_DOMAIN_NAME=Default [root@host ~]# openstack domain list +---------+---------+---------+----------------------------------------------------------------------+ | ID | Name | Enabled | Description +---------+---------+---------+----------------------------------------------------------------------+ | default | Default | True | Owns users and tenants (i.e. projects) available on Identity API v2. | +---------+---------+---------+----------------------------------------------------------------------+ Now, create a new Domain [root@host ~]# openstack domain create Domain1 +---------+----------------------------------+ | Field | Value | +---------+----------------------------------+ | enabled | True | | id | 193bed5ba5a5481c8676bb5e06cf6125 | | name | Domain1 | +---------+----------------------------------+ [root@host ~]# openstack domain list +----------------------------------+---------+---------+----------------------------------------------------------------------+ | ID | Name | Enabled | Description +----------------------------------+---------+---------+----------------------------------------------------------------------+ | default | Default | True | Owns users and tenants (i.e. projects) available on Identity API v2. | | 193bed5ba5a5481c8676bb5e06cf6125 | Domain1 | True | | +----------------------------------+---------+---------+----------------------------------------------------------------------+ Next, create a new project, lets call it 'Project1' [root@host ~]# openstack project list +----------------------------------+---------+ | ID | Name | +----------------------------------+---------+ | 534bca56aca443e889036ab52cfe40ec | admin | | 4c03ae000dd54860a4f9645d75272f5e | service | +----------------------------------+---------+ [root@host ~]# openstack project create Project1 --domain Domain1 +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | | | domain_id | 193bed5ba5a5481c8676bb5e06cf6125 | | enabled | True | | id | e597d73f169444a1bb55a1e3a4adb29d | | name | Project1 | +-------------+----------------------------------+ [root@host ~]# openstack project list +----------------------------------+----------+ | ID | Name | +----------------------------------+----------+ | 534bca56aca443e889036ab52cfe40ec | admin | | 4c03ae000dd54860a4f9645d75272f5e | service | | e597d73f169444a1bb55a1e3a4adb29d | Project1 | +----------------------------------+----------+ Next, create a new user 'User1' and assign him to 'Project1' as part of 'Domain1' [root@host ~]# openstack user create --domain Domain1 --project Project1 --password User1 User1 +--------------------+----------------------------------+ | Field | Value | +--------------------+----------------------------------+ | default_project_id | e597d73f169444a1bb55a1e3a4adb29d | | domain_id | 193bed5ba5a5481c8676bb5e06cf6125 | | enabled | True | | id | d82e44a986234ad9bbac48d58cd2d18c | | name | User1 | +--------------------+----------------------------------+ In KeyStone V3, by default, you will see 'admin' role is being configured and available [root@host ~]# openstack role list +----------------------------------+-------+ | ID | Name | +----------------------------------+-------+ | 4fbfdce0a56b47dd8d29907f534d4b52 | admin | +----------------------------------+-------+ Lets add the newly created 'User1' to 'Project1' as part of 'Domain1' [root@host ~]# openstack role add --project Project1 --user User1 admin [root@host ~]# echo $? 0 [root@host ~]# openstack role add --domain Domain1 --user User1 admin [root@host ~]# echo $? 0 Copy openrc and modify it to create a testrc (sample below) [root@host ~]# cat testrc # Mon Jun 29 03:34:36 EDT 2015 export OS_AUTH_URL=\"http://127.0.0.1:35357/v3\" export OS_IDENTITY_API_VERSION=3 export OS_AUTH_VERSION=3 export OS_USERNAME=\"User1\" export OS_PASSWORD=\"User1\" export OS_USER_DOMAIN_NAME=Domain1 export OS_PROJECT_NAME=Project1 export OS_PROJECT_DOMAIN_NAME=Domain1 Source the 'testrc', we will try and create a swift container and upload a sample object and get account level statistics. [root@host ~]# source testrc Intially, 'swift list' shows nothing. [root@host ~]# swift list [root@host ~]# Now, create a container [root@host ~]# swift post testcontainer1 We can upload an object to the 'testcontainer1' [root@host ~]# swift upload testcontainer1 file1 file1 [root@host ~]# swift list testcontainer1 Now, ensure swift stat shows [root@host ~]# swift stat Account: AUTH_e597d73f169444a1bb55a1e3a4adb29d Containers: 1 Objects: 1 Bytes: 166978 Containers in policy \"policy-0\": 1 Objects in policy \"policy-0\": 1 Bytes in policy \"policy-0\": 166978 X-Account-Project-Domain-Id: 193bed5ba5a5481c8676bb5e06cf6125 X-Timestamp: 1435818271.19603 X-Trans-Id: txa197a1317218450c8a734-005594e2ec Content-Type: text/plain; charset=utf-8 Accept-Ranges: bytes [root@host ~]# swift list testcontainer1 file1 Now, source 'openrc' to 'admin' user of 'admin' project with 'Default' domain [root@host ~]# source openrc As 'admin' user, If you try to list the container created by 'User1' of 'Project1' and 'Domain1', you will not be able to do this. This requires ResellerAdmin level access, which is what we are going to configure now. [root@host ~]# swift list testcontainer1 Container 'testcontainer1' not found Swift proxy-server.conf needs to be edited to add 'reseller_admin_role' and 'reseller_prefix'. As per KeyStone V3 documentation, the name 'ResellerAdmin' is not hardcoded and can be user defined.'resellerprefix' can be set to a user defined prefix. We will use 'AUTH', but you can use something else as well. By default, on /etc/swift/proxy-server.conf, you will see something like this [filter:keystone] use = egg:swift#keystoneauth operator_roles = admin, SwiftOperator is_admin = true cache = swift.cache Edit the proxy-server.conf (filter:keystone section) and add the lines highlighted in bold. [filter:keystone] use = egg:swift#keystoneauth operator_roles = admin, SwiftOperator reseller_admin_role = ResellerAdmin reseller_prefix = AUTH_ is_admin = true cache = swift.cache Now, create ResellerAdmin role. [root@host ~]# openstack role create ResellerAdmin +-------+----------------------------------+ | Field | Value | +-------+----------------------------------+ | id | 9658c423949a460bbb6ed04b19fae672 | | name | ResellerAdmin | +-------+----------------------------------+ [root@host ~]# openstack role list +----------------------------------+---------------+ | ID | Name | +----------------------------------+---------------+ | 4fbfdce0a56b47dd8d29907f534d4b52 | admin | | 9658c423949a460bbb6ed04b19fae672 | ResellerAdmin | +----------------------------------+---------------+ Add 'ResellerAdmin' role to 'admin' user of 'admin' project with 'Default' domain. [root@host ~]# openstack role add --domain Default --user admin ResellerAdmin [root@host ~]# openstack role add --project admin --user admin ResellerAdmin The following command confirms, that 'ResellerAdmin' role is assigned to 'admin' user. [root@host ~]# openstack role assignment list --role ResellerAdmin +----------------------------------+----------------------------------+-------+----------------------------------+---------+ | Role | User | Group | Project | Domain | +----------------------------------+----------------------------------+-------+----------------------------------+---------+ | 9658c423949a460bbb6ed04b19fae672 | 56dbaf1aa80247c7a4834c0fea604926 | | | default | | 9658c423949a460bbb6ed04b19fae672 | 56dbaf1aa80247c7a4834c0fea604926 | | 534bca56aca443e889036ab52cfe40ec | | +----------------------------------+----------------------------------+-------+----------------------------------+---------+ [root@host ~]# openstack user show 56dbaf1aa80247c7a4834c0fea604926 +--------------------+----------------------------------+ | Field | Value | +--------------------+----------------------------------+ | default_project_id | 534bca56aca443e889036ab52cfe40ec | | domain_id | default | | enabled | True | | id | 56dbaf1aa80247c7a4834c0fea604926 | | name | admin | +--------------------+----------------------------------+ [root@host ~]# openstack project show 534bca56aca443e889036ab52cfe40ec +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | | | domain_id | default | | enabled | True | | id | 534bca56aca443e889036ab52cfe40ec | | name | admin | +-------------+----------------------------------+ As 'admin' user, invoke keystone to get a new token. [root@host ~]# export TOKEN=`openstack --os-username admin --os-project-name admin token issue | grep id | head -n 1 | awk '{print $4}' ` [root@host ~]# echo $TOKEN a5957db94c3e4fb6aa2f118cbdf63896 You have provided 'ResellerAdmin' access to user 'admin'. Find, the project Id that you want to access using 'ResellerAdmin' access. [root@host ~]# openstack project list +----------------------------------+----------+ | ID | Name | +----------------------------------+----------+ | 534bca56aca443e889036ab52cfe40ec | admin | | 4c03ae000dd54860a4f9645d75272f5e | service | | e597d73f169444a1bb55a1e3a4adb29d | Project1 | +----------------------------------+----------+ Now, as 'admin' user from 'admin' project of 'Default' domain, get the account stat for Project1. Remember, we added 'AUTH' as the reseller prefix in proxy-server.conf. So append the project id to 'AUTH' and issue the following openstack command. The following swift command line shows how to invoke the account stats using the reseller prefix. Note, the swift endpoint URI can be obtained via 'openstack endpoint list'. [root@host ~]# swift --os-auth-token $TOKEN --os-storage-url http://localhost:8080/v1/AUTH_e597d73f169444a1bb55a1e3a4adb29d stat Account: AUTH_e597d73f169444a1bb55a1e3a4adb29d Containers: 1 Objects: 1 Bytes: 166978 Containers in policy \"policy-0\": 1 Objects in policy \"policy-0\": 1 Bytes in policy \"policy-0\": 166978 X-Account-Project-Domain-Id: 193bed5ba5a5481c8676bb5e06cf6125 X-Timestamp: 1435818271.19603 X-Trans-Id: tx2d07e74417ca4e0abea68-005594dda2 Content-Type: text/plain; charset=utf-8 Accept-Ranges: bytes [root@host ~]# swift --os-auth-token $TOKEN --os-storage-url http://localhost:8080/v1/AUTH_e597d73f169444a1bb55a1e3a4adb29d list testcontainer1 [root@host ~]# swift --os-auth-token $TOKEN --os-storage-url http://localhost:8080/v1/AUTH_e597d73f169444a1bb55a1e3a4adb29d list testcontainer1 file1 Now, you are able to query any account using a single user using 'ResellerAdmin' level access, without requiring password of those accounts. Happy OpenStack Swift!!! Note: The information in this blog is provided \"AS IS\". The opinions expressed on this blog represent my own and not those of my employer. Log in to reply. Updated on Jul 2, 2015 at 11:21 PM by manbazha Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-account-quota-outcome.html":{"url":"docs/swift-account-quota-outcome.html","title":"设置account的quota","keywords":"","body":"swift使用ResellerAdmin角色来设置account的quota env 系统：Ubuntu Server 16.04×64 存储设置：4T 架构部署: 主机名 IP 作用 keystone 192.168.100.50 keystone node Proxy 192.168.100.50/192.168.0.50 代理节点 object1 192.168.100.105 存储节点1(zone1) object2 192.168.100.106 存储节点2(zone1) object3 192.168.100.107 存储节点3(zone1) object4 192.168.100.107 存储节点4(zone1) 注意 开启 ResellerAdmin 角色 不是admin给admin配置，而是 ResellerAdmin 角色的用户给 其它用户 配置 quota. step 配置 proxy-swift.conf 要把 swift proxy 节点的 reseller_admin_role = ResellerAdmin 打开注释，重启服务 root@controller:~# grep ResellerAdmin -rn /etc/swift/proxy-server.conf 417:reseller_admin_role = ResellerAdmin root@controller:~# service swift-proxy restart root@controller:~# 把 admin 用户赋予 ResellerAdmin root@controller:~# openstack role add --user admin --project admin ResellerAdmin root@controller:~# openstack project list +----------------------------------+--------------+ | ID | Name | +----------------------------------+--------------+ | 3320a273a7094d789f99756b0e9d0f4f | testreseller | | 7d6eaa90d74a4f239963933c3a744df3 | demo | | a640c74e595c44c4902d1c5ebc3afa8a | service | | f04ec0abf3d1460dad82608bb03af589 | admin | +----------------------------------+--------------+ root@controller:~# openstack role add --user admin ResellerAdmin Must specify either system, domain, or project root@controller:~# 验证一下，是否 ResellerAdmin 加上了 root@controller:~# cat admin-openrc export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=openstack export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 root@controller:~# . admin-openrc root@controller:~# openstack role assignment list --role ResellerAdmin +----------------------------------+----------------------------------+-------+----------------------------------+--------+-----------+ | Role | User | Group | Project | Domain | Inherited | +----------------------------------+----------------------------------+-------+----------------------------------+--------+-----------+ | 1671364ac7a945e5ae164e100de2c366 | 8533cb3873974fa29f03832aef7007ca | | f04ec0abf3d1460dad82608bb03af589 | | False | +----------------------------------+----------------------------------+-------+----------------------------------+--------+-----------+ root@controller:~# openstack user show 8533cb3873974fa29f03832aef7007ca +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | default | | enabled | True | | id | 8533cb3873974fa29f03832aef7007ca | | name | admin | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ root@controller:~# openstack project show f04ec0abf3d1460dad82608bb03af589 +-------------+-----------------------------------------------+ | Field | Value | +-------------+-----------------------------------------------+ | description | Bootstrap project for initializing the cloud. | | domain_id | default | | enabled | True | | id | f04ec0abf3d1460dad82608bb03af589 | | is_domain | False | | name | admin | | parent_id | default | | tags | [] | +-------------+-----------------------------------------------+ root@controller:~# 确实是加上了。 创建新环境 创建新的project, user, role 来实验 准备语句： openstack project create --description \"ptest1\" ptest1 --domain default openstack user create --project ptest1 --password openstack utest1 openstack role create rtest1 openstack role add --user utest1 --project ptest1 rtest1 开始吧 root@controller:~# . admin-openrc root@controller:~# openstack project create --description \"ptest1\" ptest1 --domain default +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | ptest1 | | domain_id | default | | enabled | True | | id | e8ed1722599643b5802a322341b4e02c | | is_domain | False | | name | ptest1 | | parent_id | default | | tags | [] | +-------------+----------------------------------+ root@controller:~# openstack user create --project ptest1 --password openstack utest1 +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | e8ed1722599643b5802a322341b4e02c | | domain_id | default | | enabled | True | | id | 5b8f1fbc7a4548b78b4c59b47ac43e77 | | name | utest1 | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ root@controller:~# openstack role add --user utest1 --project ptest1 user check root@controller:~# openstack role list --user utest1 --project ptest1 Listing assignments using role list is deprecated. Use role assignment list --user --project --names instead. +----------------------------------+------+---------+--------+ | ID | Name | Project | User | +----------------------------------+------+---------+--------+ | aa47471c39884d708f552a3b5aa2f067 | user | ptest1 | utest1 | +----------------------------------+------+---------+--------+ root@controller:~# 查一下 swift stat swift --os-auth-url http://controller:5000/v3 --auth-version 3\\ > --os-project-name ptest1 --os-project-domain-name default \\ > --os-username utest1 --os-user-domain-name default \\ > --os-password openstack stat Account: AUTH_e8ed1722599643b5802a322341b4e02c Containers: 0 Objects: 0 Bytes: 0 X-Put-Timestamp: 1548334374.80379 X-Timestamp: 1548334374.80379 X-Trans-Id: tx97bf4a41e2f54021a2602-005c49b525 Content-Type: text/plain; charset=utf-8 X-Openstack-Request-Id: tx97bf4a41e2f54021a2602-005c49b525 root@controller:~# 或者 写成一个 openrc 文件，也方便 root@controller:~# cat utest1-openrc export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=ptest1 export OS_USERNAME=utest1 export OS_PASSWORD=openstack export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 root@controller:~# . utest1-openrc root@controller:~# swift stat Account: AUTH_e8ed1722599643b5802a322341b4e02c Containers: 1 Objects: 0 Bytes: 0 Containers in policy \"policy-0\": 1 Objects in policy \"policy-0\": 0 Bytes in policy \"policy-0\": 0 Meta Quota-Bytes: 200 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: txc1aecf0f20a04d7795dfb-005c49b7ff X-Timestamp: 1548334795.35484 X-Trans-Id: txc1aecf0f20a04d7795dfb-005c49b7ff Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# 去 http://192.168.0.50/horizon/project/ 创建 container 吧 http://192.168.0.50/horizon/project/containers/container/lt200 (可以不看)下面展示我走过的一段弯路 如果在这个新环境中，创建一个新的role，并赋予新用户utest1, 由于 新role没有添加属性，会导致无法通过 swift stat 查询。具体如下： root@controller:~# . admin-openrc root@controller:~# openstack role create rtest1 +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | None | | id | a541e61be9564648a23f4f2d535e0014 | | name | rtest1 | +-----------+----------------------------------+ root@controller:~# openstack role add --user utest1 --project ptest1 rtest1 root@controller:~# root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3\\ > --os-project-name ptest1 --os-project-domain-name default \\ > --os-username utest1 --os-user-domain-name default \\ > --os-password openstack stat Unauthorized. Check username/id, password, tenant name/id and user/tenant domain name/id. root@controller:~# root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3\\ > --os-project-name ptest1 --os-project-domain-name default \\ > --os-username utest1 --os-user-domain-name default \\ > --os-password openstack stat Account HEAD failed: http://controller:8080/v1/AUTH_e8ed1722599643b5802a322341b4e02c 403 Forbidden Failed Transaction ID: tx6a128832b5a544b7941d0-005c49af6e root@controller:~# root@controller:~# openstack role list --user utest1 --project ptest1 Listing assignments using role list is deprecated. Use role assignment list --user --project --names instead. +----------------------------------+--------+---------+--------+ | ID | Name | Project | User | +----------------------------------+--------+---------+--------+ | a541e61be9564648a23f4f2d535e0014 | rtest1 | ptest1 | utest1 | +----------------------------------+--------+---------+--------+ root@controller:~# openstack role remove --user utest1 --project ptest1 rtest1 root@controller:~# root@controller:~# openstack role list --user utest1 --project ptest1 Listing assignments using role list is deprecated. Use role assignment list --user --project --names instead. root@controller:~# 给 utest1 这个 accout 限制 大小 utest1租户的url地址 utest1租户的url地址为: http://controller:8080/v1/AUTH_e8ed1722599643b5802a322341b4e02c 这个地址可以有多种方式拿到，最简单的，就是 输入之前创建时的 project, user, password 信息, 访问：http://192.168.0.50/horizon/project/api_access/ 效果如下： 那么使用admin为demo租户设置限额200bytes, (设置时，需要. admin-openrc获取admin权限),则设置如下： root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3\\ > --os-project-name ptest1 --os-project-domain-name default \\ > --os-username utest1 --os-user-domain-name default \\ > --os-password openstack stat Account: AUTH_e8ed1722599643b5802a322341b4e02c Containers: 1 Objects: 0 Bytes: 0 Containers in policy \"policy-0\": 1 Objects in policy \"policy-0\": 0 Bytes in policy \"policy-0\": 0 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: tx4181f950431446739181e-005c49b765 X-Timestamp: 1548334795.35484 X-Trans-Id: tx4181f950431446739181e-005c49b765 Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# swift -V 3 -A http://controller:5000/v3 -U admin:admin -K openstack --os-storage-url=http://controller:8080/v1/AUTH_e8ed1722599643b5802a322341b4e02c post -m quota-bytes:200 Unauthorized. Check username/id, password, tenant name/id and user/tenant domain name/id. root@controller:~# . admin-openrc root@controller:~# swift -V 3 -A http://controller:5000/v3 -U admin:admin -K openstack --os-storage-url=http://controller:8080/v1/AUTH_e8ed1722599643b5802a322341b4e02c post -m quota-bytes:200 root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3 --os-project-name ptest1 --os-project-domain-name default --os-username utest1 --os-user-domain-name default --os-password openstack stat Account: AUTH_e8ed1722599643b5802a322341b4e02c Containers: 1 Objects: 0 Bytes: 0 Containers in policy \"policy-0\": 1 Objects in policy \"policy-0\": 0 Bytes in policy \"policy-0\": 0 Meta Quota-Bytes: 200 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: txb6e7ce54af8d4497a5e18-005c49b78a X-Timestamp: 1548334795.35484 X-Trans-Id: txb6e7ce54af8d4497a5e18-005c49b78a Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# 成功了。 check 访问 http://192.168.0.50/horizon/project/containers/container/lt200 。此时，依然可以上传 大于 200 bytes 的文件（别问我为什么？哈哈原因就是这：Due to the eventual consistency further uploads might be possible until the account size has been updated.）。但是过一会，就上传不了。哈哈。说明，真正在生产的时候，这里应该要限制一下时间，等待集群刷新。哈哈。 效果如下： 因为之前的account 的总大小已经大于200，所以，哈哈，现在连4K的文件都上传不了。 当然，你可以试一下，把文件全删除掉，然后，你以为不能上传大于 200 bytes 的文件。NO! 依然可以上传，我就测试过了，上传了 2个 14K 的文件。 root@controller:~# swift stat Account: AUTH_e8ed1722599643b5802a322341b4e02c Containers: 1 Objects: 2 Bytes: 29880 Containers in policy \"policy-0\": 1 Objects in policy \"policy-0\": 2 Bytes in policy \"policy-0\": 29880 Meta Quota-Bytes: 200 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: tx566d10c4346b4c3ea0c75-005c49bbf5 X-Timestamp: 1548334795.35484 X-Trans-Id: tx566d10c4346b4c3ea0c75-005c49bbf5 Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# 过了好一会，才更新状态： root@controller:~# swift stat Account: AUTH_e8ed1722599643b5802a322341b4e02c Containers: 1 Objects: 2 Bytes: 29880 Containers in policy \"policy-0\": 1 Objects in policy \"policy-0\": 2 Bytes in policy \"policy-0\": 29880 Meta Quota-Bytes: 200 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: tx9018806c738746c6b6d68-005c49bdaf X-Timestamp: 1548334795.35484 X-Trans-Id: tx9018806c738746c6b6d68-005c49bdaf Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# swift stat Account: AUTH_e8ed1722599643b5802a322341b4e02c Containers: 1 Objects: 1 Bytes: 14940 Containers in policy \"policy-0\": 1 Objects in policy \"policy-0\": 1 Bytes in policy \"policy-0\": 14940 Meta Quota-Bytes: 200 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: tx46a6314ab5a8463fb3541-005c49bdc9 X-Timestamp: 1548334795.35484 X-Trans-Id: tx46a6314ab5a8463fb3541-005c49bdc9 Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# 当然，过了一小会，就真的再也不能上传了。哈哈。 去除 quota-bytes root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3\\ > --os-project-name demo --os-project-domain-name default \\ > --os-username demo --os-user-domain-name default \\ > --os-password openstack stat Account: AUTH_7d6eaa90d74a4f239963933c3a744df3 Containers: 8 Objects: 51 Bytes: 8239369253 Containers in policy \"policy-0\": 8 Objects in policy \"policy-0\": 51 Bytes in policy \"policy-0\": 8239369253 Meta Quota-Bytes: 200 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: tx1baa9f71649a4f5f9a5e0-005c49a63a X-Timestamp: 1546928904.74292 X-Trans-Id: tx1baa9f71649a4f5f9a5e0-005c49a63a Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# swift -V 3 -A http://controller:5000/v3 -U admin:admin -K openstack --os-storage-url=http://controller:8080/v1/AUTH_e8ed1722599643b5802a322341b4e02c post -m quota-bytes: Unauthorized. Check username/id, password, tenant name/id and user/tenant domain name/id. root@controller:~# . admin-openrc root@controller:~# swift -V 3 -A http://controller:5000/v3 -U admin:admin -K openstack --os-storage-url=http://controller:8080/v1/AUTH_e8ed1722599643b5802a322341b4e02c post -m quota-bytes: root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3 --os-project-name demo --os-project-domain-name default --os-username demo --os-user-domain-name default --os-password openstack stat Account: AUTH_7d6eaa90d74a4f239963933c3a744df3 Containers: 9 Objects: 51 Bytes: 8239369253 Containers in policy \"policy-0\": 9 Objects in policy \"policy-0\": 51 Bytes in policy \"policy-0\": 8239369253 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: tx39a80777df094e57800d4-005c49a6c2 X-Timestamp: 1546928904.74292 X-Trans-Id: tx39a80777df094e57800d4-005c49a6c2 Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# check http://192.168.0.50/horizon/project/containers/container/lt200 上传成功。 ref https://www.ibm.com/developerworks/community/forums/html/topic?id=813eb4fd-c0c6-43a5-9317-a35e4081ef72 https://docs.openstack.org/swift/latest/middleware.html#module-swift.common.middleware.account_quotas https://docs.openstack.org/swift/latest/middleware.html#module-swift.common.middleware.container_quotas https://docs.openstack.org/swift/latest/#common-configuration https://docs.openstack.org/keystone/rocky/admin/cli-manage-projects-users-and-roles.html https://blog.csdn.net/my_vips/article/details/17919167 https://blog.csdn.net/dysj4099/article/details/8941465 https://blog.csdn.net/my_vips/article/details/25878751 https://developer.openstack.org/api-ref/object-store/?expanded=#create-update-or-delete-container-metadata https://docs.openstack.org/swift/latest/api/container_quotas.html Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-account-quota-process.html":{"url":"docs/swift-account-quota-process.html","title":"设置account的quota（分析过程）","keywords":"","body":"swift使用ResellerAdmin角色来设置account的quota env 系统：Ubuntu Server 16.04×64 存储设置：4T 架构部署: 主机名 IP 作用 keystone 192.168.100.50 keystone node Proxy 192.168.100.50/192.168.0.50 代理节点 object1 192.168.100.105 存储节点1(zone1) object2 192.168.100.106 存储节点2(zone1) object3 192.168.100.107 存储节点3(zone1) object4 192.168.100.107 存储节点4(zone1) 注意 无 step 配置 proxy-swift.conf 要把 swift proxy 节点的 reseller_admin_role = ResellerAdmin 打开注释，重启服务 root@controller:~# grep ResellerAdmin -rn /etc/swift/proxy-server.conf 417:reseller_admin_role = ResellerAdmin root@controller:~# service swift-proxy restart root@controller:~# 创建新环境 创建新的project, user, role 来实验 准备语句： openstack project create --description \"testTenant\" testreseller --domain default openstack user create --project testreseller --password openstack testreseller openstack role create reseller openstack role add --user testreseller --project testreseller reseller openstack role add --user testreseller --project testreseller ResellerAdmin 开始吧 root@controller:~# openstack project create --description \"testTenant\" testreseller \\ > --domain default +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | testTenant | | domain_id | default | | enabled | True | | id | 3320a273a7094d789f99756b0e9d0f4f | | is_domain | False | | name | testreseller | | parent_id | default | | tags | [] | +-------------+----------------------------------+ root@controller:~# openstack project list +----------------------------------+--------------+ | ID | Name | +----------------------------------+--------------+ | 3320a273a7094d789f99756b0e9d0f4f | testreseller | | 7d6eaa90d74a4f239963933c3a744df3 | demo | | a640c74e595c44c4902d1c5ebc3afa8a | service | | f04ec0abf3d1460dad82608bb03af589 | admin | +----------------------------------+--------------+ root@controller:~# openstack user create --project testreseller --password openstack testreseller +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | 3320a273a7094d789f99756b0e9d0f4f | | domain_id | default | | enabled | True | | id | c83158ab5c3348ab9d148673c23903be | | name | testreseller | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ root@controller:~# openstack role create reseller +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | None | | id | 09d4fdc0e9544f509c599cd62ab40e6c | | name | reseller | +-----------+----------------------------------+ root@controller:~# openstack role add --user testreseller --project testreseller reseller root@controller:~# 创建 ResellerAdmin ，把新用户 testreseller 赋予这个role root@controller:~# openstack role create ResellerAdmin +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | None | | id | 1671364ac7a945e5ae164e100de2c366 | | name | ResellerAdmin | +-----------+----------------------------------+ root@controller:~# openstack role add --user testreseller --project testreseller ResellerAdmin root@controller:~# 验证一下，是否 ResellerAdmin 加上了 root@controller:~# cat admin-openrc export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=openstack export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 root@controller:~# root@controller:~# . admin-openrc root@controller:~# openstack role assignment list --role ResellerAdmin +----------------------------------+----------------------------------+-------+----------------------------------+--------+-----------+ | Role | User | Group | Project | Domain | Inherited | +----------------------------------+----------------------------------+-------+----------------------------------+--------+-----------+ | 1671364ac7a945e5ae164e100de2c366 | c83158ab5c3348ab9d148673c23903be | | 3320a273a7094d789f99756b0e9d0f4f | | False | +----------------------------------+----------------------------------+-------+----------------------------------+--------+-----------+ root@controller:~# openstack user show c83158ab5c3348ab9d148673c23903be +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | 3320a273a7094d789f99756b0e9d0f4f | | domain_id | default | | enabled | True | | id | c83158ab5c3348ab9d148673c23903be | | name | testreseller | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ root@controller:~# 确实是加上了。 给 testreseller 这个 accout 限制 大小 root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3\\ > --os-project-name testreseller --os-project-domain-name default \\ > --os-username testreseller --os-user-domain-name default \\ > --os-password openstack post -m quota-bytes:10000 root@controller:~# 查看一下用户 admin 与 testreseller 的区别。 root@controller:~# cat admin-openrc export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=openstack export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 root@controller:~# . admin-openrc root@controller:~# swift stat -v StorageURL: http://controller:8080/v1/AUTH_f04ec0abf3d1460dad82608bb03af589 Auth Token: gAAAAABcSZpeeHt6OUARCkWhJaVQ-1lOWxqxscVYMl1eAnrmfwXk40injCPY7V0ZjQOv7wReLgjv4Z4qM_ZzDlNK-KRz2AQFLTVxe4PdQJk7dEazzzQ-MTpMtD2WEqrYfgxdtbEXLE3Iw9Tthhbn-XHnc2OM3sJdIsTVBnvhDc4Tf4NBnItIQ2c Account: AUTH_f04ec0abf3d1460dad82608bb03af589 Containers: 1 Objects: 1 Bytes: 26262280 Containers in policy \"policy-0\": 1 Objects in policy \"policy-0\": 1 Bytes in policy \"policy-0\": 26262280 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: txc6f0ed7792c546bfbd160-005c499a5e X-Timestamp: 1547641627.86782 X-Trans-Id: txc6f0ed7792c546bfbd160-005c499a5e Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# root@controller:~# cat testreseller-openrc export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=testreseller export OS_USERNAME=testreseller export OS_PASSWORD=openstack export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 root@controller:~# . testreseller-openrc root@controller:~# swift stat -v StorageURL: http://controller:8080/v1/AUTH_3320a273a7094d789f99756b0e9d0f4f Auth Token: gAAAAABcSY0Uyh27qJWIkueCQZ6ZWhXP-NJbGj4dJ6qRefiYEqBoT_9SfR86cyucBRPxL3dxOlwy-5mHtZ0M8lE-TveWKbLijW3rJDNolFEQria_tn9rdE4q5SK5a5Rl6LhF7XxBaTBb5cXFRozFcivAi8RKbfgqCX6mLpFnupEWfc_Dh5JwDCA Account: AUTH_3320a273a7094d789f99756b0e9d0f4f Containers: 0 Objects: 0 Bytes: 0 Meta Quota-Bytes: 10000 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: txa424375b986a4b82a61e4-005c498d14 X-Timestamp: 1548323944.28538 X-Trans-Id: txa424375b986a4b82a61e4-005c498d14 Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# 观察到： admin 用户不受影响 testreseller 用户有多一行Meta Quota-Bytes: 10000 也就是说，限制testreseller 用户的存储大小为 10000 bytes check chrome打开： http://192.168.0.50/horizon/project/containers/container/lt10000 但是，发现，上传了 4M, 30M, 70M, 112M 的文件，都可以。为什么？？？ 问题解决了 从 https://blog.csdn.net/my_vips/article/details/17919167 看到下面这一句话： 使用admin为test租户设置限额,test租户的url地址为:http://192.168.26.69:8080/v1/AUTH_3e2a0a2df18b4f86a52e2dc6ad3cb989 swift -V 2 -A http://192.168.26.69:5000/v2.0 -U admin:admin -K ADMIN_PASS --os-storage-url=http://192.168.26.69:8080/v1/AUTH_3e2a0a2df18b4f86a52e2dc6ad3cb989 post -m quota-bytes:100000000 原来，不是自己给自己配置，而是通过具体 ResellerAdmin 角色的用户给 其它用户 配置 quota-bytes. (可不看)之前的错误尝试 root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3\\ > --os-project-name demo --os-project-domain-name default \\ > --os-username demo --os-user-domain-name default \\ > --os-password openstack post -m quota-bytes:100 Account POST failed: http://controller:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 403 Forbidden [first 60 chars of response] ForbiddenAccess was denied to this resourc Failed Transaction ID: txfa9b8b212b504f10af48c-005c49a542 root@controller:~# . admin-openrc root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3 --os-project-name demo --os-project-domain-name default --os-username demo --os-user-domain-name default --os-password openstack post -m quota-bytes:100 Account POST failed: http://controller:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 403 Forbidden [first 60 chars of response] ForbiddenAccess was denied to this resourc Failed Transaction ID: tx2814b6402eba42e39c8a1-005c49a55b root@controller:~# 把 admin 用户赋予 ResellerAdmin root@controller:~# openstack role add --user admin --project admin ResellerAdmin root@controller:~# openstack project list +----------------------------------+--------------+ | ID | Name | +----------------------------------+--------------+ | 3320a273a7094d789f99756b0e9d0f4f | testreseller | | 7d6eaa90d74a4f239963933c3a744df3 | demo | | a640c74e595c44c4902d1c5ebc3afa8a | service | | f04ec0abf3d1460dad82608bb03af589 | admin | +----------------------------------+--------------+ root@controller:~# openstack role add --user admin ResellerAdmin Must specify either system, domain, or project root@controller:~# demo租户的url地址为: http://controller:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3(这个地址可以有多种方式拿到，最简单的，就是访问：http://192.168.0.50/horizon/project/api_access/) 那么使用admin为demo租户设置限额200bytes, 则设置如下： root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3\\ > --os-project-name demo --os-project-domain-name default \\ > --os-username demo --os-user-domain-name default \\ > --os-password openstack stat Account: AUTH_7d6eaa90d74a4f239963933c3a744df3 Containers: 8 Objects: 51 Bytes: 8239369253 Containers in policy \"policy-0\": 8 Objects in policy \"policy-0\": 51 Bytes in policy \"policy-0\": 8239369253 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: txd1e045cb10584e7da8e5f-005c49a53b X-Timestamp: 1546928904.74292 X-Trans-Id: txd1e045cb10584e7da8e5f-005c49a53b Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# root@controller:~# swift -V 3 -A http://controller:5000/v3 -U admin:admin -K openstack --os-storage-url=http://controller:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 post -m quota-bytes:200 root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3\\ > --os-project-name demo --os-project-domain-name default \\ > --os-username demo --os-user-domain-name default \\ > --os-password openstack stat Account: AUTH_7d6eaa90d74a4f239963933c3a744df3 Containers: 8 Objects: 51 Bytes: 8239369253 Containers in policy \"policy-0\": 8 Objects in policy \"policy-0\": 51 Bytes in policy \"policy-0\": 8239369253 Meta Quota-Bytes: 200 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: tx1baa9f71649a4f5f9a5e0-005c49a63a X-Timestamp: 1546928904.74292 X-Trans-Id: tx1baa9f71649a4f5f9a5e0-005c49a63a Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# 成功了。 check http://192.168.0.50/horizon/project/containers/container/lt200 因为之前的account 的总大小已经大于200，所以，哈哈，现在连4K的文件都上传不了。 去除 quota-bytes root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3\\ > --os-project-name demo --os-project-domain-name default \\ > --os-username demo --os-user-domain-name default \\ > --os-password openstack stat Account: AUTH_7d6eaa90d74a4f239963933c3a744df3 Containers: 8 Objects: 51 Bytes: 8239369253 Containers in policy \"policy-0\": 8 Objects in policy \"policy-0\": 51 Bytes in policy \"policy-0\": 8239369253 Meta Quota-Bytes: 200 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: tx1baa9f71649a4f5f9a5e0-005c49a63a X-Timestamp: 1546928904.74292 X-Trans-Id: tx1baa9f71649a4f5f9a5e0-005c49a63a Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# swift -V 3 -A http://controller:5000/v3 -U admin:admin -K openstack --os-storage-url=http://controller:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 post -m quota-bytes: root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3 --os-project-name demo --os-project-domain-name default --os-username demo --os-user-domain-name default --os-password openstack stat Account: AUTH_7d6eaa90d74a4f239963933c3a744df3 Containers: 9 Objects: 51 Bytes: 8239369253 Containers in policy \"policy-0\": 9 Objects in policy \"policy-0\": 51 Bytes in policy \"policy-0\": 8239369253 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: tx39a80777df094e57800d4-005c49a6c2 X-Timestamp: 1546928904.74292 X-Trans-Id: tx39a80777df094e57800d4-005c49a6c2 Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# check http://192.168.0.50/horizon/project/containers/container/lt200 上传成功。 ref https://www.ibm.com/developerworks/community/forums/html/topic?id=813eb4fd-c0c6-43a5-9317-a35e4081ef72 https://docs.openstack.org/swift/latest/middleware.html#module-swift.common.middleware.account_quotas https://docs.openstack.org/swift/latest/middleware.html#module-swift.common.middleware.container_quotas https://docs.openstack.org/swift/latest/#common-configuration https://docs.openstack.org/keystone/rocky/admin/cli-manage-projects-users-and-roles.html https://blog.csdn.net/my_vips/article/details/17919167 https://blog.csdn.net/dysj4099/article/details/8941465 https://blog.csdn.net/my_vips/article/details/25878751 https://developer.openstack.org/api-ref/object-store/?expanded=#create-update-or-delete-container-metadata https://docs.openstack.org/swift/latest/api/container_quotas.html Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swfit-account-quota-restful-api.html":{"url":"docs/swfit-account-quota-restful-api.html","title":"REST调用accout quota","keywords":"","body":"通过 RESTFUL API 调用实现 account的 限额 env 系统：Ubuntu Server 16.04×64 存储设置：4T 架构部署: 主机名 IP 作用 keystone 192.168.100.50 keystone node Proxy 192.168.100.50/192.168.0.50 代理节点 object1 192.168.100.105 存储节点1(zone1) object2 192.168.100.106 存储节点2(zone1) object3 192.168.100.107 存储节点3(zone1) object4 192.168.100.107 存储节点4(zone1) Assumptions: swift使用ResellerAdmin角色来设置account的quota KeyStone V3 is setup and Swift is configured. Python openstack client command line interface is available. By default, 'admin' user and 'admin' project is created and assigned to 'default' Domain. We will use this in this exercise, however you can configure different user, project and domain to accomplish ResellerAdmin setup. Openrc file is available to work with openstack CLI client. 注意 开启 ResellerAdmin 角色 不是admin给admin配置，而是 ResellerAdmin 角色的用户给 其它用户 配置 quota. step check accout quota 生效 root@controller:~# . admin-openrc root@controller:~# swift -V 3 -A http://controller:5000/v3 -U admin:admin -K openstack --os-storage-url=http://controller:8080/v1/AUTH_e8ed1722599643b5802a322341b4e02c post -m quota-bytes:14568 root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3 --os-project-name ptest1 --os-project-domain-name default --os-username utest1 --os-user-domain-name default --os-password openstack stat Account: AUTH_e8ed1722599643b5802a322341b4e02c Containers: 1 Objects: 7 Bytes: 108071 Containers in policy \"policy-0\": 1 Objects in policy \"policy-0\": 7 Bytes in policy \"policy-0\": 108071 Meta Quota-Bytes: 14568 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: tx92ec60cc71c647a0b8522-005c4bceeb X-Timestamp: 1548334795.35484 X-Trans-Id: tx92ec60cc71c647a0b8522-005c4bceeb Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# 构造 RESTFUL API 思路 因为我们这里是要修改 account 的 quota 通过 https://docs.openstack.org/swift/latest/middleware.html#module-swift.common.middleware.account_quotas 中的 swift -A http://127.0.0.1:8080/auth/v1.0 -U account:reseller -K secret post -m quota-bytes:10000 我们知道： 发送的是post请求 \"-m quota-bytes:\" ，修改的是metadata 那就去找 object-store API 文档 https://developer.openstack.org/api-ref/object-store/index.html ，找到 Account ，找到 POST. 也就是查找到： https://developer.openstack.org/api-ref/object-store/index.html?expanded=create-update-or-delete-account-metadata-detail 比如，Update account metadata: curl -i $publicURL -X POST -H \"X-Auth-Token: $token\" -H \"X-Account-Meta-Subject: AmericanLiterature\" 展开 ，可以搜索 quota ，然后，找到 X-Account-Meta-Quota-Bytes (Optional) header string If present, this is the limit on the total size in bytes of objects stored in the account. Typically this value is set by an administrator. 。看一下大意，应该 X-Account-Meta-Quota-Bytes 就是我们想要的。 那么，对于我们的目标，quota, 就是把 swift -V 3 -A http://controller:5000/v3 -U admin:admin -K openstack --os-storage-url=http://controller:8080/v1/AUTH_e8ed1722599643b5802a322341b4e02c post -m quota-bytes:14568 转成下面语句： curl -i $publicURL -X POST -H \"X-Auth-Token: $token\" -H \"X-Account-Meta-Quota-Bytes: 14568\" 下面，我们就先去找 publicURL 和 token。 当然，这里的 token ,就是 使用 admin 的 token了。 拿 publicURL 这里我们依然是修改 utest1 的 quota utest1 的 http://192.168.0.50:8080/v1/AUTH_e8ed1722599643b5802a322341b4e02c 拿 token 这里使用 admin 用户的 token root@controller:~# . admin-openrc root@controller:~# swift stat -v | grep Token Auth Token: gAAAAABcS9g0300q8mc5Hb9cakWIxkDDVTC4HwkwQ2TMsXP3jH0CkFYMuMlMeJ1b2pmx04GQ822oN_goj7AiAeWspEFn0W5vg9WuDLkvEuANAMPpX6TzlHIg9trNglZBqPxdczpOO33FrKifAtqYksNs3Sdbj6xFcgzrltXrwkuf6vdM13JUzkA root@controller:~# curl 调用 RESTFUL POST root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3 --os-project-name ptest1 --os-project-domain-name default --os-username utest1 --os-user-domain-name default --os-password openstack stat Account: AUTH_e8ed1722599643b5802a322341b4e02c Containers: 1 Objects: 7 Bytes: 108071 Containers in policy \"policy-0\": 1 Objects in policy \"policy-0\": 7 Bytes in policy \"policy-0\": 108071 Meta Quota-Bytes: 14568 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: tx6b8de30f245d471cbb2b5-005c4bda73 X-Timestamp: 1548334795.35484 X-Trans-Id: tx6b8de30f245d471cbb2b5-005c4bda73 Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# curl -i http://192.168.0.50:8080/v1/AUTH_e8ed1722599643b5802a322341b4e02c -X POST -H \"X-Auth-Token: gAAAAABcS9g0300q8mc5Hb9cakWIxkDDVTC4HwkwQ2TMsXP3jH0CkFYMuMlMeJ1b2pmx04GQ822oN_goj7AiAeWspEFn0W5vg9WuDLkvEuANAMPpX6TzlHIg9trNglZBqPxdczpOO33FrKifAtqYksNs3Sdbj6xFcgzrltXrwkuf6vdM13JUzkA\" -H \"X-Account-Meta-Quota-Bytes: 123456\" HTTP/1.1 204 No Content Content-Length: 0 Content-Type: text/html; charset=UTF-8 X-Trans-Id: tx87763d360ed5495eae4a7-005c4bda99 X-Openstack-Request-Id: tx87763d360ed5495eae4a7-005c4bda99 Date: Sat, 26 Jan 2019 03:57:13 GMT root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3 --os-project-name ptest1 --os-project-domain-name default --os-username utest1 --os-user-domain-name default --os-password openstack stat Account: AUTH_e8ed1722599643b5802a322341b4e02c Containers: 1 Objects: 7 Bytes: 108071 Containers in policy \"policy-0\": 1 Objects in policy \"policy-0\": 7 Bytes in policy \"policy-0\": 108071 Meta Quota-Bytes: 123456 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: txc44a58507f4d4b76b07dc-005c4bda9c X-Timestamp: 1548334795.35484 X-Trans-Id: txc44a58507f4d4b76b07dc-005c4bda9c Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# 好了，这样就把 utest1 在 ptest1 和 Default 中的quota 设置为 123456 bytes 了。 HEAD 同理，如果你想要获取 用户 的 metadata，则参考： https://developer.openstack.org/api-ref/object-store/index.html?expanded=show-account-metadata-detail#show-account-metadata root@controller:~# curl -i http://192.168.0.50:8080/v1/AUTH_e8ed1722599643b5802a322341b4e02c -X HEAD -H \"X-Auth-Token: gAAAAABcS_rsWTz2Qhz931Wc-oL7c9QwOvHsTXOKJ3RY6A83HX90MbZ0fhrg2b3Yt_PpXwNgt_SQKHO5RS6HnixW9MAVE0TvSmEcGHn-P8rQrJD1auweHFLC_Jl_hPRcYvRf1Mtl802MFwxuIbevAhd2tEu8q-D5seqZbBgBP8R7HhD6JIlTGdE\" Warning: Setting custom HTTP method to HEAD with -X/--request may not work the Warning: way you want. Consider using -I/--head instead. HTTP/1.1 204 No Content Content-Length: 0 X-Account-Container-Count: 1 X-Account-Object-Count: 7 X-Account-Storage-Policy-Policy-0-Bytes-Used: 108071 X-Account-Storage-Policy-Policy-0-Container-Count: 1 X-Timestamp: 1548334795.35484 X-Account-Storage-Policy-Policy-0-Object-Count: 7 X-Account-Bytes-Used: 108071 X-Account-Meta-Quota-Bytes: 1234567 Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes x-account-project-domain-id: default X-Trans-Id: tx0924923381ba4b13aa118-005c4bfafd X-Openstack-Request-Id: tx0924923381ba4b13aa118-005c4bfafd Date: Sat, 26 Jan 2019 06:15:25 GMT root@controller:~# 上面返回结果中的 X-Account-Meta-Quota-Bytes 就是我们想要的。 python 调用 RESTFUL 啥也不说了，直接上代码： import requests import json URL = 'http://192.168.0.50:5000/v3/auth/tokens' body = { \"auth\": { \"identity\": { \"methods\": [ \"password\" ], \"password\": { \"user\": { \"name\": \"admin\", \"domain\": { \"name\": \"Default\" }, \"password\": \"openstack\" } } }, \"scope\": { \"project\": { \"domain\": { \"name\": \"Default\" }, \"name\": \"admin\" } } } } body = json.dumps(body) headers = {'Content-Type':'application/json'} res = requests.post(URL,data=body,headers=headers) token =res.headers['X-Subject-Token'] # utest1 storage URL swiftproxy=\"192.168.0.50\" storageURL = \"http://\" + swiftproxy + \":8080/v1/AUTH_e8ed1722599643b5802a322341b4e02c\" # set quotabytes = \"654321\" headers = {'X-Auth-Token':token,\"Content-Type\": 'application/json', \"X-Account-Meta-Quota-Bytes\": quotabytes} res = requests.post(storageURL,headers=headers) print(res.status_code) # get headers = {'X-Auth-Token':token,\"Content-Type\": 'application/json'} res_get = requests.get(storageURL,headers=headers) print(res_get.headers['X-Account-Meta-Quota-Bytes']) print(res_get.text) ref https://www.ibm.com/developerworks/community/forums/html/topic?id=813eb4fd-c0c6-43a5-9317-a35e4081ef72 https://docs.openstack.org/swift/latest/middleware.html#module-swift.common.middleware.account_quotas https://docs.openstack.org/swift/latest/middleware.html#module-swift.common.middleware.container_quotas https://docs.openstack.org/swift/latest/#common-configuration https://docs.openstack.org/keystone/rocky/admin/cli-manage-projects-users-and-roles.html https://blog.csdn.net/my_vips/article/details/17919167 https://blog.csdn.net/dysj4099/article/details/8941465 https://blog.csdn.net/my_vips/article/details/25878751 https://developer.openstack.org/api-ref/object-store/?expanded=#create-update-or-delete-container-metadata https://docs.openstack.org/swift/latest/api/container_quotas.html Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/analysis-openstack-swift-overview_erasure_code.html":{"url":"docs/analysis-openstack-swift-overview_erasure_code.html","title":"与 erasure code 相关","keywords":"","body":"+++ title = \"swift with overview-erasure-code\" date = 2019-01-18T00:00:00-08:00 lastmod = 2019-01-19T00:13:57-08:00 tags = [\"swift\", \"openstack\"] categories = [\"swift\"] draft = false weight = 3001 +++ 本文记录一下，关于 swift 与 erasure-code 相关的内容。 关于 erasure code 可以优先看一下 https://blog.csdn.net/chdhust/article/details/78311494 Ref https://docs.openstack.org/swift/queens/overview%5Ferasure%5Fcode.html Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-benchmarking-getput.html":{"url":"docs/swift-benchmarking-getput.html","title":"getput使用","keywords":"","body":"openstack swift benchmarking 工具 getput 使用 env 如前，不再重复。 step 根据 https://docs.openstack.org/swift/queens/associated_projects.html 我们知道有3个 benchmarking 工具 getput - getput tool suite COSbench - COSbench tool suite ssbench - ssbench tool suite 其中后面 COSbench ， ssbench 暂时都没有很好的支持 keystone v3, 并经过一定的尝试后，放弃了。 下面我们只讲 getput 文档 https://github.com/markseger/getput/blob/master/getting-started.txt 这里面有介绍 另一个 Introduction.pdf 中也有类似的 看起来，能展示的东西，可以满足我们的需求 安装 箭头位置报错了。 看了下代码，加一个括号呀。 git clone https://github.com/markseger/getput.git && cd getput/ vim setup.py pip install python-swiftclient pip3 install python-swiftclient python setup.py install 命令示例如下 getput getput -cc -oo -n1 -s1k -tp,d getput -cc -oo -n1 -s10k -tp,g,d getput -cc -oo -n1 -s10k -tp,g,d --procs 2 getput -cc -oo -n1 -s100k -tp,g,d --procs 2 getput -cc -oo -n1 -s1000k -tp,g,d --procs 2 getput -cc -oo -n1 -s100000k -tp,g,d --procs 2 getput -cc -oo -n1 -s100000k -tp,g,d --procs 1 getput -cc -oo -n1 -s10000k -tp,g,d --procs 1 getput -cc -oo -n1 -s10000k -tp,g,d --procs 2 getput -cc -oo -n1 -s10000k -tp,g,d --procs 3 getput -cc -oo -n1 -s1m -tp,g,d --procs 3 参数具体含义，直接看文档吧，也很简单。 ref https://docs.openstack.org/swift/queens/associated_projects.html https://github.com/markseger/getput Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-logfiles.html":{"url":"docs/swift-logfiles.html","title":"组件日志","keywords":"","body":"当 swift 出问题时，我们应该看哪些日志文件 apache2 相关： /var/log/apache2/access.log /var/log/apache2/error.log keystone 相关： 进入 apache2/keystone /var/log/apache2/keystone_access.log /var/log/apache2/keystone.log keystone 自己日志 /var/log/keystone/keystone-wsgi-public.log swift 相关: /var/log/syslog rsyncd 相关： /var/log/rsyncd.log mysql 相关： 如果没有配置log dir,则 log 在系统log（/var/log/syslog）中。 Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-faq.html":{"url":"docs/swift-faq.html","title":"常见问题","keywords":"","body":"swift 常见问题 chrome 访问： http://192.168.0.50/horizon/auth/login/ 返回 Internal Server Error 返回下面信息 Internal Server Error The server encountered an internal error or misconfiguration and was unable to complete your request. Please contact the server administrator at webmaster@localhost to inform them of the time this error occurred, and the actions you performed just before this error. More information about this error may be available in the server error log. Apache/2.4.18 (Ubuntu) Server at 192.168.0.50 Port 80 然后，我想去修改一下 /etc/hosts 文件 E514: write error (file system full?) 写入错误，磁盘满了？ root@controller:/etc# df -h Filesystem Size Used Avail Use% Mounted on udev 3.9G 0 3.9G 0% /dev tmpfs 797M 49M 748M 7% /run /dev/sda1 19G 19G 0 100% / tmpfs 3.9G 0 3.9G 0% /dev/shm tmpfs 5.0M 0 5.0M 0% /run/lock tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup tmpfs 797M 0 797M 0% /run/user/1000 root@controller:/etc# 是满了。 chrome 访问：http://192.168.0.50/horizon/project/containers/ 出现，container 没有展示出来的现象。 然后，我们检查一下 swift 服务 root@controller:~# . demo-openrc root@controller:~# swift list Account GET failed: http://controller:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3?format=json 503 Service Unavailable [first 60 chars of response] Service UnavailableThe server is currently Failed Transaction ID: tx1ecd29acbd4e4c889d19d-005c467241 root@controller:~# 报错了。说明 swift 服务挂了。 去 controller 查一下 swift-proxy 服务 root@controller:/home/ubuntu# service swift-proxy status ● swift-proxy.service - LSB: Swift proxy server Loaded: loaded (/etc/init.d/swift-proxy; bad; vendor preset: enabled) Active: active (running) since Tue 2019-01-22 09:18:24 CST; 3min 34s ago Docs: man:systemd-sysv-generator(8) Process: 1597 ExecStart=/etc/init.d/swift-proxy start (code=exited, status=0/SUCCESS) Tasks: 5 Memory: 102.7M CPU: 3.650s CGroup: /system.slice/swift-proxy.service ├─2250 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─2556 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─2557 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─2558 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf └─2559 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf Jan 22 09:21:50 controller proxy-server[2559]: Account GET returning 503 for [] (txn: tx8149ac2f570a43589cc19-005c46702e) (client_ip: 127.0.0.1) Jan 22 09:21:50 controller proxy-server[2559]: 127.0.0.1 127.0.0.1 22/Jan/2019/01/21/50 GET /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Flimit%3D1001%26format%3Djson HTTP/1.0 503 - python-swiftclie Jan 22 09:21:54 controller proxy-server[2559]: Account HEAD returning 503 for [] (txn: txa6194df8967840519fa19-005c467032) Jan 22 09:21:54 controller proxy-server[2559]: - - 22/Jan/2019/01/21/54 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Fformat%3Djson HTTP/1.0 503 - Swift - - - - txa6194df8967840519fa19-005c4670 Jan 22 09:21:54 controller proxy-server[2559]: Account HEAD returning 503 for [] (txn: tx82330a029ccc4710a669f-005c467032) Jan 22 09:21:54 controller proxy-server[2559]: - - 22/Jan/2019/01/21/54 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Fformat%3Djson HTTP/1.0 503 - Swift - - - - tx82330a029ccc4710a669f-005c4670 Jan 22 09:21:54 controller proxy-server[2559]: Account GET returning 503 for [] (txn: txa6194df8967840519fa19-005c467032) (client_ip: 127.0.0.1) Jan 22 09:21:54 controller proxy-server[2559]: 127.0.0.1 127.0.0.1 22/Jan/2019/01/21/54 GET /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Flimit%3D1001%26format%3Djson HTTP/1.0 503 - python-swiftclie Jan 22 09:21:54 controller proxy-server[2559]: Account GET returning 503 for [] (txn: tx82330a029ccc4710a669f-005c467032) (client_ip: 127.0.0.1) Jan 22 09:21:54 controller proxy-server[2559]: 127.0.0.1 127.0.0.1 22/Jan/2019/01/21/54 GET /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Flimit%3D1001%26format%3Djson HTTP/1.0 503 - python-swiftclie root@controller:~# service swift-proxy restart root@controller:~# service swift-proxy status ● swift-proxy.service - LSB: Swift proxy server Loaded: loaded (/etc/init.d/swift-proxy; bad; vendor preset: enabled) Active: active (running) since Tue 2019-01-22 09:26:56 CST; 6s ago Docs: man:systemd-sysv-generator(8) Process: 4688 ExecStop=/etc/init.d/swift-proxy stop (code=exited, status=0/SUCCESS) Process: 4699 ExecStart=/etc/init.d/swift-proxy start (code=exited, status=0/SUCCESS) Tasks: 5 Memory: 84.2M CPU: 945ms CGroup: /system.slice/swift-proxy.service ├─4711 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─4720 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─4721 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─4722 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf └─4723 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf Jan 22 09:26:53 controller proxy-server[4722]: Starting Keystone auth_token middleware Jan 22 09:26:53 controller proxy-server[4722]: AuthToken middleware is set with keystone_authtoken.service_token_roles_required set to False. This is backwards compatible but deprecated behaviour. Jan 22 09:26:53 controller proxy-server[4723]: Starting Keystone auth_token middleware Jan 22 09:26:53 controller proxy-server[4721]: Starting Keystone auth_token middleware Jan 22 09:26:53 controller proxy-server[4723]: AuthToken middleware is set with keystone_authtoken.service_token_roles_required set to False. This is backwards compatible but deprecated behaviour. Jan 22 09:26:53 controller proxy-server[4721]: AuthToken middleware is set with keystone_authtoken.service_token_roles_required set to False. This is backwards compatible but deprecated behaviour. Jan 22 09:26:56 controller swift-proxy[4699]: Starting proxy-server...(/etc/swift/proxy-server.conf) Jan 22 09:26:56 controller swift-proxy[4699]: No handlers could be found for logger \"keystonemiddleware._common.config\" Jan 22 09:26:56 controller swift-proxy[4699]: ...done. Jan 22 09:26:56 controller systemd[1]: Started LSB: Swift proxy server. root@controller:~# . demo-openrc root@controller:~# swift list Account GET failed: http://controller:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3?format=json 503 Service Unavailable [first 60 chars of response] Service UnavailableThe server is currently Failed Transaction ID: tx1ecd29acbd4e4c889d19d-005c467241 root@controller:~# 重启后，依旧，则要去看一下 storage node 中的 swift-init all status 发现确实是 storage node 的问题，则 swift-init all restart 。 回到 controller node 检查 swift list 就有了，同时，chrome 也有了。 当 swift-proxy status 报错时，应该怎么查问题 proxy node 找问题点 telnet storageIP 6200 如果 telnet 不了，就是对方服务问题，不是 proxy 问题。 storage node 找问题点 telnet storageIP 6200 如果 telnet 不了，就是 服务 问题 服务起来了没有 netstat -tlnp | grep 6200 如果 没有，则服务可能没有启动或启动失败。 swift-init all 检查 swift-init all status swift-init all restart Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-admin-cannot-login.html":{"url":"docs/swift-admin-cannot-login.html","title":"admin被删除","keywords":"","body":"一次 admin 用户在horizon无法登录的事故 env admin 用户在horizon无法登录 controller 节点上： swift stat 无效 openstack endpoint list 无效 openstack 其它操作 无效 后来，才知道：admin用户 被删除了（跑路么...嘿嘿） step 查数据库是否正常 root@ubuntu:/home/administrator# systemctl status mariadb ● mariadb.service - MariaDB 10.2.22 database server Loaded: loaded (/lib/systemd/system/mariadb.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/mariadb.service.d └─migrated-from-my.cnf-settings.conf Active: active (running) since 一 2019-03-04 17:39:31 CST; 17h ago Docs: man:mysqld(8) https://mariadb.com/kb/en/library/systemd/ Process: 2056 ExecStartPost=/bin/sh -c systemctl unset-environment _WSREP_START_POSITION (code=exited, status=0/SUCCESS) Process: 2052 ExecStartPost=/etc/mysql/debian-start (code=exited, status=0/SUCCESS) Process: 1171 ExecStartPre=/bin/sh -c [ ! -e /usr/bin/galera_recovery ] && VAR= || VAR=`/usr/bin/galera_recovery`; [ $? -eq 0 ] && systemctl set-environment _WSREP_START_POSITION=$VAR || exit 1 (code=exited, status=0/SUCCESS) Process: 1139 ExecStartPre=/bin/sh -c systemctl unset-environment _WSREP_START_POSITION (code=exited, status=0/SUCCESS) Process: 1105 ExecStartPre=/usr/bin/install -m 755 -o mysql -g root -d /var/run/mysqld (code=exited, status=0/SUCCESS) Main PID: 1707 (mysqld) Status: \"Taking your SQL requests now...\" Tasks: 36 Memory: 104.8M CPU: 43.268s CGroup: /system.slice/mariadb.service └─1707 /usr/sbin/mysqld 3月 05 10:36:56 ubuntu mysqld[1707]: 2019-03-05 10:36:56 139938802427648 [Warning] Aborted connection 33 to db: 'keystone' user: 'keystone' host: 'controller' (Got timeout reading communication packets) 3月 05 10:43:11 ubuntu mysqld[1707]: 2019-03-05 10:43:11 139939162048256 [Warning] Aborted connection 34 to db: 'keystone' user: 'keystone' host: 'controller' (Got timeout reading communication packets) 3月 05 10:43:11 ubuntu mysqld[1707]: 2019-03-05 10:43:11 139938802124544 [Warning] Aborted connection 37 to db: 'keystone' user: 'keystone' host: 'controller' (Got timeout reading communication packets) 3月 05 10:43:11 ubuntu mysqld[1707]: 2019-03-05 10:43:11 139939161745152 [Warning] Aborted connection 35 to db: 'keystone' user: 'keystone' host: 'controller' (Got timeout reading communication packets) 3月 05 11:00:18 ubuntu mysqld[1707]: 2019-03-05 11:00:18 139939162048256 [Warning] Aborted connection 40 to db: 'keystone' user: 'keystone' host: 'controller' (Got timeout reading communication packets) 3月 05 11:08:11 ubuntu mysqld[1707]: 2019-03-05 11:08:11 139939161745152 [Warning] Aborted connection 38 to db: 'keystone' user: 'keystone' host: 'controller' (Got an error reading communication packets) 3月 05 11:08:11 ubuntu mysqld[1707]: 2019-03-05 11:08:11 139939162351360 [Warning] Aborted connection 42 to db: 'keystone' user: 'keystone' host: 'controller' (Got an error reading communication packets) 3月 05 11:08:11 ubuntu mysqld[1707]: 2019-03-05 11:08:11 139938802427648 [Warning] Aborted connection 41 to db: 'keystone' user: 'keystone' host: 'controller' (Got an error reading communication packets) 3月 05 11:08:11 ubuntu mysqld[1707]: 2019-03-05 11:08:11 139938802124544 [Warning] Aborted connection 39 to db: 'keystone' user: 'keystone' host: 'controller' (Got an error reading communication packets) 3月 05 11:08:11 ubuntu mysqld[1707]: 2019-03-05 11:08:11 139939162048256 [Warning] Aborted connection 43 to db: 'keystone' user: 'keystone' host: 'controller' (Got an error reading communication packets) root@ubuntu:/home/administrator# netstat -tnlp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:25672 0.0.0.0:* LISTEN 1902/beam tcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTEN 1707/mysqld tcp 0 0 192.168.10.13:2379 0.0.0.0:* LISTEN 1115/etcd tcp 0 0 0.0.0.0:11211 0.0.0.0:* LISTEN 1111/memcached tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 2073/python tcp 0 0 0.0.0.0:4369 0.0.0.0:* LISTEN 1521/epmd tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1504/sshd tcp 0 0 127.0.0.1:6010 0.0.0.0:* LISTEN 17959/2 tcp6 0 0 :::5000 :::* LISTEN 18114/apache2 tcp6 0 0 :::5672 :::* LISTEN 1902/beam tcp6 0 0 :::2380 :::* LISTEN 1115/etcd tcp6 0 0 :::80 :::* LISTEN 18114/apache2 tcp6 0 0 :::4369 :::* LISTEN 1521/epmd tcp6 0 0 :::22 :::* LISTEN 1504/sshd tcp6 0 0 ::1:6010 :::* LISTEN 17959/2 root@ubuntu:/home/administrator# mysql -u root -p Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 50 Server version: 10.2.22-MariaDB-10.2.22+maria~xenial-log mariadb.org binary distribution Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MariaDB [(none)]> exit Bye root@ubuntu:/home/administrator# mysql -u keystone -p Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 51 Server version: 10.2.22-MariaDB-10.2.22+maria~xenial-log mariadb.org binary distribution Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MariaDB [(none)]> keystone 用户是正常的呀... 这时，你怎么查，也查不出问题的。。。 后来，同事反馈，因为，测试工程师，不小心，把admin用户给删除了！~ 太尴尬了。 root@ubuntu:/home/administrator# . admin-openrc root@ubuntu:/home/administrator# openstack endpoint list The request you have made requires authentication. (HTTP 401) (Request-ID: req-9107092c-98fd-4462-89a1-bd0ce4900e39) root@ubuntu:/home/administrator# keystone 加 admin 用户 参考 https://docs.openstack.org/keystone/rocky/install/keystone-install-ubuntu.html#install-and-configure-components 第5小步 Bootstrap the Identity service ： root@ubuntu:/home/administrator# keystone-manage bootstrap --bootstrap-password 962ae831854bbd768a2f \\ > --bootstrap-admin-url http://controller:5000/v3/ \\ > --bootstrap-internal-url http://controller:5000/v3/ \\ > --bootstrap-public-url http://controller:5000/v3/ \\ > --bootstrap-region-id RegionOne root@ubuntu:/home/administrator# service apache2 restart 验证 root@ubuntu:/home/administrator# . admin-openrc root@ubuntu:/home/administrator# openstack --os-auth-url http://controller:5000/v3 \\ > --os-project-domain-name Default --os-user-domain-name Default \\ > --os-project-name admin --os-username admin token issue +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2019-03-05T05:11:19+0000 | | id | gAAAAABcffbnfJIQuwWelSU7NkenAkXopEbJW6fqPE2mYPC0z95gMlAptMUsnbVe1UhA4HVnollox_TSYaQHJ8EgFi4SFWeQlJT9H_gVhpih-g_YQY7yetb5HolBv-S_ludzvW6Tr2GSdc3S4fpEZiYJZ3FqGQEBXfnK5VX7fG28xoVw1EsVwL0 | | project_id | 544bf3a68de64987ae3bd92f640facc4 | | user_id | 89bd983df24742ff8892d190a06d5f58 | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ root@ubuntu:/home/administrator# admin用户还原成功了！~ swift stat OK root@ubuntu:/home/administrator# . admin-openrc root@ubuntu:/home/administrator# swift stat -v Account HEAD failed: http://controller:8080/v1/AUTH_544bf3a68de64987ae3bd92f640facc4 503 Service Unavailable Failed Transaction ID: tx3c28aa4aa7264929a015d-005c7dfba0 root@ubuntu:/home/administrator# openstack user list +----------------------------------+-------+ | ID | Name | +----------------------------------+-------+ | 1319e9203c0c4000819b625bf987742c | swift | | 89bd983df24742ff8892d190a06d5f58 | admin | | c3248a1ddb4048349796e4e523d03b54 | demo | +----------------------------------+-------+ root@ubuntu:/home/administrator# openstack role add --project service --user swift admin root@ubuntu:/home/administrator# openstack role list +----------------------------------+-------+ | ID | Name | +----------------------------------+-------+ | a9c6feb32e8d4d099e0140111c9fc137 | admin | | dfbf44be3cfe4f4790944bfc4300a3ba | user | +----------------------------------+-------+ root@ubuntu:/home/administrator# openstack service create --name swift \\ > --description \"OpenStack Object Storage\" object-store +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Object Storage | | enabled | True | | id | 31a05ac2789644b29cea4c561e094f3a | | name | swift | | type | object-store | +-------------+----------------------------------+ root@ubuntu:/home/administrator# openstack endpoint create --region RegionOne \\ > object-store public http://controller:8080/v1/AUTH_%\\(project_id\\)s Multiple service matches found for 'object-store', use an ID to be more specific. root@ubuntu:/home/administrator# openstack endpoint list +----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------------------------+ | 3bd2e6635a1e469ab5284c11d859e4b3 | RegionOne | keystone | identity | True | admin | http://controller:5000/v3/ | | 3fc42de3790b4c5f8d635a9721ed23a6 | RegionOne | swift | object-store | True | public | http://controller:8080/v1/AUTH_%(project_id)s | | 88634b7b1e6c4b0989f079f00adbffb8 | RegionOne | keystone | identity | True | internal | http://controller:5000/v3/ | | 8b4faa17e4404ef4aa709c0e78636742 | RegionOne | swift | object-store | True | internal | http://controller:8080/v1/AUTH_%(project_id)s | | bf0706eea0e04412ae4588ec9dd067a0 | RegionOne | keystone | identity | True | public | http://controller:5000/v3/ | | bfae1eb954c84364803023604999ef93 | RegionOne | swift | object-store | True | admin | http://controller:8080/v1 | +----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------------------------+ root@ubuntu:/home/administrator# openstack endpoint list +----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------------------------+ | 3bd2e6635a1e469ab5284c11d859e4b3 | RegionOne | keystone | identity | True | admin | http://controller:5000/v3/ | | 3fc42de3790b4c5f8d635a9721ed23a6 | RegionOne | swift | object-store | True | public | http://controller:8080/v1/AUTH_%(project_id)s | | 88634b7b1e6c4b0989f079f00adbffb8 | RegionOne | keystone | identity | True | internal | http://controller:5000/v3/ | | 8b4faa17e4404ef4aa709c0e78636742 | RegionOne | swift | object-store | True | internal | http://controller:8080/v1/AUTH_%(project_id)s | | bf0706eea0e04412ae4588ec9dd067a0 | RegionOne | keystone | identity | True | public | http://controller:5000/v3/ | | bfae1eb954c84364803023604999ef93 | RegionOne | swift | object-store | True | admin | http://controller:8080/v1 | +----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------------------------+ root@ubuntu:/home/administrator# openstack endpoint create --region RegionOne \\ > object-store internal http://controller:8080/v1/AUTH_%\\(project_id\\)s Multiple service matches found for 'object-store', use an ID to be more specific. root@ubuntu:/home/administrator# . admin-openrc root@ubuntu:/home/administrator# openstack endpoint create --region RegionOne object-store internal http://controller:8080/v1/AUTH_%\\(project_id\\)s Multiple service matches found for 'object-store', use an ID to be more specific. root@ubuntu:/home/administrator# service swift-proxy status ● swift-proxy.service - LSB: Swift proxy server Loaded: loaded (/etc/init.d/swift-proxy; bad; vendor preset: enabled) Active: active (running) since 一 2019-03-04 17:39:48 CST; 18h ago Docs: man:systemd-sysv-generator(8) Process: 1264 ExecStart=/etc/init.d/swift-proxy start (code=exited, status=0/SUCCESS) Tasks: 2 Memory: 124.6M CPU: 6min 37.175s CGroup: /system.slice/swift-proxy.service ├─2073 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf └─2084 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf 3月 05 12:31:11 ubuntu proxy-server[2084]: Identity response: {\"error\": {\"message\": \"The request you have made requires authentication.\", \"code\": 401, \"title\": \"Unauthorized\"}} 3月 05 12:31:11 ubuntu proxy-server[2084]: Unable to validate token: Identity server rejected authorization necessary to fetch token data 3月 05 12:31:12 ubuntu proxy-server[2084]: 192.168.10.13 192.168.10.13 05/Mar/2019/04/31/12 HEAD /v1/AUTH_544bf3a68de64987ae3bd92f640facc4%3Fformat%3Djson HTTP/1.0 503 - python-swift 3月 05 12:31:28 ubuntu proxy-server[2084]: Identity server rejected authorization 3月 05 12:31:28 ubuntu proxy-server[2084]: Identity response: {\"error\": {\"message\": \"The request you have made requires authentication.\", \"code\": 401, \"title\": \"Unauthorized\"}} 3月 05 12:31:28 ubuntu proxy-server[2084]: Retrying validation 3月 05 12:31:29 ubuntu proxy-server[2084]: Identity server rejected authorization 3月 05 12:31:29 ubuntu proxy-server[2084]: Identity response: {\"error\": {\"message\": \"The request you have made requires authentication.\", \"code\": 401, \"title\": \"Unauthorized\"}} 3月 05 12:31:29 ubuntu proxy-server[2084]: Unable to validate token: Identity server rejected authorization necessary to fetch token data 3月 05 12:31:29 ubuntu proxy-server[2084]: 192.168.10.13 192.168.10.13 05/Mar/2019/04/31/29 HEAD /v1/AUTH_544bf3a68de64987ae3bd92f640facc4%3Fformat%3Djson HTTP/1.0 503 - python-swift root@ubuntu:/home/administrator# service swift-proxy restart root@ubuntu:/home/administrator# service swift-proxy status ● swift-proxy.service - LSB: Swift proxy server Loaded: loaded (/etc/init.d/swift-proxy; bad; vendor preset: enabled) Active: active (running) since 二 2019-03-05 12:36:31 CST; 28s ago Docs: man:systemd-sysv-generator(8) Process: 20857 ExecStop=/etc/init.d/swift-proxy stop (code=exited, status=0/SUCCESS) Process: 20871 ExecStart=/etc/init.d/swift-proxy start (code=exited, status=0/SUCCESS) Tasks: 2 Memory: 88.1M CPU: 3.284s CGroup: /system.slice/swift-proxy.service ├─20882 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf └─20891 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf 3月 05 12:36:28 ubuntu proxy-server[20882]: Started child 20891 3月 05 12:36:28 ubuntu proxy-server[20891]: Adding required filter copy to pipeline at position 10 3月 05 12:36:28 ubuntu proxy-server[20891]: Adding required filter listing_formats to pipeline at position 5 3月 05 12:36:28 ubuntu proxy-server[20891]: Pipeline was modified. New pipeline is \"catch_errors gatekeeper healthcheck proxy-logging cache listing_formats container_sync bulk rateli 3月 05 12:36:28 ubuntu proxy-server[20891]: Starting Keystone auth_token middleware 3月 05 12:36:28 ubuntu proxy-server[20891]: AuthToken middleware is set with keystone_authtoken.service_token_roles_required set to False. This is backwards compatible but deprecated 3月 05 12:36:31 ubuntu swift-proxy[20871]: Starting proxy-server...(/etc/swift/proxy-server.conf) 3月 05 12:36:31 ubuntu swift-proxy[20871]: No handlers could be found for logger \"keystonemiddleware._common.config\" 3月 05 12:36:31 ubuntu swift-proxy[20871]: ...done. 3月 05 12:36:31 ubuntu systemd[1]: Started LSB: Swift proxy server. root@ubuntu:/home/administrator# . admin-openrc root@ubuntu:/home/administrator# swift stat -v StorageURL: http://controller:8080/v1/AUTH_544bf3a68de64987ae3bd92f640facc4 Auth Token: gAAAAABcffz0ZaBYiClGBQWWG80xSRlldlvyAqboD9x6uPgn38DgnZICHzeDCLF7_EJUVX4Uljn_IzQucX6502N85_ZvLQLCs-OFbUrRKu5H7fqMrLrXyNZnZlanXNglghb9VRa8BRUGddxnms2y4EWofDj2XxunYQVuHzWlym37ghCbw8QCrRY Account: AUTH_544bf3a68de64987ae3bd92f640facc4 Containers: 3 Objects: 7 Bytes: 1019964752 Containers in policy \"policy-0\": 3 Objects in policy \"policy-0\": 7 Bytes in policy \"policy-0\": 1019964752 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: txed77851303b14b9f934b5-005c7dfcf4 X-Timestamp: 1550483325.13780 X-Trans-Id: txed77851303b14b9f934b5-005c7dfcf4 Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@ubuntu:/home/administrator# swift list container1 test testtest root@ubuntu:/home/administrator# 现在 swift 已经可以连接上了。 horizon 登录后， 无法获取swift服务信息，重启 memcached 但是 通过 horizon 连接登录后，还是会显示 无法获取swift服务信息 。 因为安装是依据 https://docs.openstack.org/horizon/queens/install/install-ubuntu.html 配置在 /etc/openstack-dashboard/local_settings.py, 无修改。 日志在 apache2 上，看一下： administrator@ubuntu:~$ tail -f /var/log/apache2/access.log 192.168.10.15 - - [05/Mar/2019:14:34:17 +0800] \"GET /horizon/project/containers/ HTTP/1.1\" 200 4781 \"-\" \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\" 192.168.10.15 - - [05/Mar/2019:14:34:17 +0800] \"GET /horizon/i18n/js/horizon+openstack_dashboard/ HTTP/1.1\" 200 108038 \"http://192.168.10.13/horizon/project/containers/\" \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\" 192.168.10.15 - - [05/Mar/2019:14:34:17 +0800] \"GET /horizon/api/swift/info/ HTTP/1.1\" 500 320 \"http://192.168.10.13/horizon/project/containers/\" \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\" 192.168.10.15 - - [05/Mar/2019:14:34:17 +0800] \"GET /horizon/api/swift/containers/ HTTP/1.1\" 500 320 \"http://192.168.10.13/horizon/project/containers/\" \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\" 192.168.10.15 - - [05/Mar/2019:14:34:17 +0800] \"GET /horizon/api/swift/containers/ HTTP/1.1\" 500 320 \"http://192.168.10.13/horizon/project/containers/\" \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\" 192.168.10.15 - - [05/Mar/2019:14:34:17 +0800] \"GET /horizon/header/ HTTP/1.1\" 200 416 \"http://192.168.10.13/horizon/project/containers/\" \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\" GET /horizon/api/swift/info/ HTTP/1.1\" 500 看来 swift 返回不对。看一下apache2, keystone和swift 日志。 /var/log/keystone/keystone-wsgi-public.log root@ubuntu:/var/log/keystone# tail -f /var/log/keystone/keystone-wsgi-public.log 2019-03-05 15:41:33.335 23839 INFO keystone.common.wsgi [req-abe7bad4-5f1c-4981-beea-008cd27d0023 89bd983df24742ff8892d190a06d5f58 - - default -] GET http://controller:5000/v3/users/89bd983df24742ff8892d190a06d5f58/projects 2019-03-05 15:41:34.056 23835 INFO keystone.common.wsgi [req-d61ed24d-6934-471d-bdfd-084e03823608 89bd983df24742ff8892d190a06d5f58 - - default -] GET http://controller:5000/v3/users/89bd983df24742ff8892d190a06d5f58/projects swift 日志 没反应，应该是还没有到这里来。 apache error log： /var/log/apache2/error.log root@ubuntu:/var/log/keystone# tail -f /var/log/apache2/error.log [Tue Mar 05 08:48:48.446270 2019] [wsgi:error] [pid 26080:tid 140389557745408] ERROR openstack_dashboard.api.rest.utils HTTP exception with no status/code [Tue Mar 05 08:48:48.446902 2019] [wsgi:error] [pid 26078:tid 140389482137344] ERROR openstack_dashboard.api.rest.utils HTTP exception with no status/code [Tue Mar 05 08:48:48.447043 2019] [wsgi:error] [pid 26078:tid 140389482137344] Traceback (most recent call last): [Tue Mar 05 08:48:48.447159 2019] [wsgi:error] [pid 26078:tid 140389482137344] File \"/usr/share/openstack-dashboard/openstack_dashboard/api/rest/utils.py\", line 127, in _wrapped [Tue Mar 05 08:48:48.447259 2019] [wsgi:error] [pid 26078:tid 140389482137344] data = function(self, request, *args, **kw) [Tue Mar 05 08:48:48.447369 2019] [wsgi:error] [pid 26078:tid 140389482137344] File \"/usr/share/openstack-dashboard/openstack_dashboard/api/rest/swift.py\", line 40, in get [Tue Mar 05 08:48:48.447473 2019] [wsgi:error] [pid 26078:tid 140389482137344] capabilities = api.swift.swift_get_capabilities(request) [Tue Mar 05 08:48:48.447583 2019] [wsgi:error] [pid 26078:tid 140389482137344] File \"/usr/share/openstack-dashboard/openstack_dashboard/api/swift.py\", line 382, in swift_get_capabilities [Tue Mar 05 08:48:48.447680 2019] [wsgi:error] [pid 26078:tid 140389482137344] return swift_api(request).get_capabilities() [Tue Mar 05 08:48:48.447788 2019] [wsgi:error] [pid 26078:tid 140389482137344] File \"/usr/share/openstack-dashboard/openstack_dashboard/api/swift.py\", line 107, in swift_api [Tue Mar 05 08:48:48.447896 2019] [wsgi:error] [pid 26078:tid 140389482137344] endpoint = base.url_for(request, 'object-store') [Tue Mar 05 08:48:48.448004 2019] [wsgi:error] [pid 26080:tid 140389557745408] Traceback (most recent call last): [Tue Mar 05 08:48:48.448100 2019] [wsgi:error] [pid 26078:tid 140389482137344] File \"/usr/share/openstack-dashboard/openstack_dashboard/api/base.py\", line 341, in url_for [Tue Mar 05 08:48:48.448212 2019] [wsgi:error] [pid 26080:tid 140389557745408] File \"/usr/share/openstack-dashboard/openstack_dashboard/api/rest/utils.py\", line 127, in _wrapped [Tue Mar 05 08:48:48.448309 2019] [wsgi:error] [pid 26078:tid 140389482137344] raise exceptions.ServiceCatalogException(service_type) [Tue Mar 05 08:48:48.448418 2019] [wsgi:error] [pid 26080:tid 140389557745408] data = function(self, request, *args, **kw) [Tue Mar 05 08:48:48.448513 2019] [wsgi:error] [pid 26078:tid 140389482137344] ServiceCatalogException: Invalid service catalog: object-store [Tue Mar 05 08:48:48.448623 2019] [wsgi:error] [pid 26080:tid 140389557745408] File \"/usr/share/openstack-dashboard/openstack_dashboard/api/rest/swift.py\", line 63, in get [Tue Mar 05 08:48:48.449208 2019] [wsgi:error] [pid 26080:tid 140389557745408] containers, has_more = api.swift.swift_get_containers(request) [Tue Mar 05 08:48:48.449310 2019] [wsgi:error] [pid 26080:tid 140389557745408] File \"/usr/share/openstack-dashboard/openstack_dashboard/api/swift.py\", line 141, in swift_get_containers [Tue Mar 05 08:48:48.449409 2019] [wsgi:error] [pid 26080:tid 140389557745408] headers, containers = swift_api(request).get_account(limit=limit + 1, [Tue Mar 05 08:48:48.449518 2019] [wsgi:error] [pid 26080:tid 140389557745408] File \"/usr/share/openstack-dashboard/openstack_dashboard/api/swift.py\", line 107, in swift_api [Tue Mar 05 08:48:48.449616 2019] [wsgi:error] [pid 26080:tid 140389557745408] endpoint = base.url_for(request, 'object-store') [Tue Mar 05 08:48:48.449723 2019] [wsgi:error] [pid 26080:tid 140389557745408] File \"/usr/share/openstack-dashboard/openstack_dashboard/api/base.py\", line 341, in url_for [Tue Mar 05 08:48:48.449820 2019] [wsgi:error] [pid 26080:tid 140389557745408] raise exceptions.ServiceCatalogException(service_type) [Tue Mar 05 08:48:48.449931 2019] [wsgi:error] [pid 26080:tid 140389557745408] ServiceCatalogException: Invalid service catalog: object-store [Tue Mar 05 08:48:48.474587 2019] [wsgi:error] [pid 26078:tid 140389557745408] ERROR openstack_dashboard.api.rest.utils HTTP exception with no status/code [Tue Mar 05 08:48:48.474819 2019] [wsgi:error] [pid 26078:tid 140389557745408] Traceback (most recent call last): [Tue Mar 05 08:48:48.474925 2019] [wsgi:error] [pid 26078:tid 140389557745408] File \"/usr/share/openstack-dashboard/openstack_dashboard/api/rest/utils.py\", line 127, in _wrapped [Tue Mar 05 08:48:48.475053 2019] [wsgi:error] [pid 26078:tid 140389557745408] data = function(self, request, *args, **kw) [Tue Mar 05 08:48:48.475153 2019] [wsgi:error] [pid 26078:tid 140389557745408] File \"/usr/share/openstack-dashboard/openstack_dashboard/api/rest/swift.py\", line 63, in get [Tue Mar 05 08:48:48.475268 2019] [wsgi:error] [pid 26078:tid 140389557745408] containers, has_more = api.swift.swift_get_containers(request) [Tue Mar 05 08:48:48.475378 2019] [wsgi:error] [pid 26078:tid 140389557745408] File \"/usr/share/openstack-dashboard/openstack_dashboard/api/swift.py\", line 141, in swift_get_containers [Tue Mar 05 08:48:48.475490 2019] [wsgi:error] [pid 26078:tid 140389557745408] headers, containers = swift_api(request).get_account(limit=limit + 1, [Tue Mar 05 08:48:48.475589 2019] [wsgi:error] [pid 26078:tid 140389557745408] File \"/usr/share/openstack-dashboard/openstack_dashboard/api/swift.py\", line 107, in swift_api [Tue Mar 05 08:48:48.475737 2019] [wsgi:error] [pid 26078:tid 140389557745408] endpoint = base.url_for(request, 'object-store') [Tue Mar 05 08:48:48.475834 2019] [wsgi:error] [pid 26078:tid 140389557745408] File \"/usr/share/openstack-dashboard/openstack_dashboard/api/base.py\", line 341, in url_for [Tue Mar 05 08:48:48.475960 2019] [wsgi:error] [pid 26078:tid 140389557745408] raise exceptions.ServiceCatalogException(service_type) [Tue Mar 05 08:48:48.476057 2019] [wsgi:error] [pid 26078:tid 140389557745408] ServiceCatalogException: Invalid service catalog: object-store 也就是说，chrome => apache2 => horizon => keystone => swift 中的 horizon => keystone 出现了问题，也就是认证出问题了。 根据这个提示，来到 /usr/share/openstack-dashboard/openstack_dashboard/api/base.py 的 341 行，这个地方，加一些调试信息 def url_for(request, service_type, endpoint_type=None, region=None): endpoint_type = endpoint_type or getattr(settings, 'OPENSTACK_ENDPOINT_TYPE', 'publicURL') fallback_endpoint_type = getattr(settings, 'SECONDARY_ENDPOINT_TYPE', None) catalog = request.user.service_catalog service = get_service_from_catalog(catalog, service_type) print(\"endpoint_type:::\", endpoint_type) print(\"fallback_endpoint_type:::\", fallback_endpoint_type) print(\" catalog, service:::\", catalog) print(\" service:::\", service) if service: if not region: region = request.user.services_region url = get_url_for_service(service, region, endpoint_type) if not url and fallback_endpoint_type: url = get_url_for_service(service, region, fallback_endpoint_type) if url: return url raise exceptions.ServiceCatalogException(service_type) service apache2 restart 重启 apache2 , 输出信息如下： [Tue Mar 05 09:07:26.208745 2019] [wsgi:error] [pid 27029:tid 139698939741952] ('endpoint_type:::', 'publicURL') [Tue Mar 05 09:07:26.208944 2019] [wsgi:error] [pid 27029:tid 139698939741952] ('fallback_endpoint_type:::', None) [Tue Mar 05 09:07:26.209129 2019] [wsgi:error] [pid 27029:tid 139698939741952] (' catalog, service:::', [{u'endpoints': [], u'type': u'object-store', u'id': u'31a05ac2789644b29cea4c561e094f3a', u'name': u'swift'}, {u'endpoints': [{u'url': u'http://controller:8080/v1/AUTH_544bf3a68de64987ae3bd92f640facc4', u'interface': u'public', u'region': u'RegionOne', u'region_id': u'RegionOne', u'id': u'3fc42de3790b4c5f8d635a9721ed23a6'}, {u'url': u'http://controller:8080/v1/AUTH_544bf3a68de64987ae3bd92f640facc4', u'interface': u'internal', u'region': u'RegionOne', u'region_id': u'RegionOne', u'id': u'8b4faa17e4404ef4aa709c0e78636742'}, {u'url': u'http://controller:8080/v1', u'interface': u'admin', u'region': u'RegionOne', u'region_id': u'RegionOne', u'id': u'bfae1eb954c84364803023604999ef93'}], u'type': u'object-store', u'id': u'c9c91a6a09f34a129dcf587f5e006e33', u'name': u'swift'}, {u'endpoints': [{u'url': u'http://controller:5000/v3/', u'interface': u'admin', u'region': u'RegionOne', u'region_id': u'RegionOne', u'id': u'093cee60e5a144e3a65441fa2db6511e'}, {u'url': u'http://controller:5000/v3/', u'interface': u'internal', u'region': u'RegionOne', u'region_id': u'RegionOne', u'id': u'88634b7b1e6c4b0989f079f00adbffb8'}, {u'url': u'http://controller:5000/v3/', u'interface': u'public', u'region': u'RegionOne', u'region_id': u'RegionOne', u'id': u'bf0706eea0e04412ae4588ec9dd067a0'}], u'type': u'identity', u'id': u'e16c2dfb748942c98f57b7cbe3c522c7', u'name': u'keystone'}]) [Tue Mar 05 09:07:26.209279 2019] [wsgi:error] [pid 27029:tid 139698939741952] (' service:::', {u'endpoints': [], u'type': u'object-store', u'id': u'31a05ac2789644b29cea4c561e094f3a', u'name': u'swift'}) [Tue Mar 05 09:07:26.210264 2019] [wsgi:error] [pid 27029:tid 139698939741952] 可以看到，catalog 与 service 中的 endpoint 为空。说明有空的。 root@ubuntu:/etc/apache2# openstack catalog list +----------+--------------+-----------------------------------------------------------------------------+ | Name | Type | Endpoints | +----------+--------------+-----------------------------------------------------------------------------+ | swift | object-store | | | swift | object-store | RegionOne | | | | public: http://controller:8080/v1/AUTH_544bf3a68de64987ae3bd92f640facc4 | | | | RegionOne | | | | internal: http://controller:8080/v1/AUTH_544bf3a68de64987ae3bd92f640facc4 | | | | RegionOne | | | | admin: http://controller:8080/v1 | | | | | | keystone | identity | RegionOne | | | | admin: http://controller:5000/v3/ | | | | RegionOne | | | | internal: http://controller:5000/v3/ | | | | RegionOne | | | | public: http://controller:5000/v3/ | | | | | +----------+--------------+-----------------------------------------------------------------------------+ root@ubuntu:/etc/apache2# openstack service list +----------------------------------+----------+--------------+ | ID | Name | Type | +----------------------------------+----------+--------------+ | 1d812c20c2a34a4f93b9ce317d004de6 | swift | object-store | | c9c91a6a09f34a129dcf587f5e006e33 | swift | object-store | | e16c2dfb748942c98f57b7cbe3c522c7 | keystone | identity | +----------------------------------+----------+--------------+ 那有可能系统取了第一个的 catalog , 尝试把它删除 root@ubuntu:/etc/apache2# openstack catalog show swift +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | endpoints | | | id | 1d812c20c2a34a4f93b9ce317d004de6 | | name | swift | | type | object-store | +-----------+----------------------------------+ root@ubuntu:/etc/apache2# openstack catalog delete 1d812c20c2a34a4f93b9ce317d004de6 openstack: 'catalog delete 1d812c20c2a34a4f93b9ce317d004de6' is not an openstack command. See 'openstack --help'. Did you mean one of these? catalog list catalog show console log show console url show 注意观察此 id 不是catalog 的，而是 service 的，那就把这个service 删除。 root@ubuntu:/etc/apache2# openstack service show 1d812c20c2a34a4f93b9ce317d004de6 +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Object Storage2 | | enabled | True | | id | 1d812c20c2a34a4f93b9ce317d004de6 | | name | swift | | type | object-store | +-------------+----------------------------------+ root@ubuntu:/etc/apache2# openstack service delete 1d812c20c2a34a4f93b9ce317d004de6 root@ubuntu:/etc/apache2# openstack catalog list +----------+--------------+-----------------------------------------------------------------------------+ | Name | Type | Endpoints | +----------+--------------+-----------------------------------------------------------------------------+ | swift | object-store | RegionOne | | | | public: http://controller:8080/v1/AUTH_544bf3a68de64987ae3bd92f640facc4 | | | | RegionOne | | | | internal: http://controller:8080/v1/AUTH_544bf3a68de64987ae3bd92f640facc4 | | | | RegionOne | | | | admin: http://controller:8080/v1 | | | | | | keystone | identity | RegionOne | | | | admin: http://controller:5000/v3/ | | | | RegionOne | | | | internal: http://controller:5000/v3/ | | | | RegionOne | | | | public: http://controller:5000/v3/ | | | | | +----------+--------------+-----------------------------------------------------------------------------+ root@ubuntu:/etc/apache2# service apache2 restart 重启 apache2 , 输出信息如下： [Tue Mar 05 09:07:26.208745 2019] [wsgi:error] [pid 27029:tid 139698939741952] ('endpoint_type:::', 'publicURL') [Tue Mar 05 09:07:26.208944 2019] [wsgi:error] [pid 27029:tid 139698939741952] ('fallback_endpoint_type:::', None) [Tue Mar 05 09:07:26.209129 2019] [wsgi:error] [pid 27029:tid 139698939741952] (' catalog, service:::', [{u'endpoints': [], u'type': u'object-store', u'id': u'31a05ac2789644b29cea4c561e094f3a', u'name': u'swift'}, {u'endpoints': [{u'url': u'http://controller:8080/v1/AUTH_544bf3a68de64987ae3bd92f640facc4', u'interface': u'public', u'region': u'RegionOne', u'region_id': u'RegionOne', u'id': u'3fc42de3790b4c5f8d635a9721ed23a6'}, {u'url': u'http://controller:8080/v1/AUTH_544bf3a68de64987ae3bd92f640facc4', u'interface': u'internal', u'region': u'RegionOne', u'region_id': u'RegionOne', u'id': u'8b4faa17e4404ef4aa709c0e78636742'}, {u'url': u'http://controller:8080/v1', u'interface': u'admin', u'region': u'RegionOne', u'region_id': u'RegionOne', u'id': u'bfae1eb954c84364803023604999ef93'}], u'type': u'object-store', u'id': u'c9c91a6a09f34a129dcf587f5e006e33', u'name': u'swift'}, {u'endpoints': [{u'url': u'http://controller:5000/v3/', u'interface': u'admin', u'region': u'RegionOne', u'region_id': u'RegionOne', u'id': u'093cee60e5a144e3a65441fa2db6511e'}, {u'url': u'http://controller:5000/v3/', u'interface': u'internal', u'region': u'RegionOne', u'region_id': u'RegionOne', u'id': u'88634b7b1e6c4b0989f079f00adbffb8'}, {u'url': u'http://controller:5000/v3/', u'interface': u'public', u'region': u'RegionOne', u'region_id': u'RegionOne', u'id': u'bf0706eea0e04412ae4588ec9dd067a0'}], u'type': u'identity', u'id': u'e16c2dfb748942c98f57b7cbe3c522c7', u'name': u'keystone'}]) [Tue Mar 05 09:07:26.209279 2019] [wsgi:error] [pid 27029:tid 139698939741952] (' service:::', {u'endpoints': [], u'type': u'object-store', u'id': u'31a05ac2789644b29cea4c561e094f3a', u'name': u'swift'}) [Tue Mar 05 09:07:26.210264 2019] [wsgi:error] [pid 27029:tid 139698939741952] 什么情况，居然，没有变化。那明显不对呀。什么问题...缓存... service memcached restart OK。这下成功了。刷新 chrome ，可以得到正确的结果了。 Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-controller-change-ip.html":{"url":"docs/swift-controller-change-ip.html","title":"controller节点更换IP","keywords":"","body":"controller node 更换 IP root@controller:~# . demo-openrc root@controller:~# swift stat ^C Aborted root@controller:~# cat /etc/hosts 127.0.0.1 localhost 127.0.1.1 controller # controller 192.168.100.50 controller # compute1 192.168.100.61 compute1 # block1 192.168.100.101 block1 # controller #192.168.0.51 controller # storage 192.168.0.198 swift0198 192.168.0.180 swift0180 192.168.0.135 swift0135 # The following lines are desirable for IPv6 capable hosts ::1 localhost ip6-localhost ip6-loopback ff02::1 ip6-allnodes ff02::2 ip6-allrouters root@controller:~# service swift-proxy status ● swift-proxy.service - LSB: Swift proxy server Loaded: loaded (/etc/init.d/swift-proxy; bad; vendor preset: enabled) Active: active (running) since Thu 2019-01-10 11:16:57 CST; 2min 38s ago Docs: man:systemd-sysv-generator(8) Process: 5510 ExecStart=/etc/init.d/swift-proxy start (code=exited, status=0/SUCCESS) Tasks: 5 Memory: 88.5M CPU: 2.874s CGroup: /system.slice/swift-proxy.service ├─5520 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─5529 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─5530 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─5531 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf └─5532 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf Jan 10 11:18:07 controller proxy-server[5530]: STDERR: ERROR:root:Error limiting server controller:11211 (txn: tx6d251040de4d487c8441b-005c Jan 10 11:18:07 controller proxy-server[5530]: - - 10/Jan/2019/03/18/07 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Fformat%3Djson HTTP Jan 10 11:18:08 controller keystoneauth.identity.generic.base[5530]: Failed to discover available identity versions when contacting http:// Jan 10 11:18:08 controller proxy-server[5530]: 127.0.0.1 127.0.0.1 10/Jan/2019/03/18/08 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Ffo Jan 10 11:18:08 controller proxy-server[5530]: Error: An error occurred: #012Traceback (most recent call last):#012 File \"/usr/lib/python2 Jan 10 11:18:16 controller proxy-server[5530]: Account HEAD returning 503 for [] (txn: tx95a396094b87489bb2393-005c36b978) Jan 10 11:18:16 controller proxy-server[5530]: - - 10/Jan/2019/03/18/16 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Fformat%3Djson HTTP Jan 10 11:18:17 controller keystoneauth.identity.generic.base[5530]: Failed to discover available identity versions when contacting http:// Jan 10 11:18:17 controller proxy-server[5530]: 127.0.0.1 127.0.0.1 10/Jan/2019/03/18/17 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Ffo Jan 10 11:18:17 controller proxy-server[5530]: Error: An error occurred: #012Traceback (most recent call last):#012 File \"/usr/lib/python2 root@controller:~# netstat -nlpt Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:8774 0.0.0.0:* LISTEN 2711/python2 tcp 0 0 0.0.0.0:8775 0.0.0.0:* LISTEN 2711/python2 tcp 0 0 0.0.0.0:9191 0.0.0.0:* LISTEN 2686/python2 tcp 0 0 0.0.0.0:25672 0.0.0.0:* LISTEN 2089/beam.smp tcp 0 0 192.168.100.50:3306 0.0.0.0:* LISTEN 2064/mysqld tcp 0 0 192.168.100.50:11211 0.0.0.0:* LISTEN 5494/memcached tcp 0 0 192.168.100.50:2379 0.0.0.0:* LISTEN 1396/etcd tcp 0 0 0.0.0.0:9292 0.0.0.0:* LISTEN 2698/python2 tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 5520/python tcp 0 0 0.0.0.0:4369 0.0.0.0:* LISTEN 1828/epmd tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1619/sshd tcp 0 0 127.0.0.1:6010 0.0.0.0:* LISTEN 2518/0 tcp 0 0 0.0.0.0:9696 0.0.0.0:* LISTEN 2691/python2 tcp 0 0 0.0.0.0:6080 0.0.0.0:* LISTEN 2703/python2 tcp6 0 0 :::5672 :::* LISTEN 2089/beam.smp tcp6 0 0 :::5000 :::* LISTEN 2221/apache2 tcp6 0 0 :::8776 :::* LISTEN 2221/apache2 tcp6 0 0 :::8778 :::* LISTEN 2221/apache2 tcp6 0 0 :::2380 :::* LISTEN 1396/etcd tcp6 0 0 :::80 :::* LISTEN 2221/apache2 tcp6 0 0 :::4369 :::* LISTEN 1828/epmd tcp6 0 0 :::22 :::* LISTEN 1619/sshd tcp6 0 0 ::1:6010 :::* LISTEN 2518/0 root@controller:~# 看到上面，主要就是 mysql , memcached, etcd 三个进程使用了 192.168.100.50，现在我们要把这些IP, 更换成 192.168.0.51 到 https://docs.openstack.org/install-guide/environment.html 找到相应的配置文件，修改这些配置至新的controller node IPv6 如mysql 就是 https://docs.openstack.org/install-guide/environment-sql-database-ubuntu.html mysql 替换如下： root@controller:~# cat /etc/mysql/mariadb.conf.d/99-openstack.cnf [mysqld] feedback=ON innodb_use_sys_malloc = 1 bind-address = 192.168.100.50 default-storage-engine = innodb innodb_file_per_table = on max_connections = 4096 collation-server = utf8_general_ci character-set-server = utf8 root@controller:~# root@controller:~# grep \"192.168.100.50\" -rl /etc/mysql/mariadb.conf.d/99-openstack.cnf | xargs sed -i \"s/192.168.100.50/192.168.0.51/g\" root@controller:~# cat /etc/mysql/mariadb.conf.d/99-openstack.cnf [mysqld] feedback=ON innodb_use_sys_malloc = 1 bind-address = 192.168.0.51 default-storage-engine = innodb innodb_file_per_table = on max_connections = 4096 collation-server = utf8_general_ci character-set-server = utf8 root@controller:~# memcached 替换如下： root@controller:~# grep \"192.168.100.50\" -rl /etc/memcached.conf | xargs sed -i \"s/192.168.100.50/192.168.0.51/g\" root@controller:~# grep \"192.168.100.50\" -rl /etc/memcached.conf root@controller:~# grep \"192.168.0.51\" -rl /etc/memcached.conf /etc/memcached.conf root@controller:~# service memcached restart root@controller:~# service memcached status ● memcached.service - memcached daemon Loaded: loaded (/lib/systemd/system/memcached.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2019-01-10 11:33:20 CST; 3s ago Main PID: 5871 (memcached) Tasks: 6 Memory: 480.0K CPU: 5ms CGroup: /system.slice/memcached.service └─5871 /usr/bin/memcached -m 64 -p 11211 -u memcache -l 192.168.0.51 Jan 10 11:33:20 controller systemd[1]: Started memcached daemon. root@controller:~# etcd 替换如下： root@controller:~# grep \"192.168.100.50\" -rl /etc/default/etcd | xargs sed -i \"s/192.168.100.50/192.168.0.51/g\" root@controller:~# systemctl restart etcd root@controller:~# systemctl status etcd ● etcd.service - etcd - highly-available key value store Loaded: loaded (/lib/systemd/system/etcd.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2019-01-10 11:35:51 CST; 5s ago Docs: https://github.com/coreos/etcd man:etcd Main PID: 5922 (etcd) Tasks: 10 Memory: 6.7M CPU: 248ms CGroup: /system.slice/etcd.service └─5922 /usr/bin/etcd Jan 10 11:35:51 controller etcd[5922]: a591474479d0011e became follower at term 20 Jan 10 11:35:51 controller etcd[5922]: newRaft a591474479d0011e [peers: [a591474479d0011e], term: 20, commit: 117159, applied: 110011, lastindex: 117159, lastterm: 20] Jan 10 11:35:51 controller etcd[5922]: starting server... [version: 2.2.5, cluster version: 2.2] Jan 10 11:35:51 controller systemd[1]: Started etcd - highly-available key value store. Jan 10 11:35:51 controller etcd[5922]: a591474479d0011e is starting a new election at term 20 Jan 10 11:35:51 controller etcd[5922]: a591474479d0011e became candidate at term 21 Jan 10 11:35:51 controller etcd[5922]: a591474479d0011e received vote from a591474479d0011e at term 21 Jan 10 11:35:51 controller etcd[5922]: a591474479d0011e became leader at term 21 Jan 10 11:35:51 controller etcd[5922]: raft.node: a591474479d0011e elected leader a591474479d0011e at term 21 Jan 10 11:35:51 controller etcd[5922]: published {Name:controller ClientURLs:[http://192.168.0.51:2379]} to cluster 18c3b45aeb9b5e12 root@controller:~# 做了这些更换，再检查一下： root@controller:/home/ubuntu# netstat -tlnp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 192.168.100.50:3306 0.0.0.0:* LISTEN 2064/mysqld tcp 0 0 192.168.0.51:2379 0.0.0.0:* LISTEN 5922/etcd tcp 0 0 192.168.0.51:11211 0.0.0.0:* LISTEN 5871/memcached root@controller:/home/ubuntu# 发现mysql 还是没有更换成功。为什么？ 再重启 root@controller:~# service mysql restart root@controller:~# netstat -tlnp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 192.168.100.50:3306 0.0.0.0:* LISTEN 2064/mysqld root@controller:~# 涛声依旧 root@controller:~# service mysql stop root@controller:~# netstat -tlnp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 192.168.100.50:3306 0.0.0.0:* LISTEN 2064/mysqld root@controller:~# 涛声依旧 这只能够 kill -9 了。 root@controller:~# kill -9 2064 root@controller:~# netstat -tlnp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 192.168.0.51:3306 0.0.0.0:* LISTEN 6713/mysqld root@controller:~# 这里终于正常了，有我们想要的IP。 这个时候，要再次重启一下。 root@controller:~# service mysql status ● mysql.service - LSB: Start and stop the mysql database server daemon Loaded: loaded (/etc/init.d/mysql; bad; vendor preset: enabled) Active: active (running) since Thu 2019-01-10 11:55:02 CST; 1min 34s ago Docs: man:systemd-sysv-generator(8) Process: 6623 ExecStop=/etc/init.d/mysql stop (code=exited, status=1/FAILURE) Process: 6663 ExecStart=/etc/init.d/mysql start (code=exited, status=0/SUCCESS) Tasks: 41 Memory: 129.2M CPU: 634ms CGroup: /system.slice/mysql.service ├─1890 /bin/bash /usr/bin/mysqld_safe ├─6713 /usr/sbin/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib/mysql/plugin --user=mysql --skip-log-error --pid-file=/var/run/mysqld/mysqld.pid --socke └─6714 logger -t mysqld -p daemon error Jan 10 11:55:41 controller mysqld[6714]: 190110 11:55:41 [Note] InnoDB: Completed initialization of buffer pool Jan 10 11:55:41 controller mysqld[6714]: 190110 11:55:41 [Note] InnoDB: Highest supported file format is Barracuda. Jan 10 11:55:41 controller mysqld[6714]: 190110 11:55:41 [Note] InnoDB: The log sequence numbers 28098537 and 28098537 in ibdata files do not match the log sequence number 29193113 in t Jan 10 11:55:41 controller mysqld[6714]: 190110 11:55:41 [Note] InnoDB: Restoring possible half-written data pages from the doublewrite buffer... Jan 10 11:55:41 controller mysqld[6714]: 190110 11:55:41 [Note] InnoDB: 128 rollback segment(s) are active. Jan 10 11:55:41 controller mysqld[6714]: 190110 11:55:41 [Note] InnoDB: Waiting for purge to start Jan 10 11:55:41 controller mysqld[6714]: 190110 11:55:41 [Note] InnoDB: Percona XtraDB (http://www.percona.com) 5.6.39-83.1 started; log sequence number 29193113 Jan 10 11:55:41 controller mysqld[6714]: 190110 11:55:41 [Note] Server socket created on IP: '192.168.0.51'. Jan 10 11:55:41 controller mysqld[6714]: 190110 11:55:41 [Note] /usr/sbin/mysqld: ready for connections. Jan 10 11:55:41 controller mysqld[6714]: Version: '10.0.36-MariaDB-0ubuntu0.16.04.1' socket: '/var/run/mysqld/mysqld.sock' port: 3306 Ubuntu 16.04 root@controller:~# service mysql restart root@controller:~# service mysql status ● mysql.service - LSB: Start and stop the mysql database server daemon Loaded: loaded (/etc/init.d/mysql; bad; vendor preset: enabled) Active: active (running) since Thu 2019-01-10 11:56:49 CST; 1s ago Docs: man:systemd-sysv-generator(8) Process: 6783 ExecStop=/etc/init.d/mysql stop (code=exited, status=1/FAILURE) Process: 6808 ExecStart=/etc/init.d/mysql start (code=exited, status=0/SUCCESS) Tasks: 42 Memory: 129.3M CPU: 39ms CGroup: /system.slice/mysql.service ├─1890 /bin/bash /usr/bin/mysqld_safe ├─6713 /usr/sbin/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib/mysql/plugin --user=mysql --skip-log-error --pid-file=/var/run/mysqld/mysqld.pid --socke └─6714 logger -t mysqld -p daemon error Jan 10 11:56:49 controller systemd[1]: mysql.service: Failed with result 'exit-code'. Jan 10 11:56:49 controller systemd[1]: Starting LSB: Start and stop the mysql database server daemon... Jan 10 11:56:49 controller mysql[6808]: * Starting MariaDB database server mysqld Jan 10 11:56:49 controller mysql[6808]: ...done. Jan 10 11:56:49 controller systemd[1]: Started LSB: Start and stop the mysql database server daemon. root@controller:~# root@controller:~# mysql -u root -p Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 21 Server version: 10.0.36-MariaDB-0ubuntu0.16.04.1 Ubuntu 16.04 Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MariaDB [(none)]> MariaDB [(none)]> Bye root@controller:~# 现在再回到刚刚的 https://docs.openstack.org/swift/queens/install/verify.html root@controller:~# swift stat Auth version 1.0 requires ST_AUTH, ST_USER, and ST_KEY environment variables to be set or overridden with -A, -U, or -K. Auth version 2.0 requires OS_AUTH_URL, OS_USERNAME, OS_PASSWORD, and OS_TENANT_NAME OS_TENANT_ID to be set or overridden with --os-auth-url, --os-username, --os-password, --os-tenant-name or os-tenant-id. Note: adding \"-V 2\" is necessary for this. root@controller:~# 这里又是另一个问题了哈！~ root@controller:~# service swift-proxy status ● swift-proxy.service - LSB: Swift proxy server Loaded: loaded (/etc/init.d/swift-proxy; bad; vendor preset: enabled) Active: active (running) since Thu 2019-01-10 13:17:11 CST; 13s ago Docs: man:systemd-sysv-generator(8) Process: 8761 ExecStop=/etc/init.d/swift-proxy stop (code=exited, status=0/SUCCESS) Process: 8773 ExecStart=/etc/init.d/swift-proxy start (code=exited, status=0/SUCCESS) Tasks: 5 Memory: 88.2M CPU: 1.200s CGroup: /system.slice/swift-proxy.service ├─8784 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─8793 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─8794 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─8795 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf └─8796 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf Jan 10 13:17:21 controller proxy-server[8796]: ERROR Insufficient Storage 192.168.0.180:6202/sdc (txn: tx052351248e9443ff9d8f8-005c36d561) Jan 10 13:17:21 controller proxy-server[8796]: ERROR Insufficient Storage 192.168.0.135:6202/sdc (txn: tx052351248e9443ff9d8f8-005c36d561) Jan 10 13:17:21 controller proxy-server[8796]: Account HEAD returning 503 for [507, 507, 507] (txn: tx052351248e9443ff9d8f8-005c36d561) Jan 10 13:17:21 controller proxy-server[8796]: - - 10/Jan/2019/05/17/21 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Fformat%3Djson HTTP/1.0 503 - Swift - - - - tx052351248e9443ff9d8f8- Jan 10 13:17:23 controller proxy-server[8796]: Account HEAD returning 503 for [] (txn: tx052351248e9443ff9d8f8-005c36d561) (client_ip: 192.168.0.51) Jan 10 13:17:23 controller proxy-server[8796]: 192.168.0.51 192.168.0.51 10/Jan/2019/05/17/23 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Fformat%3Djson HTTP/1.0 503 - python-swiftclie Jan 10 13:17:24 controller proxy-server[8796]: Account HEAD returning 503 for [] (txn: tx409727b312c54262bb43a-005c36d564) Jan 10 13:17:24 controller proxy-server[8796]: - - 10/Jan/2019/05/17/24 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Fformat%3Djson HTTP/1.0 503 - Swift - - - - tx409727b312c54262bb43a- Jan 10 13:17:24 controller proxy-server[8796]: Account HEAD returning 503 for [] (txn: tx409727b312c54262bb43a-005c36d564) (client_ip: 192.168.0.51) Jan 10 13:17:24 controller proxy-server[8796]: 192.168.0.51 192.168.0.51 10/Jan/2019/05/17/24 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Fformat%3Djson HTTP/1.0 503 - python-swiftclie root@controller:~# 看这个问题，应该是与 \"ERROR Insufficient Storage 192.168.0.180:6202/sdc\" 相关了。一看就知道了，我们这里是配置错了，应该是 sdb 而不是 sdc 。所以，应该是我们在配置 ring.gz 的时候弄错了！ 重复 https://docs.openstack.org/swift/queens/install/initial-rings.html 过程，重新 rebalance 吧。 再来 root@controller:~# swift stat Auth version 1.0 requires ST_AUTH, ST_USER, and ST_KEY environment variables to be set or overridden with -A, -U, or -K. Auth version 2.0 requires OS_AUTH_URL, OS_USERNAME, OS_PASSWORD, and OS_TENANT_NAME OS_TENANT_ID to be set or overridden with --os-auth-url, --os-username, --os-password, --os-tenant-name or os-tenant-id. Note: adding \"-V 2\" is necessary for this. root@controller:~# 这个问题是在 controller node 中的 /etc/swift/proxy-server.conf 中的 auth_url和memcached_servers配置的问题，配置成下面这样就可以了。 root@controller:~# grep controller -rn /etc/swift/proxy-server.conf 365:www_authenticate_uri = http://controller:5000 366:auth_url = http://controller:5000 367:memcached_servers = controller:11211 472:memcache_servers = controller:11211 root@controller:~# 重新启动 swift-proxy 。 这时候，果然成功了。 root@controller:~# . demo-openrc root@controller:~# swift stat Account: AUTH_7d6eaa90d74a4f239963933c3a744df3 Containers: 0 Objects: 0 Bytes: 0 X-Put-Timestamp: 1547100678.74534 X-Timestamp: 1547100678.74534 X-Trans-Id: txc0d3298c805c45a9afb35-005c36e203 Content-Type: text/plain; charset=utf-8 X-Openstack-Request-Id: txc0d3298c805c45a9afb35-005c36e203 root@controller:~# 但是在创建 container 的时候出错了！ root@controller:~# openstack container create container1 Internal Server Error (HTTP 500) (Request-ID: txb93d09f0ee1a4e9fbe9c7-005c36e410) root@controller:~# service swift-proxy status ● swift-proxy.service - LSB: Swift proxy server Loaded: loaded (/etc/init.d/swift-proxy; bad; vendor preset: enabled) Active: active (running) since Thu 2019-01-10 14:10:23 CST; 9min ago Docs: man:systemd-sysv-generator(8) Process: 9674 ExecStop=/etc/init.d/swift-proxy stop (code=exited, status=0/SUCCESS) Process: 9686 ExecStart=/etc/init.d/swift-proxy start (code=exited, status=0/SUCCESS) Tasks: 5 Memory: 98.8M CPU: 9.731s CGroup: /system.slice/swift-proxy.service ├─9697 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─9706 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─9707 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─9708 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf └─9709 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf Jan 10 14:12:58 controller proxy-server[9706]: Container GET returning 503 for (503, 503, 503) (txn: tx8f9df230a2f544808d3a5-005c36e26a) (client_ip: 192.168.0.51) Jan 10 14:12:58 controller proxy-server[9706]: Could not autocreate account '/AUTH_7d6eaa90d74a4f239963933c3a744df3' (txn: tx8f9df230a2f544808d3a5-005c36e26a) (client_ip: 192.168.0.51) Jan 10 14:12:58 controller proxy-server[9706]: 192.168.0.51 192.168.0.51 10/Jan/2019/06/12/58 PUT /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3/container1 HTTP/1.0 500 - python-swiftclient-3.5 Jan 10 14:20:01 controller proxy-server[9708]: - - 10/Jan/2019/06/20/01 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Fformat%3Djson HTTP/1.0 200 - Swift - - - - txb93d09f0ee1a4e9fbe9c7- Jan 10 14:20:01 controller proxy-server[9708]: ERROR 500 Trying to PUT /AUTH_7d6eaa90d74a4f239963933c3a744df3 From Container Server 192.168.0.198:6202/sdb (txn: txb93d09f0ee1a4e9fbe9c7-005 Jan 10 14:20:01 controller proxy-server[9708]: ERROR 500 Trying to PUT /AUTH_7d6eaa90d74a4f239963933c3a744df3 From Container Server 192.168.0.135:6202/sdb (txn: txb93d09f0ee1a4e9fbe9c7-005 Jan 10 14:20:01 controller proxy-server[9708]: ERROR 500 Trying to PUT /AUTH_7d6eaa90d74a4f239963933c3a744df3 From Container Server 192.168.0.180:6202/sdb (txn: txb93d09f0ee1a4e9fbe9c7-005 Jan 10 14:20:01 controller proxy-server[9708]: Container GET returning 503 for (503, 503, 503) (txn: txb93d09f0ee1a4e9fbe9c7-005c36e410) (client_ip: 192.168.0.51) Jan 10 14:20:01 controller proxy-server[9708]: Could not autocreate account '/AUTH_7d6eaa90d74a4f239963933c3a744df3' (txn: txb93d09f0ee1a4e9fbe9c7-005c36e410) (client_ip: 192.168.0.51) Jan 10 14:20:01 controller proxy-server[9708]: 192.168.0.51 192.168.0.51 10/Jan/2019/06/20/01 PUT /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3/container1 HTTP/1.0 500 - osc-lib/1.9.0%20keysto root@controller:~# 这个就，搞大发了，又出错了。要不要这样对我！~ ERROR 500 Trying to PUT /AUTH_7d6eaa90d74a4f239963933c3a744df3 From Container Server 192.168.0.198:6202/sdb 这里说的是 \"Container Server 192.168.0.198:6202\" , 但是正确的对应是下面这样的： object.builder 6200 container.builder 6201 account.builder 6202 为什么？ 然后，没有思路了。 选择重新启动所有的node。 但是，其中2个storage node 挂了，要重新烧一下盘！~悲剧！ 按照肖工给的流程，重新烧好了！ 然后，重新配置 storage node ,然后上面这个问题就消失了，哈哈。 那就可以了！ Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-demo-cannot-get-user-info.html":{"url":"docs/swift-demo-cannot-get-user-info.html","title":"无法获取用户信息","keywords":"","body":"env 当demo用户登录 horizone , 访问 http://controller/horizon/identity/users/ 时 提示 \"Error: 无法获取用户信息\" step 因为我们知道，demo是 属于: project: demo user: demo role: user 那么，我们只要把 demo(project)中的 demo(user)赋予 admin(role)，就可以了。 root@ubuntu:/home/administrator# openstack project list +----------------------------------+---------+ | ID | Name | +----------------------------------+---------+ | 10636c6cebef4643b336d8edee168fc7 | demo | | 544bf3a68de64987ae3bd92f640facc4 | admin | | bac2c98b90af448da37507f939f6fb33 | test1 | | c944137b50c34ecd9f03f8911e817209 | service | +----------------------------------+---------+ root@ubuntu:/home/administrator# openstack role list +----------------------------------+-------+ | ID | Name | +----------------------------------+-------+ | a9c6feb32e8d4d099e0140111c9fc137 | admin | | dfbf44be3cfe4f4790944bfc4300a3ba | user | +----------------------------------+-------+ root@ubuntu:/home/administrator# openstack role show user +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | None | | id | dfbf44be3cfe4f4790944bfc4300a3ba | | name | user | +-----------+----------------------------------+ root@ubuntu:/home/administrator# openstack role add --project demo --user demo admin root@ubuntu:/home/administrator# ref https://docs.openstack.org/keystone/rocky/install/keystone-openrc-ubuntu.html Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-faq-auth-version-require-python-keystoneclient.html":{"url":"docs/swift-faq-auth-version-require-python-keystoneclient.html","title":"auth version require python-keystoneclient","keywords":"","body":"报 Auth versions 2.0 and 3 require python-keystoneclient 错误 env root@controller:~# . admin-openrc root@controller:~# swift stat Auth versions 2.0 and 3 require python-keystoneclient, install it or use Auth version 1.0 which requires ST_AUTH, ST_USER, and ST_KEY environment variables to be set or overridden with -A, -U, or -K. root@controller:~# step 检查 python-keystoneclient 是否安装 https://docs.openstack.org/swift/queens/install/controller-install-ubuntu.html apt-get install swift swift-proxy python-swiftclient \\ python-keystoneclient python-keystonemiddleware \\ memcached 发现已经安装过了。 swift-proxy 服务是否正常 root@controller:/etc/swift# service swift-proxy status ● swift-proxy.service - LSB: Swift proxy server Loaded: loaded (/etc/init.d/swift-proxy; bad; vendor preset: enabled) Active: active (running) since Wed 2019-03-06 12:55:50 CST; 7s ago Docs: man:systemd-sysv-generator(8) Process: 25557 ExecStop=/etc/init.d/swift-proxy stop (code=exited, status=0/SUCCESS) Process: 25568 ExecStart=/etc/init.d/swift-proxy start (code=exited, status=0/SUCCESS) Tasks: 5 Memory: 83.6M CPU: 1.101s CGroup: /system.slice/swift-proxy.service ├─25581 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─25590 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─25591 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─25592 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf └─25593 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf Mar 06 12:55:47 controller proxy-server[25590]: Starting Keystone auth_token middleware Mar 06 12:55:47 controller proxy-server[25590]: AuthToken middleware is set with keystone_authtoken.service_token_roles_required set to False. This is backwards compatible but deprecated behaviour. Please set this to True. Mar 06 12:55:47 controller proxy-server[25592]: Starting Keystone auth_token middleware Mar 06 12:55:47 controller proxy-server[25592]: AuthToken middleware is set with keystone_authtoken.service_token_roles_required set to False. This is backwards compatible but deprecated behaviour. Please set this to True. Mar 06 12:55:47 controller proxy-server[25593]: Starting Keystone auth_token middleware Mar 06 12:55:47 controller proxy-server[25593]: AuthToken middleware is set with keystone_authtoken.service_token_roles_required set to False. This is backwards compatible but deprecated behaviour. Please set this to True. Mar 06 12:55:50 controller swift-proxy[25568]: Starting proxy-server...(/etc/swift/proxy-server.conf) Mar 06 12:55:50 controller swift-proxy[25568]: No handlers could be found for logger \"keystonemiddleware._common.config\" Mar 06 12:55:50 controller swift-proxy[25568]: ...done. Mar 06 12:55:50 controller systemd[1]: Started LSB: Swift proxy server. root@controller:/etc/swift# openstack 是否正常 root@controller:~# openstack endpoint list +----------------------------------|-----------|--------------|--------------|---------|-----------|-------------------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------|-----------|--------------|--------------|---------|-----------|-------------------------------------------------+ | 014c4ed5c42042c394d62a1194bf07ce | RegionOne | keystone | identity | True | internal | http://controller:5000/v3/ | | 0b3dc5788cac4f2cb66edc3efacf10c0 | RegionOne | keystone | identity | True | public | http://controller:5000/v3/ | | 19e727c1668c4e8aae4315e13057fbbd | RegionOne | cinderv2 | volumev2 | True | public | http://controller:8776/v2/%(project_id)s | | 2548787f06524287b6a27d0a562da375 | RegionOne | glance | image | True | internal | http://controller:9292 | | 295cfee33ff04ab7890a84b47e95bc3f | RegionOne | keystone | identity | True | admin | http://controller:5000/v3/ | | 50d3b61b60654a65bda26584b4fe0896 | RegionOne | neutron | network | True | admin | http://controller:9696 | | 55367bdc8db6438da3a4ed82b1a1b04c | RegionOne | glance | image | True | public | http://controller:9292 | | 5b60dcd8e374410f83e675201d76f06f | RegionOne | placement | placement | True | admin | http://controller:8778 | | 76d61f360fc140a58b4258c344ea9ae5 | RegionOne | nova | compute | True | internal | http://controller:8774/v2.1 | | 7eece3971f9f4105b031214666940d54 | RegionOne | placement | placement | True | public | http://controller:8778 | | 8237211a85314e0c914ea34851e324b7 | RegionOne | cinderv2 | volumev2 | True | admin | http://controller:8776/v2/%(project_id)s | | 99b6673fda7f48bb8b6ae11a87ba0e1b | RegionOne | glance | image | True | admin | http://controller:9292 | | a61cbe1448284dd482725fb3067fa25c | RegionOne | nova | compute | True | public | http://controller:8774/v2.1 | | a9b074060f2e45f898c388dd382980c3 | RegionOne | swift | object-store | True | admin | http://192.168.0.50:8080/v1/AUTH_%(project_id)s | | bf59c9ad0d5e4fc5ad47bb9da0171e70 | RegionOne | swift | object-store | True | public | http://192.168.0.50:8080/v1/AUTH_%(project_id)s | | cfb1df03d1bd42719dd506e0fbee7e5a | RegionOne | cinderv3 | volumev3 | True | internal | http://controller:8776/v3/%(project_id)s | | cfe6b374e86048c98951d079a3cdaa42 | RegionOne | placement | placement | True | internal | http://controller:8778 | | d58337580bc34b97b83129ead16f46d3 | RegionOne | swift | object-store | True | internal | http://192.168.0.50:8080/v1/AUTH_%(project_id)s | | d9bb0785c23143ed855d0bb74dfe53bb | RegionOne | cinderv3 | volumev3 | True | public | http://controller:8776/v3/%(project_id)s | | e1f98d0e79f549a5bc46bcf05705c4b1 | RegionOne | neutron | network | True | public | http://controller:9696 | | f089ebe1cb5040319e942dbc607e9930 | RegionOne | cinderv3 | volumev3 | True | admin | http://controller:8776/v3/%(project_id)s | | f3350a1d66fa4813a091beef3782b6fd | RegionOne | neutron | network | True | internal | http://controller:9696 | | f5460d92ef8b428b9dc354628acdb289 | RegionOne | cinderv2 | volumev2 | True | internal | http://controller:8776/v2/%(project_id)s | | fb5c30679ed14e078646041e75e77294 | RegionOne | nova | compute | True | admin | http://controller:8774/v2.1 | +----------------------------------|-----------|--------------|--------------|---------|-----------|-------------------------------------------------+ root@controller:~# 或者 root@controller:~# openstack --os-identity-api-version=3 --os-auth-url=http://keystonehost:5000/ --os-username=swift --os-user-domain-id=default --os-project-name=service --os-project-domain-id=default --os-password=openstack catalog show object-store +-----------|-------------------------------------------------------------------------------+ | Field | Value | +-----------|-------------------------------------------------------------------------------+ | endpoints | RegionOne | | | admin: http://192.168.0.50:8080/v1/AUTH_a640c74e595c44c4902d1c5ebc3afa8a | | | RegionOne | | | public: http://192.168.0.50:8080/v1/AUTH_a640c74e595c44c4902d1c5ebc3afa8a | | | RegionOne | | | internal: http://192.168.0.50:8080/v1/AUTH_a640c74e595c44c4902d1c5ebc3afa8a | | | | | id | e929673efa1a4acb9adc4a06e4f56a31 | | name | swift | | type | object-store | +-----------|-------------------------------------------------------------------------------+ root@controller:~# 检查 keystone 是否正常 https://docs.openstack.org/keystone/rocky/install/keystone-verify-ubuntu.html root@controller:~# unset OS_AUTH_URL OS_PASSWORD root@controller:~# openstack --os-auth-url http://controller:5000/v3 \\ > --os-project-domain-name Default --os-user-domain-name Default \\ > --os-project-name admin --os-username admin token issue Password: +------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2019-03-06T07:45:03+0000 | | id | gAAAAABcf2xvzNJHdU_36s_NlO_2GFU4NY1CDi1LLEXacDeYHLKLAiZovw4CiTZJ7kVUPhqL_fXRYFrQPNGRU-9bt0WnBrGrpkOBhs-lVJxuyMM7f_MA6jo8UeQDEtryEkLcahejk5wtmwBHhTKozhM3oEQmvFvckGuVLDY6dzsPVHoE1uTXjGk | | project_id | f04ec0abf3d1460dad82608bb03af589 | | user_id | 8533cb3873974fa29f03832aef7007ca | +------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ root@controller:~# keystone 服务正常 那这时，看一下 keystone 日志，应该会能接收到请求。 但是，实际观察中，keystone 日志，没有收到请求。这说明，swift 本身的问题，没有发出请求到 keystone 。 swift 访问是否正常 root@controller:~# swift --os-auth-url http://controller:5000/v3 --auth-version 3\\ > --os-project-name demo --os-project-domain-name Default \\ > --os-username demo --os-user-domain-name Default \\ > --os-password openstack list Auth versions 2.0 and 3 require python-keystoneclient, install it or use Auth version 1.0 which requires ST_AUTH, ST_USER, and ST_KEY environment variables to be set or overridden with -A, -U, or -K. root@controller:~# 看一下版本与位置 root@controller:~# swift --version python-swiftclient 3.6.0 root@controller:~# which swift /usr/local/bin/swift root@controller:~# 但是，观察了另一台机器的swift 却是在 /usr/bin/swift 下的。 版本不一样。 root@controller:~# /usr/bin/swift --version python-swiftclient 3.5.0 root@controller:~# /usr/bin/swift --os-auth-url http://controller:5000/v3 --auth-version 3 --os-project-name demo --os-project-domain-name Default --os-username demo --os-user-domain-name Default --os-password openstack list 222 A-DOLcontainer A-DOLcontainer_segments LargeFileContainer LargeFileContainer_segments container1 container1_segments container2 jinweilai-work pub1 test3 test4 testcontainer1 root@controller:~# 原来是swift客户端版本不同导致的。那么使用能正常访问keystone的客户端吧。 root@controller:~# mv /usr/local/bin/swift /usr/local/bin/swift.bak root@controller:~# which swift /usr/bin/swift root@controller:~# . admin-openrc root@controller:~# swift list bash: /usr/local/bin/swift: No such file or directory root@controller:~# 这样还不行，看一下 PATH 顺序 root@controller:~# echo $PATH /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:$JAVA_HOME/bin root@controller:~# /usr/bin/swift stat -v StorageURL: http://192.168.0.50:8080/v1/AUTH_f04ec0abf3d1460dad82608bb03af589 Auth Token: gAAAAABcf22hOyYW2Qfa9JS7rKMiNScbVhEY-UtqJ_DsL86x5hXP_ZvMPCp5qHicxrMT3hJSWMPPXexYm1MkDzx43fvqbfXKr3UpbFQFTto4zqb51Rhc61iHyC4LQi5c2oK9VVU_5rLSkg0mNR6OALLvhOWzAgZHqQrZzMpLJ0SGwYrBtfgO-3Y Account: AUTH_f04ec0abf3d1460dad82608bb03af589 Containers: 2 Objects: 2 Bytes: 26264931 Containers in policy \"policy-0\": 2 Objects in policy \"policy-0\": 2 Bytes in policy \"policy-0\": 26264931 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: txe47025f385c84cc4a6821-005c7f6da1 X-Timestamp: 1547641627.86782 X-Trans-Id: txe47025f385c84cc4a6821-005c7f6da1 Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# /usr/bin/swift list DOLcontainer container1 root@controller:~# which swift /usr/bin/swift root@controller:~# swift list bash: /usr/local/bin/swift: No such file or directory 又还原了。那就直接建立 软链接 过去吧。 root@controller:~# ln -s /usr/bin/swift /usr/local/bin/swift root@controller:~# which swift /usr/local/bin/swift root@controller:~# swift --version python-swiftclient 3.5.0 root@controller:~# swift stat -v StorageURL: http://192.168.0.50:8080/v1/AUTH_f04ec0abf3d1460dad82608bb03af589 Auth Token: gAAAAABcf23XSjMmOIEeHmAw9XehvC-a4AUwhHKkz7HlKcR0uk1Dm_BMG3ZsiDw_1EMd677DHU4AeXQNmsS7fIkQv7wJSbo8IznVzZaZVgDczNJ-QgfUewvGyOYKs46QwYlitOxFX21l-55Uip58zT3GOvEDZL6TU8Wd2suSeQ6INMHQ2d5kYwA Account: AUTH_f04ec0abf3d1460dad82608bb03af589 Containers: 2 Objects: 2 Bytes: 26264931 Containers in policy \"policy-0\": 2 Objects in policy \"policy-0\": 2 Bytes in policy \"policy-0\": 26264931 X-Account-Project-Domain-Id: default X-Openstack-Request-Id: txe8d22a79b22b4587be827-005c7f6dd7 X-Timestamp: 1547641627.86782 X-Trans-Id: txe8d22a79b22b4587be827-005c7f6dd7 Content-Type: application/json; charset=utf-8 Accept-Ranges: bytes root@controller:~# 现在正常了。 (忽略)swift.bak 的保留在这里 root@controller:~# /usr/local/bin/swift.bak --version python-swiftclient 3.6.0 root@controller:~# /usr/local/bin/swift.bak stat -v Auth versions 2.0 and 3 require python-keystoneclient, install it or use Auth version 1.0 which requires ST_AUTH, ST_USER, and ST_KEY environment variables to be set or overridden with -A, -U, or -K. root@controller:~# Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-installed-source-code-address.html":{"url":"docs/swift-installed-source-code-address.html","title":"swift安装包源码地址","keywords":"","body":"swift 安装后，源码地址： root@controller:/usr/lib/python2.7/dist-packages/swift# ls account cli common container __init__.py __init__.pyc locale obj proxy root@controller:/usr/lib/python2.7/dist-packages/swift# Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-multy-proxy-node-faq.html":{"url":"docs/swift-multy-proxy-node-faq.html","title":"多代理节点安装出现的问题","keywords":"","body":"本文记录 swift 配置多proxy节点过程中出现的问题 关闭 proxy node 的swift-proxy 后，proxybak node 也失效 现场 出现下面的2个错误： Account GET failed: http://swiftproxy:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3?format=json 500 Internal Error An error occurred HTTPConnectionPool(host='swiftproxy', port=8080): Max retries exceeded with url: /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 (Caused by NewConnectionError(': Failed to establish a new connection: [Errno 111] Connection refused',)) 第一个问题是与认证相关；第二个问题是服务不能正常访问。 proxy node root@controller:~# service swift-proxy stop proxybak node root@ubuntu:/etc/swift# swift list HTTPConnectionPool(host='controller', port=8080): Max retries exceeded with url: /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3?format=json (Caused by NewConnectionError(': Failed to establish a new connection: [Errno 111] Connection refused',)) root@ubuntu:/etc/swift# 为什么会这样？按道理，这个时候，应该正常，可以访问的。 这里显示是连接到 controller 的 8080 ，这个地方是哪里来的呢？这个当然就是openstack endpoint注册的 swift 的 三个endpoint 之一了。 回到一台 能访问 openstack endpoint 的机器，查一下。 （本节，不要操作）更新 swift服务的endpoint 然后，重新添加一个新的 proxybak 地址的 swift endpoint。（注意：实际上是覆盖，而不是增加） openstack endpoint create --region RegionOne \\ object-store public http://swiftproxy:8080/v1/AUTH_%\\(project_id\\)s openstack endpoint create --region RegionOne \\ object-store internal http://swiftproxy:8080/v1/AUTH_%\\(project_id\\)s openstack endpoint create --region RegionOne \\ object-store admin http://swiftproxy:8080/v1 现在就有2个swift服务的URL，效果如下： root@controller:~# . admin-openrc root@controller:~# openstack endpoint list | grep swift | 057cd51c8ace4daa8834d25ae15998f4 | RegionOne | swift | object-store | True | admin | http://controller:8080/v1 | | 51952812b675436899ba118585b2dae2 | RegionOne | swift | object-store | True | public | http://swiftproxy:8080/v1/AUTH_%(project_id)s | | 87de9f6afbd74792bc80c7d092eb7da6 | RegionOne | swift | object-store | True | internal | http://controller:8080/v1/AUTH_%(project_id)s | | beccdad7270643cabf8a271258c102c0 | RegionOne | swift | object-store | True | admin | http://swiftproxy:8080/v1 | | d3f5808482cb42f386c3741b8e1c1d82 | RegionOne | swift | object-store | True | public | http://controller:8080/v1/AUTH_%(project_id)s | | e577371ea7e349ffa2a86f7aef2ef35e | RegionOne | swift | object-store | True | internal | http://swiftproxy:8080/v1/AUTH_%(project_id)s | root@controller:~# 再次访问 root@controller:~# swift list Account GET failed: http://swiftproxy:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3?format=json 500 Internal Error An error occurred Failed Transaction ID: txeaebd48bfe1346dc90161-005c481bf9 root@controller:~# cd /etc/swift/ root@ubuntu:/etc/swift# ping swiftproxy PING swiftproxy (192.168.0.141) 56(84) bytes of data. 64 bytes from swiftproxy (192.168.0.141): icmp_seq=1 ttl=64 time=0.025 ms ^C --- swiftproxy ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.025/0.025/0.025/0.000 ms root@ubuntu:/etc/swift# 查看 swift-proxy 日志 swift-proxy 日志是系统日志 /var/log/syslog 这里看到了报错如下： Jan 23 16:00:09 ubuntu proxy-server: Deferring reject downstream Jan 23 16:00:10 ubuntu proxy-server: - - 23/Jan/2019/08/00/10 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 HTTP/1.0 204 - Swift - - - - txf2bd6100da9d4e45bb421-005c481f09 - 0.0574 RL - 1548230409.998038054 1548230410.055468082 - Jan 23 16:00:10 ubuntu proxy-server: 192.168.0.141 192.168.0.141 23/Jan/2019/08/00/10 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 HTTP/1.0 500 - python-swiftclient-3.0.0 gAAAAABcSB8JuXYf... - - - txf2bd6100da9d4e45bb421-005c481f09 - 0.0734 - - 1548230409.982681036 1548230410.056046963 - Jan 23 16:00:10 ubuntu proxy-server: Error: An error occurred: #012Traceback (most recent call last):#012 File \"/usr/lib/python2.7/dist-packages/swift/common/middleware/catch_errors.py\", line 41, in handle_request#012 resp = self._app_call(env)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/wsgi.py\", line 1038, in _app_call#012 resp = self.app(env, self._start_response)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/middleware/gatekeeper.py\", line 99, in __call__#012 return self.app(env, gatekeeper_response)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/middleware/healthcheck.py\", line 57, in __call__#012 return self.app(env, start_response)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/middleware/proxy_logging.py\", line 346, in __call__#012 six.reraise(exc_type, exc_value, exc_traceback)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/middleware/proxy_logging.py\", line 338, in __call__#012 iterable = self.app(env, my_start_response)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/middleware/memcache.py\", line 109, in __call__#012 return self.app(env, start_response)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/swob.py\", line 1386, in _wsgify_self#012 return func(self, Request(env))(env, start_response)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/swob.py\", line 1386, in _wsgify_self#012 return func(self, Request(env))(env, start_response)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/middleware/ratelimit.py\", line 301, in __call__#012 return self.app(env, start_response)#012 File \"/usr/lib/python2.7/dist-packages/webob/dec.py\", line 130, in __call__#012 resp = self.call_func(req, *args, **self.kwargs)#012 File \"/usr/lib/python2.7/dist-packages/webob/dec.py\", line 195, in call_func#012 return self.func(req, *args, **kwargs)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/__init__.py\", line 464, in __call__#012 response = self.process_request(req)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/__init__.py\", line 732, in process_request#012 resp = super(AuthProtocol, self).process_request(request)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/__init__.py\", line 492, in process_request#012 data, user_auth_ref = self._do_fetch_token(request.user_token)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/__init__.py\", line 531, in _do_fetch_token#012 data = self.fetch_token(token)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/__init__.py\", line 835, in fetch_token#012 cached = self._cache_get_hashes(token_hashes)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/__init__.py\", line 818, in _cache_get_hashes#012 cached = self._token_cache.get(token)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/_cache.py\", line 222, in get#012 with self._cache_pool.reserve() as cache:#012 File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__#012 return self.gen.next()#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/_cache.py\", line 77, in reserve#012 c = memorycache.get_client(self._memcached_servers)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/openstack/common/memorycache.py\", line 44, in get_client#012 import memcache#012ImportError: No module named memcache (txn: txf2bd6100da9d4e45bb421-005c481f09) Jan 23 16:00:11 ubuntu proxy-server: 192.168.0.141 192.168.0.141 23/Jan/2019/08/00/11 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 HTTP/1.0 500 - python-swiftclient-3.0.0 gAAAAABcSB8JuXYf... - - - tx2336d108a1b94b8ba1f39-005c481f0b - 0.0214 - - 1548230411.059453964 1548230411.080816031 - Jan 23 16:00:11 ubuntu proxy-server: Error: An error occurred: #012Traceback (most recent call last):#012 File \"/usr/lib/python2.7/dist-packages/swift/common/middleware/catch_errors.py\", line 41, in handle_request#012 resp = self._app_call(env)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/wsgi.py\", line 1038, in _app_call#012 resp = self.app(env, self._start_response)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/middleware/gatekeeper.py\", line 99, in __call__#012 return self.app(env, gatekeeper_response)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/middleware/healthcheck.py\", line 57, in __call__#012 return self.app(env, start_response)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/middleware/proxy_logging.py\", line 346, in __call__#012 six.reraise(exc_type, exc_value, exc_traceback)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/middleware/proxy_logging.py\", line 338, in __call__#012 iterable = self.app(env, my_start_response)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/middleware/memcache.py\", line 109, in __call__#012 return self.app(env, start_response)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/swob.py\", line 1386, in _wsgify_self#012 return func(self, Request(env))(env, start_response)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/swob.py\", line 1386, in _wsgify_self#012 return func(self, Request(env))(env, start_response)#012 File \"/usr/lib/python2.7/dist-packages/swift/common/middleware/ratelimit.py\", line 301, in __call__#012 return self.app(env, start_response)#012 File \"/usr/lib/python2.7/dist-packages/webob/dec.py\", line 130, in __call__#012 resp = self.call_func(req, *args, **self.kwargs)#012 File \"/usr/lib/python2.7/dist-packages/webob/dec.py\", line 195, in call_func#012 return self.func(req, *args, **kwargs)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/__init__.py\", line 464, in __call__#012 response = self.process_request(req)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/__init__.py\", line 732, in process_request#012 resp = super(AuthProtocol, self).process_request(request)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/__init__.py\", line 492, in process_request#012 data, user_auth_ref = self._do_fetch_token(request.user_token)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/__init__.py\", line 531, in _do_fetch_token#012 data = self.fetch_token(token)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/__init__.py\", line 835, in fetch_token#012 cached = self._cache_get_hashes(token_hashes)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/__init__.py\", line 818, in _cache_get_hashes#012 cached = self._token_cache.get(token)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/_cache.py\", line 222, in get#012 with self._cache_pool.reserve() as cache:#012 File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__#012 return self.gen.next()#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/auth_token/_cache.py\", line 77, in reserve#012 c = memorycache.get_client(self._memcached_servers)#012 File \"/usr/lib/python2.7/dist-packages/keystonemiddleware/openstack/common/memorycache.py\", line 44, in get_client#012 import memcache#012ImportError: No module named memcache (txn: tx2336d108a1b94b8ba1f39-005c481f0b) 在这里可以看到 import memcache#012ImportError: No module named memcache。所以应该是python执行import memcache 没有成功。 python 安装 python-memcached root@ubuntu:~# pip install python-memcached 再来一次 root@ubuntu:/etc/swift# . demo-openrc root@ubuntu:/etc/swift# swift stat -v StorageURL: http://swiftproxy:8080/v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 Auth Token: gAAAAABcSC01KwFvQHMmrotw_7VKFVeUhXtqmrUCD8x0HEqfHQCXQ4MX4KI-KD6Qq4b5K8J-XdoflfOJxWgFoVKWftnC5AHokX_Eyu0fIzI3oarICGXh5kSsM49r2RXInw5TRljZnBT05iRbRsPGKoCvvdxcxsCW8nzzOh_RZWs4PbPBGZSeU6g Account: AUTH_7d6eaa90d74a4f239963933c3a744df3 Containers: 2 Objects: 30 Bytes: 3718416484 Containers in policy \"policy-0\": 2 Objects in policy \"policy-0\": 30 Bytes in policy \"policy-0\": 3718416484 X-Account-Project-Domain-Id: default X-Timestamp: 1547188179.49086 X-Trans-Id: txa5bbe4ae9a1243ab84539-005c482d35 Content-Type: text/plain; charset=utf-8 Accept-Ranges: bytes root@ubuntu:/etc/swift# swift-proxy 日志也正常了。 pipJan 23 16:17:01 ubuntu CRON[15089]: (root) CMD ( cd / && run-parts --report /etc/cron.hourly) Jan 23 16:31:55 ubuntu proxy-server: Deferring reject downstream Jan 23 16:31:55 ubuntu proxy-server: ERROR with Account server 192.168.0.134:6202/sdb re: Trying to HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3: ConnectionTimeout (0.5s) (txn: tx4b41c78a86b246c59725d-005c48267b) Jan 23 16:31:56 ubuntu proxy-server: - - 23/Jan/2019/08/31/56 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 HTTP/1.0 204 - Swift - - - - tx4b41c78a86b246c59725d-005c48267b - 1.0088 RL - 1548232315.264564991 1548232316.273391962 - Jan 23 16:31:57 ubuntu proxy-server: 192.168.0.141 192.168.0.141 23/Jan/2019/08/31/57 HEAD /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3 HTTP/1.0 204 - python-swiftclient-3.0.0 gAAAAABcSCZ7G8AM... - - - tx4b41c78a86b246c59725d-005c48267b - 2.3865 - - 1548232315.181828022 1548232317.568300962 - Jan 23 16:32:28 ubuntu proxy-server: 192.168.0.141 192.168.0.141 23/Jan/2019/08/32/28 GET /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Fformat%3Djson HTTP/1.0 200 - python-swiftclient-3.0.0 gAAAAABcSCabDkEo... - 114 - tx85f8ee55b83a4dfeab891-005c48269b - 0.8730 - - 1548232347.452719927 1548232348.325676918 - Jan 23 16:32:28 ubuntu proxy-server: ERROR with Account server 192.168.0.134:6202/sdb re: Trying to GET /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3: ConnectionTimeout (0.5s) (txn: txe1bf0548bc31486aafd60-005c48269c) (client_ip: 192.168.0.141) Jan 23 16:32:28 ubuntu proxy-server: 192.168.0.141 192.168.0.141 23/Jan/2019/08/32/28 GET /v1/AUTH_7d6eaa90d74a4f239963933c3a744df3%3Fformat%3Djson%26marker%3Dcontainer2 HTTP/1.0 200 - python-swiftclient-3.0.0 gAAAAABcSCabDkEo... - 2 - txe1bf0548bc31486aafd60-005c48269c - 0.6325 - - 1548232348.327672958 1548232348.960134983 - 问题解决。 Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-problem-with-xfs-file-system.html":{"url":"docs/swift-problem-with-xfs-file-system.html","title":"swift storage node 出现 xfs 错误","keywords":"","body":"+++ title = \"swift storage node 出现 xfs 错误\" date = 2019-01-09T00:00:00+08:00 lastmod = 2019-01-09T15:14:12+08:00 tags = [\"openstack\", \"swift\", \"xfs\"] categories = [\"openstack\"] draft = false weight = 3002 +++ env 在安装 swift storage node 过程中，把storage node关机，再开机时，开机登陆页面，出现如下的错误： 错误中，主要是包含 \"XFS(sdb): Internal error xfs_trans_cancel\", \"XFS(sdb): Corruption of in -memory data detected\" 等字样，这明显是 xfs 相关的问题。 在安装过程中，与 xfs 相关，主要就是挂载分区或挂载硬盘的时候咯。 而且查看 storage node 的挂载文件夹 root@swift107:/etc/swift# chown -R swift:swift /srv/node chown: cannot access '/srv/node/sdb': Input/output error root@swift107:/etc/swift# cd /srv/node/ root@swift107:/srv/node# ls ls: cannot access 'sdb': Input/output error sdb sdc root@swift107:/srv/node# 而且，如果是在 controller node 有监控的话，可以看到： root@controller:/etc/swift# service swift-proxy status ● swift-proxy.service - LSB: Swift proxy server Loaded: loaded (/etc/init.d/swift-proxy; bad; vendor preset: enabled) Active: active (running) since Wed 2019-01-09 10:19:46 CST; 5min ago Docs: man:systemd-sysv-generator(8) Process: 5475 ExecStop=/etc/init.d/swift-proxy stop (code=exited, status=0/SUCCESS) Process: 5486 ExecStart=/etc/init.d/swift-proxy start (code=exited, status=0/SUCCESS) Tasks: 5 Memory: 100.3M CPU: 5.790s CGroup: /system.slice/swift-proxy.service ├─5497 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─5506 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─5507 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf ├─5508 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf └─5509 /usr/bin/python /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf Jan 09 10:23:39 controller proxy-server[5509]: ERROR Insufficient Storage 192.168.100.107:6200/sda6 (txn: txff3a41a6156d41b39866c-00 Jan 09 10:23:39 controller proxy-server[5509]: ERROR Insufficient Storage 192.168.100.107:6200/sdb (txn: txff3a41a6156d41b39866c-005 Jan 09 10:23:39 controller proxy-server[5509]: 127.0.0.1 127.0.0.1 09/Jan/2019/02/23/39 PUT /v1/AUTH_7d6eaa90d74a4f239963933c3a744df Jan 09 10:23:39 controller proxy-server[5508]: 127.0.0.1 127.0.0.1 09/Jan/2019/02/23/39 GET /v1/AUTH_7d6eaa90d74a4f239963933c3a744df Jan 09 10:23:39 controller proxy-server[5507]: ERROR Insufficient Storage 192.168.100.107:6201/sdb (txn: txba2112e7ac2c4fd09ebc7-005 Jan 09 10:23:39 controller proxy-server[5507]: 127.0.0.1 127.0.0.1 09/Jan/2019/02/23/39 GET /v1/AUTH_7d6eaa90d74a4f239963933c3a744df root@controller:/etc/swift# 报出了 \" ERROR Insufficient Storage\" 错误。 step 通过 - https://www.experts-exchange.com/questions/26974279/Problem-with-xfs-file-system.html 知道，要先 umount 再使用 `xfs_repair -L `去修复一下，再mount。 root@swift107:/home/ubuntu# umount /srv/node/sdb umount: /srv/node/sdb: not mounted root@swift107:/home/ubuntu# umount /srv/node/sdc root@swift107:/home/ubuntu# xfs_repair -L /dev/sdb Phase 1 - find and verify superblock... Phase 2 - using internal log - zero log... ALERT: The filesystem has valuable metadata changes in a log which is being destroyed because the -L option was used. - scan filesystem freespace and inode maps... sb_ifree 199, counted 159 sb_fdblocks 5237633, counted 5196324 - found root inode chunk Phase 3 - for each AG... - scan and clear agi unlinked lists... - process known inodes and perform inode discovery... - agno = 0 - agno = 1 - agno = 2 correcting nblocks for inode 33575016, was 1 - counted 0 imap claims a free inode 33575022 is in use, correcting imap and clearing inode cleared inode 33575022 - agno = 3 - process newly discovered inodes... Phase 4 - check for duplicate blocks... - setting up duplicate extent list... - check for inodes claiming duplicate blocks... - agno = 1 - agno = 3 - agno = 0 - agno = 2 Phase 5 - rebuild AG headers and trees... - reset superblock... Phase 6 - check inode connectivity... - resetting contents of realtime bitmap and summary inodes - traversing filesystem ... entry \"hashes.pkl\" in directory inode 33575010 references already connected inode 33575016. - traversal finished ... - moving disconnected inodes to lost+found ... Phase 7 - verify and correct link counts... Maximum metadata LSN (4:8073) is ahead of log (1:2). Format log to cycle 7. done root@swift107:/home/ubuntu# xfs_repair -L /dev/sda6 Phase 1 - find and verify superblock... Phase 2 - using internal log - zero log... - scan filesystem freespace and inode maps... - found root inode chunk Phase 3 - for each AG... - scan and clear agi unlinked lists... - process known inodes and perform inode discovery... - agno = 0 - agno = 1 - agno = 2 - agno = 3 - process newly discovered inodes... Phase 4 - check for duplicate blocks... - setting up duplicate extent list... - check for inodes claiming duplicate blocks... - agno = 0 - agno = 2 - agno = 3 - agno = 1 Phase 5 - rebuild AG headers and trees... - reset superblock... Phase 6 - check inode connectivity... - resetting contents of realtime bitmap and summary inodes - traversing filesystem ... - traversal finished ... - moving disconnected inodes to lost+found ... Phase 7 - verify and correct link counts... Maximum metadata LSN (1:30) is ahead of log (1:2). Format log to cycle 4. done root@swift107:/home/ubuntu# mount /srv/node/sdb root@swift107:/home/ubuntu# ls /srv/node/sdb -a . .. accounts containers objects quarantined tmp root@swift107:/home/ubuntu# ls /srv/node/sdc -a . .. root@swift107:/home/ubuntu# mount | grep sd /dev/sda1 on / type ext4 (rw,relatime,errors=remount-ro,data=ordered) /dev/sdb on /srv/node/sdb type xfs (rw,noatime,nodiratime,attr2,nobarrier,inode64,logbufs=8,noquota) root@swift107:/home/ubuntu# du -sh /srv/node/sdb 291M /srv/node/sdb 然后，把 controller node 中的 swift-proxy 重启一下。 service swift-proxy restart service swift-proxy status 再上传文件，看一下 /srv/node/sdb 的大小是否有增加。 root@swift107:/home/ubuntu# du -sh /srv/node/sdb 498M /srv/node/sdb 因为这台机器的 /dev/sda6 分区，也是做成了 xfs 类型文件系统，所以，一样地处理一 下。 root@swift107:/home/ubuntu# mount /srv/node/sdc mount: /dev/sda6 is already mounted or /srv/node/sdc busy /dev/sda6 is already mounted on /srv/node/sdc root@swift107:/home/ubuntu# mount | grep sd /dev/sda1 on / type ext4 (rw,relatime,errors=remount-ro,data=ordered) /dev/sdb on /srv/node/sdb type xfs (rw,noatime,nodiratime,attr2,nobarrier,inode64,logbufs=8,noquota) /dev/sda6 on /srv/node/sdc type xfs (rw,noatime,nodiratime,attr2,nobarrier,inode64,logbufs=8,noquota) root@swift107:/home/ubuntu# du -sh /srv/node/sdc 0 /srv/node/sdc root@swift107:/home/ubuntu# 这样子，就完成了。 Ref https://www.experts-exchange.com/questions/26974279/Problem-with-xfs-file-system.html Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-rsync-dev2-fix-rsyncdconf.html":{"url":"docs/swift-rsync-dev2-fix-rsyncdconf.html","title":"rsync数据通信","keywords":"","body":"观察 swift storage node 间通过 rsync 进行数据通信。 最后结论： 要把最新的 *.ring.gz 文件分发至所有的 storage node ，然后 节点运行 swift-init all restart step 修改 /etc/rsyncd.conf 然后，把各节点 rsync 重启 service rsync restart service rsync status 此时，180,198,135的rsync日志已经出现新的同步信息。 说明，不需要重启启动swift-init服务。 但是，127，134的rsync日志，依旧没有变化。(很可能没有开始同步，为什么呢？是需要时间等待么？） 现在发现 134的 /etc/swift/*.ring.gz 文件不是rebalance后的*.ring.gz文件，所以，这个时候，我怀疑是不是这个地方的问题。 从51中复制*.ring.gz文件给 134，重启rsync，没有同步。重启swift-init。 swift-init all status swift-init all restart swift-init all status 依然没有同步。为什么？ 好了。这个时候发现 原来 180，198，135能同步，而134，127与其它3个连接不上，是因为， 180，198，135使用的是同一个老版本的 *.ring.gz 文件。晕死了！ 这个时候，从51中复制*.ring.gz文件给 180。不重启任何服务。这时， 127 rsync日志 2019/01/22 17:26:37 [25850] connect from swift0180 (192.168.0.180) 2019/01/22 09:26:37 [25850] rsync to object/sdb/objects/834 from swift0180 (192.168.0.180) 2019/01/22 09:26:37 [25850] receiving file list 2019/01/22 09:26:44 [25850] sent 62 bytes received 74104148 bytes total size 74085496 134 rsync日志 2019/01/22 17:26:43 [6716] connect from swift0180 (192.168.0.180) 2019/01/22 17:26:43 [6717] connect from swift0180 (192.168.0.180) 2019/01/22 09:26:43 [6716] rsync to account/sdb/tmp/a3ed88a4-5dcb-4f09-9122-96c590ec12cf from swift0180 (192.168.0.180) 2019/01/22 09:26:43 [6716] receiving file list 2019/01/22 09:26:43 [6717] rsync to account/sdb/tmp/2043b019-6d91-4c89-b1c0-ab4a9b05f460 from swift0180 (192.168.0.180) 2019/01/22 09:26:43 [6717] receiving file list 2019/01/22 17:26:45 [6720] connect from swift0180 (192.168.0.180) 2019/01/22 09:26:45 [6720] rsync to object/sdb/objects/538 from swift0180 (192.168.0.180) 2019/01/22 09:26:45 [6720] receiving file list 2019/01/22 17:27:00 [6723] connect from swift0180 (192.168.0.180) 2019/01/22 17:27:00 [6724] connect from swift0180 (192.168.0.180) 2019/01/22 09:27:00 [6723] rsync to container/sdb/tmp/42bc9ff4-1ad1-4802-bfbc-40796ca93fde from swift0180 (192.168.0.180) 2019/01/22 09:27:00 [6723] receiving file list 2019/01/22 09:27:00 [6724] rsync to container/sdb/tmp/0f6aa3dc-2abd-4259-a0f3-89ad321882c1 from swift0180 (192.168.0.180) 2019/01/22 09:27:00 [6724] receiving file list 但是，都停止在这里了，没有下文了。然后，我再回 180看 rsync日志 都是与 135,198 的同步信息。 我考虑把 180 rsync 重启。 无效，依然连接的是 135，198. 180 swift-init restart 134 2019/01/22 17:34:46 [6768] connect from swift0180 (192.168.0.180) 2019/01/22 09:34:46 [6768] rsync to object/sdb/objects/415 from swift0180 (192.168.0.180) 2019/01/22 09:34:46 [6768] receiving file list 2019/01/22 17:34:47 [6770] connect from swift0180 (192.168.0.180) 2019/01/22 17:34:47 [6770] max connections (2) reached 2019/01/22 17:34:47 [6771] connect from swift0180 (192.168.0.180) 2019/01/22 17:34:47 [6771] max connections (2) reached 2019/01/22 17:34:49 [6772] connect from swift0180 (192.168.0.180) 2019/01/22 17:34:50 [6772] max connections (2) reached 2019/01/22 17:34:50 [6773] connect from swift0180 (192.168.0.180) 2019/01/22 17:34:50 [6773] max connections (2) reached 2019/01/22 17:35:17 [6779] connect from swift0180 (192.168.0.180) 2019/01/22 17:35:17 [6779] max connections (2) reached 127 2019/01/22 09:34:45 [25913] rsync to object/sdb/objects/415 from swift0180 (192.168.0.180) 2019/01/22 09:34:45 [25913] receiving file list 2019/01/22 09:34:46 [25913] sent 31 bytes received 198 bytes total size 487805144 2019/01/22 17:34:47 [25935] connect from swift0180 (192.168.0.180) 2019/01/22 09:34:47 [25935] rsync to container/sdb/tmp/0f6aa3dc-2abd-4259-a0f3-89ad321882c1 from swift0180 (192.168.0.180) 2019/01/22 09:34:47 [25935] receiving file list 2019/01/22 09:34:48 [25935] sent 44 bytes received 21625 bytes total size 21504 无效，依然连接的是 135，198. 这个说明，这个同步机制，会由proxy协调，并，不受 本地的rsync 影响。 现在这个时候，chrome访问： http://192.168.0.51/horizon/auth/login/?next=/horizon/project/ 这个时候，看来，只能是把 *.ring.gz 文件分发给所有节点，再试一下了。 同时，观察180的 /var/log/rsync.log 2019/01/22 09:10:24 [22897] receiving file list 2019/01/22 09:10:24 [22897] rsync: mkdir \"/sdc/objects/832\" (in object) failed: No such file or directory (2) 2019/01/22 09:10:24 [22897] rsync error: error in file IO (code 11) at main.c(674) [Receiver=3.1.1] 这里出现了 rsync error: error in file IO (code 11) at main.c(674) [Receiver=3.1.1]，这个是指同步出错了么？ 解决完这个问题后，发现，最后，依然是要把最新的 *.ring.gz 文件分发至所有的 storage node ，然后 节点运行 swift-init all restart Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-rsync-storage-node-cannot-get-rebalance-data.html":{"url":"docs/swift-rsync-storage-node-cannot-get-rebalance-data.html","title":"rebalance后数据未通信","keywords":"","body":"Q：swift controller node rebalance后 部分 storage node 没有得到均衡数据，怎么办？ A：修改 对应的 storage node 的 /etc/rsyncd.conf，把 address = MANAGEMENT_INTERFACE_IP_ADDRESS 替换为 address = 0.0.0.0 , 并重启rsync 服务 service rsync restart Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-storage-node-system-failed-re-mount.html":{"url":"docs/swift-storage-node-system-failed-re-mount.html","title":"storage node 的主板坏了","keywords":"","body":"已使用 storage node 的主板坏了，重新换新主板，并挂载。 开机 修改 hostname hostnamectl set-hostname swift0134 修改 hosts vi /etc/hosts 如果是新storage node, 开机后查看到 df -h 已有挂载新硬盘 则运行下面： umount `df -h | grep \"sda1\" | awk '{print $NF}'` 总之，最后的结果是要保持 fdisk -l 有 /dev/sda1 , 而 df -h 中无 /dev/sda1 的状态。 打开 https://docs.openstack.org/swift/queens/install/storage-install-ubuntu-debian.html 按照下面进行： sudo su newstoragenodeip=192.168.0.134 安装依赖，并挂载磁盘 apt-get install xfsprogs rsync -y mkdir -p /srv/node/sdb echo \"/dev/sda1 /srv/node/sdb xfs noatime,nodiratime,nobarrier,logbufs=8 0 2\" >> /etc/fstab mkfs.xfs -f /dev/sda1 mount /srv/node/sdb apt-get install -y swift swift-account swift-container swift-object 复制controller node 上准备好的文件。 ## copy files controller=192.168.0.51 scp -r ubuntu@$controller:/tmp/storage-node/ /tmp/ cp /tmp/storage-node/hosts /etc/hosts cp /tmp/storage-node/rsync/rsync /etc/default/rsync cp /tmp/storage-node/rsync/rsyncd.conf /etc/rsyncd.conf cp /tmp/storage-node/swift/* /etc/swift/ grep \"MANAGEMENT_INTERFACE_IP_ADDRESS\" -rl /etc/rsyncd.conf | xargs sed -i \"s/MANAGEMENT_INTERFACE_IP_ADDRESS/$newstoragenodeip/g\" ## chown chown -R swift:swift /srv/node mkdir -p /var/cache/swift chown -R root:swift /var/cache/swift chmod -R 775 /var/cache/swift 上传文件 upload http://192.168.100.50/horizon/project/containers/container/test3 check 文件大小 du -sh /srv/node/sd*/ Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/swift-mount-structure-needs-cleaning.html":{"url":"docs/swift-mount-structure-needs-cleaning.html","title":"mount Structure needs cleaning","keywords":"","body":"如果出现 mount : **** Structure needs cleaning 也是通过 xfs_repair -L 来修复 [root@pc4 ~]# mount /src/node/sdb/ mount: Structure needs cleaning [root@pc4 ~]# xfs_repair -L /dev/sda1 Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "},"docs/note-about-openstack-swift.html":{"url":"docs/note-about-openstack-swift.html","title":"笔记1","keywords":"","body":"+++ title = \"openstack swift(笔记)\" date = 2019-01-19T00:00:00-08:00 lastmod = 2019-01-21T23:40:11-08:00 tags = [\"swift\"] categories = [\"swift\"] draft = false weight = 3001 +++ replication 默认为3 每次上传的文件，会经过ring选择3个硬盘存储。 可以看下面这个图的倒数3行命令的结果 eventually consistent Eventually Consistent：最终一致性 看一下 http://duanple.blog.163.com/blog/static/7097176720114733211308/ rsyncd做同步 当增删storage node，导致各个节点的数据不均衡时，rsync会自动去均衡。 可以通过查看 rsync 的日志 /var/log/rsyncd.log swift 的简单介绍视频 关于一个 swift 的简单介绍视频，很不错的。地址在：https://pan.baidu.com/s/16KFxATE8CG%5FEDkIlWqdURg Openstack Swift实践指南 | Openstack Swift文档阅读点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 06:22:24 "}}